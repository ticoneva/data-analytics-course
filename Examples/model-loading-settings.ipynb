{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0115784-3318-40a4-9b20-cd4e8b47aaac",
   "metadata": {},
   "source": [
    "# Performance Implications of Various Model Loading Options\n",
    "\n",
    "Version: 2024-4-18\n",
    "\n",
    "In this notebook, we will explore the performance implications of various model loading options, including the precision type and model loading mechanism.\n",
    "\n",
    "TL;DR: \n",
    "- Use `vLLM` if the model you want to use is compitable with the library. Use with AWQ quantization if possible.\n",
    "- If you must load the model with the `transformers` library:\n",
    "    - For small models, manually specify the correct 16-bit format.\n",
    "    - For large models, load model in 4-bit.\n",
    "\n",
    "First we write a class that loads a model and run inference, incorporating a few useful features such as GPU detection and memory release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2b276a-06a7-469c-9262-5cc889006af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "import torch\n",
    "import time\n",
    "import humanize\n",
    "import gc\n",
    "\n",
    "# Hide all but the most critical log messages\n",
    "logging.set_verbosity(logging.FATAL)\n",
    "\n",
    "class LoadTest():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_path=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the model\n",
    "        \"\"\"\n",
    "        \n",
    "        # Is GPU available?\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "            \n",
    "        if not \"device_map\" in kwargs:\n",
    "            kwargs[\"device_map\"] = self.device\n",
    "                          \n",
    "        start = time.time()\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path,**kwargs)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        print(\"Loading time:\",round(time.time() - start,2),'s')\n",
    "        print(\"Memory footprint:\",humanize.naturalsize(self.model.get_memory_footprint()))\n",
    "\n",
    "    def generate(self,messages=None,**kwargs):\n",
    "        \"\"\"\n",
    "        Generate a response to a message\n",
    "        \"\"\"\n",
    "        \n",
    "        if messages == None:\n",
    "             # Default message\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "                {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "            ]\n",
    "            \n",
    "        # Some default settings:\n",
    "        settings_list = [(\"max_new_tokens\",1000),(\"do_sample\",True)]\n",
    "        for s in settings_list:\n",
    "            if not s[0] in kwargs:\n",
    "                kwargs[s[0]] = s[1]\n",
    "            \n",
    "        #start =time.time()\n",
    "        \n",
    "        # Encode message, move to device, run through model and decode\n",
    "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "        model_inputs = encodeds.to(self.device)\n",
    "        generated_ids = self.model.generate(model_inputs,**kwargs)\n",
    "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
    "        \n",
    "        #print(\"Inference time:\",round(time.time() - start,2),'s')\n",
    "        \n",
    "        return decoded[0]\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Clear memory occupied by the model\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf293b-618e-450b-8f26-0b51407f9d85",
   "metadata": {},
   "source": [
    "## A. Default Settings\n",
    "\n",
    "The default settings load models in 32-bit precision. In this mode,\n",
    "a 1-billion parameter model requires approximately 4GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d953c77-10a4-4ad0-b44b-63e79b8505bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f132cd74c734eecaa098e2894f5d216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 39.37 s\n",
      "Memory footprint: 30.0 GB\n",
      "6.45 s ± 903 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest()\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630836ad-7760-42f1-9a87-ecd0afbe313b",
   "metadata": {},
   "source": [
    "## B. GPU/CPU Placement with `Accelerate`\n",
    "`Accelerate` can determine automatically the best way to place the model in the device(s) \n",
    "we have access to. This is particularly useful if we have more than one GPU.\n",
    "With only one GPU, the performance should be the same as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3944188-b992-4e98-980e-66a1d3974c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b94771c9e74b62ab6b2427011e2175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 37.91 s\n",
      "Memory footprint: 30.0 GB\n",
      "6.78 s ± 646 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest(device_map=\"auto\")\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a04b25a-1dd1-4b1e-9846-c3fcb016e5b1",
   "metadata": {},
   "source": [
    "## C. Auto-Detect Parameter Precision\n",
    "\n",
    "If we specify `torch_dtype='auto'`, `AutoModelForCausalLM` will attempt \n",
    "to detect the correct data type to use based on how the pretrained model was saved.\n",
    "If the detection could be incorrect, it could lead to unnecessary conversion of\n",
    "parameters, resulting in very long loading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f69741-50ed-48cc-86d4-a833b50d89fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8e10bd6fd4467aafd7df8d046b8f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 163.68 s\n",
      "Memory footprint: 15.0 GB\n",
      "6.88 s ± 532 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest(torch_dtype=\"auto\",\n",
    "                 device_map=\"auto\",\n",
    "                 )\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f37cc-c0ab-44eb-9077-cb0f78d02abb",
   "metadata": {},
   "source": [
    "## D. Manually Specifying Parameter Precision\n",
    "\n",
    "Instead of setting `torch_dtype` to auto, we could instead manually specify a data type.\n",
    "Here we will try the two 16-bit precision data types, `float16` and `bfloat16`.\n",
    "You will see that Mistral-7B is intended to work with the former, resulting in much faster\n",
    "loading time.\n",
    "\n",
    "When loading model in 16-bit precision, a 1-billion parameter model requires approximately 2GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113dedbc-2ba7-4a88-a550-50dddcb7d5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288e68a02e0740b58d048f951fcb3284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 34.68 s\n",
      "Memory footprint: 15.0 GB\n",
      "7.29 s ± 1.16 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest(torch_dtype=torch.float16,\n",
    "                 device_map=\"auto\",\n",
    "                 )\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56445d73-403c-4459-a799-44b751dcf380",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ba686d90ea4a769ea3402330742a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 161.88 s\n",
      "Memory footprint: 15.0 GB\n",
      "6.78 s ± 1.17 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest(torch_dtype=torch.bfloat16,\n",
    "                 device_map=\"auto\",\n",
    "                 )\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72478b34-2edd-404c-abfe-47c8889ffeb8",
   "metadata": {},
   "source": [
    "In contrast, there is no such difference for the Falcon-7B model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8180c0-914f-4982-be67-135877ddb3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a708684d65614b5f843ba8bd5517155f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 9.68 s\n",
      "Memory footprint: 13.9 GB\n",
      "The slowest run took 4.45 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "13.8 s ± 7.43 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest(\"tiiuae/falcon-7b-instruct\",\n",
    "                 torch_dtype=torch.float16,\n",
    "                 device_map=\"auto\",\n",
    "                 )\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffff32d3-0205-47cc-a1fa-e079f3d63b09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438c33518cc04805a5d76463c1c253dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 8.18 s\n",
      "Memory footprint: 13.9 GB\n",
      "9.99 s ± 4.27 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest(\"tiiuae/falcon-7b-instruct\",\n",
    "                 torch_dtype=torch.bfloat16,\n",
    "                 device_map=\"auto\",\n",
    "                 )\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d4324-170f-43fe-9ee9-c81f18059ea6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## E. Load in 8-bit Precision\n",
    "By specifying `load_in_8bit = True`, we can load the model in 8-bit precision,\n",
    "which reduces memory requirement to 1/4 of the default.\n",
    "In this mode, a 1-billion parameter model requires approximately 1GB of memory.\n",
    "This option relies on the `bitsandbytes` library, and for some reason the implementation\n",
    "is quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "122ca696-5e0a-45b6-a54c-b7cfd0dde44b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6835f7c971654702ac34fce55f63330d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 36.12 s\n",
      "Memory footprint: 8.0 GB\n",
      "44.5 s ± 7.9 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest(load_in_8bit=True,\n",
    "                 device_map=\"auto\",\n",
    "                 )\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f25da-7201-435d-be05-49373554cf20",
   "metadata": {},
   "source": [
    "## F. Load in 4-bit Precision\n",
    "We can lower the memory requirement further by specifying `load_in_4bit = True`. \n",
    "At 4-bit precision, memory requirement is 1/8 of the default.\n",
    "In this mode, a 1-billion parameter model requires approximately 500MB of memory.\n",
    "This option relies on the `bitsandbytes` library. Performance is slower than 16-bit\n",
    "but faster than 8-bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727ae9a5-bbff-4aef-b961-80ae637b42a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a07c8950aa54f8bac4e3ec776c2b180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 32.08 s\n",
      "Memory footprint: 4.6 GB\n",
      "8.75 s ± 1.05 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = LoadTest(quantization_config=bnb_config,\n",
    "                 device_map=\"auto\",\n",
    "                 )\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9432d9-b035-4ad8-87e9-130deda61a31",
   "metadata": {},
   "source": [
    "## H. Flash Attention 2\n",
    "Fast attention provides a drop-in replacement for PyTorch's self-attention mechanism.\n",
    "Some models already have efficient self-attention built-in, \n",
    "in which case we will not see any speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa90725c-7736-4e4a-9426-b4dfb80f2d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcfabfce0a84f3caf9fafe8841299e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 35.69 s\n",
      "Memory footprint: 15.0 GB\n",
      "8.43 s ± 398 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest(torch_dtype=torch.float16,\n",
    "                 device_map=\"auto\",\n",
    "                 attn_implementation=\"flash_attention_2\",\n",
    "                 )\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d982d76-f2d8-4ec0-bd81-0ac5e94bc6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b827ddb58dd43fe8e9ae75adebcd314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 8.13 s\n",
      "Memory footprint: 13.9 GB\n",
      "7.2 s ± 1.54 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model = LoadTest(\"tiiuae/falcon-7b-instruct\",\n",
    "                 torch_dtype=torch.bfloat16,\n",
    "                 device_map=\"auto\",\n",
    "                 attn_implementation=\"flash_attention_2\",\n",
    "                 )\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03180e-76c8-4eaf-9e16-cc2ea9313268",
   "metadata": {},
   "source": [
    "## vLLM\n",
    "\n",
    "vLLM is a library for fast LLM inference. This is the current recommended method \n",
    "to run inference if the model you intend to use is  \n",
    "[supported](https://docs.vllm.ai/en/latest/models/supported_models.html).\n",
    "\n",
    "We first modify our custom class to utilize vLLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057e58f4-6319-4433-bd4c-83190956b626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "import torch\n",
    "import time\n",
    "import humanize\n",
    "import gc\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "\n",
    "# vLLM needs this to find NCCL\n",
    "os.environ[\"VLLM_NCCL_SO_PATH\"] = \"/usr/lib/x86_64-linux-gnu/libnccl.so.2\"\n",
    "\n",
    "class LoadTestvLLM():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_path=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the model\n",
    "        \"\"\"\n",
    "                      \n",
    "        start = time.time()\n",
    "        \n",
    "        if not \"dtype\" in kwargs:\n",
    "            kwargs[\"dtype\"] = 'float16'\n",
    "        \n",
    "        # Load model\n",
    "        self.model = LLM(model=model_path,\n",
    "                         **kwargs)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        print(\"Loading time:\",round(time.time() - start,2),'s')\n",
    "        \n",
    "        # Approx. memory usage, buffer not included\n",
    "        model_params = self.model.llm_engine.model_executor.driver_worker.model_runner.model.parameters()\n",
    "        mem_params = sum([param.nelement()*param.element_size() for param in model_params])\n",
    "        print(\"Approx. memory footprint:\",humanize.naturalsize(mem_params))   \n",
    "\n",
    "    def generate(self,messages=None,**kwargs):\n",
    "        \"\"\"\n",
    "        Generate a response to a message\n",
    "        \"\"\"\n",
    "        \n",
    "        if messages == None:\n",
    "             # Default message\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "                {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "            ]\n",
    "            \n",
    "        # Some default settings:\n",
    "        settings_list = [(\"max_tokens\",1000)]\n",
    "        for s in settings_list:\n",
    "            if not s[0] in kwargs:\n",
    "                kwargs[s[0]] = s[1]\n",
    "                        \n",
    "            \n",
    "        sampling_params = SamplingParams(**kwargs)\n",
    "                  \n",
    "        # Encode message, move to device, run through model and decode\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        outputs = self.model.generate(prompt, sampling_params)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Clear memory occupied by the model\n",
    "        \"\"\"\n",
    "        destroy_model_parallel()\n",
    "        del self.model.llm_engine.model_executor.driver_worker\n",
    "        self.model = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55017140-03b2-4e3e-ade8-54f894cfb217",
   "metadata": {},
   "source": [
    "Default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba93f826-5168-42ad-9bda-6c1a6554a50d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-29 12:55:48 config.py:449] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 12-29 12:55:48 llm_engine.py:73] Initializing an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.1', tokenizer='mistralai/Mistral-7B-Instruct-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
      "INFO 12-29 12:56:23 llm_engine.py:222] # GPU blocks: 27184, # CPU blocks: 2048\n",
      "Loading time: 36.54 s\n",
      "Approx. memory footprint: 14.5 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.34s/it]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.33s/it]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.84 s ± 431 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LoadTestvLLM()\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a4e61-0d0e-4a81-b3f4-ee6afe38578a",
   "metadata": {},
   "source": [
    "vLLm is compatible with models quantized in the AWQ format. \n",
    "The AWQ version of many models can be found [here](https://huggingface.co/TheBloke)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f35648-244b-45d4-a15c-17e429912b65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-29 13:50:24 config.py:173] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-29 13:50:24 llm_engine.py:73] Initializing an LLM engine with config: model='TheBloke/Mistral-7B-Instruct-v0.2-AWQ', tokenizer='TheBloke/Mistral-7B-Instruct-v0.2-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=awq, seed=0)\n",
      "INFO 12-29 13:51:22 llm_engine.py:222] # GPU blocks: 22843, # CPU blocks: 2048\n",
      "Loading time: 60.11 s\n",
      "Approx. memory footprint: 4.2 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.07s/it]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.63 s ± 322 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LoadTestvLLM(model_path=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\")\n",
    "%timeit -r 3 -n 1 model.generate()\n",
    "model.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
