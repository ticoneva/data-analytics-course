{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad6cb30-23e2-4a9e-a24c-e1b604fa9579",
   "metadata": {},
   "source": [
    "# Estimate Memory Requirement\n",
    "\n",
    "The `deepspeed` library provides several functions to estimate the memory requirement during model training. The estimates provided are useful even if you are not using Deepspeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922ecd4-fddc-4576-9598-dca717a999c7",
   "metadata": {},
   "source": [
    "### Estimate Based on Actual Model\n",
    "\n",
    "This requires loading the actual model, which takes time and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "859f54ca-1bd9-4a85-975b-d424126cba55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c23440f6f14f05974303b7f5591205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 1 GPU per node.\n",
      "SW: Model with 12852M total params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  287.27GB |  23.94GB | offload_optimizer=cpu \n",
      "   71.82GB | 239.39GB | offload_optimizer=none\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 1 GPU per node.\n",
      "SW: Model with 12852M total params, 163M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  323.17GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "  323.17GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "  287.27GB |  24.55GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "  287.27GB |  24.55GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "    0.92GB | 216.06GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "   71.82GB | 216.06GB | offload_param=none, offload_optimizer=none, zero_init=0\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 and 2\n",
    "from transformers import AutoModel\n",
    "from deepspeed.runtime.zero.stage_1_and_2 import estimate_zero2_model_states_mem_needs_all_live\n",
    "model = AutoModel.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\",\n",
    "                                  token=\"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\")\n",
    "estimate_zero2_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)\n",
    "\n",
    "# Stage 3\n",
    "from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live\n",
    "estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe363cdc-2124-40d0-9651-e4218a26e881",
   "metadata": {},
   "source": [
    "### Estimate Based on Theoretical Values\n",
    "\n",
    "This does not require loading the actual model, \n",
    "but you need to know the model's parameter count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4920a485-4886-4d6e-984b-22a0d0995a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPU:\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 1 GPU per node.\n",
      "SW: Model with 13000M total params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  290.57GB |  24.21GB | offload_optimizer=cpu \n",
      "   72.64GB | 242.14GB | offload_optimizer=none\n",
      "\n",
      "2 GPU:\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 2 GPUs per node.\n",
      "SW: Model with 13000M total params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  290.57GB |  24.21GB | offload_optimizer=cpu \n",
      "  145.29GB | 145.29GB | offload_optimizer=none\n",
      "\n",
      "4 GPU:\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 4 GPUs per node.\n",
      "SW: Model with 13000M total params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  290.57GB |  24.21GB | offload_optimizer=cpu \n",
      "  290.57GB |  96.86GB | offload_optimizer=none\n",
      "\n",
      "8 GPU:\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 8 GPUs per node.\n",
      "SW: Model with 13000M total params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  581.15GB |  24.21GB | offload_optimizer=cpu \n",
      "  581.15GB |  72.64GB | offload_optimizer=none\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hypothetical estimate: stage 1 and 2\n",
    "from deepspeed.runtime.zero.stage_1_and_2 import estimate_zero2_model_states_mem_needs_all_cold\n",
    "for i in [1,2,4,8]:\n",
    "    print(i,\"GPU:\")\n",
    "    estimate_zero2_model_states_mem_needs_all_cold(13e9,i,1)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2331de0c-f212-4f81-ba59-d99756a880e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPU:\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 1 GPU per node.\n",
      "SW: Model with 13000M total params, 163M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  326.89GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "  326.89GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "  290.57GB |  24.82GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "  290.57GB |  24.82GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "    0.91GB | 218.54GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "   72.64GB | 218.54GB | offload_param=none, offload_optimizer=none, zero_init=0\n",
      "\n",
      "2 GPU:\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 2 GPUs per node.\n",
      "SW: Model with 13000M total params, 163M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  326.89GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "  326.89GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "  290.57GB |  12.71GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "  290.57GB |  12.71GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "    1.82GB | 109.57GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "  145.29GB | 109.57GB | offload_param=none, offload_optimizer=none, zero_init=0\n",
      "\n",
      "4 GPU:\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 4 GPUs per node.\n",
      "SW: Model with 13000M total params, 163M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  326.89GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "  326.89GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "  290.57GB |   6.66GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "  290.57GB |   6.66GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "    3.64GB |  55.09GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "  290.57GB |  55.09GB | offload_param=none, offload_optimizer=none, zero_init=0\n",
      "\n",
      "8 GPU:\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 8 GPUs per node.\n",
      "SW: Model with 13000M total params, 163M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "  326.89GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n",
      "  581.15GB |   0.61GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n",
      "  290.57GB |   3.63GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n",
      "  581.15GB |   3.63GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n",
      "    7.29GB |  27.85GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "  581.15GB |  27.85GB | offload_param=none, offload_optimizer=none, zero_init=0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hypothetical estimate: stage 3\n",
    "from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_cold\n",
    "for i in [1,2,4,8]:\n",
    "    print(i,\"GPU:\")\n",
    "    estimate_zero3_model_states_mem_needs_all_cold(13e9,163e6,i,1)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa1591-69f6-4837-80f9-ab5160c61eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
