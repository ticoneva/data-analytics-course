{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Version: 2022-9-22b\n",
    "\n",
    "In this notebook, we will learn how to run a regression in Python and investigate the effect of regularization. We will be using several Python libraries: ```pandas``` for organizing data, ```statsmodel```, ```scikit-learn``` for analysis and ```numpy``` for calculating mean and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #organize data\n",
    "import numpy as np #calculate mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Loading Data\n",
    "\n",
    "We first load the data we need. ```auto.csv``` contains the information of 74 vehicle models from 1978. We can load it with panda's ```read_csv()``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data. Specify the full path if your data \n",
    "# is not in the same directory as the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch columns just like if we are using a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See more than one columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default ```pandas``` returns a lot of rows. If you just want to get an idea how the data looks lke, it might be useful just to fetch a few rows from the top or the bottom of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top\n",
    "\n",
    "# bottom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Ordinary Least Squares Regression\n",
    "\n",
    "Now that we have data, we can run a simple **Ordinary Least Squares** (OLS) regression. \n",
    "\n",
    "\n",
    "Let $y_i$ be the target/dependent variable and \n",
    "$x_i$ a vector of features/independent variables of observation $i$.\n",
    "OLS have the following objective function:\n",
    "$$\n",
    "\\min_{a, \\beta}{\\sum_{i}{[y_i-(a+x_i\\beta)]^2}}\n",
    "$$\n",
    "$a$ is called the *intecept* and $\\beta$ *coefficients*. These are parameters that needs to be estimated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS is common enough that it is included in more than one library. The choices are:\n",
    "- ```statsmodels.api.OLS```: formatted output, statistics\n",
    "- ```sklearn.linear_model.LinearRegression```: no output, machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Statsmodel``` gives formatted output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodel OLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# statsmodel does not add the constant by default, so add manually\n",
    "results = sm.OLS(auto[\"price\"],\n",
    "                 sm.add_constant(auto[[\"mpg\",\"weight\"]])).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model in ```scikit-learn```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Run OLS: price on mpg and weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did above is common to all ```scikit-learn``` models:\n",
    "1. Specify the model we need.\n",
    "2. Train the model with ```fit()```.\n",
    "\n",
    "To get an idea how well the model does we can use the ```score()``` method.\n",
    "For regressions this returns the $R^2$ statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#R-squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitted values can be obtained by calling ```predict()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the price of a car with mpg of 10 and weight of 2000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning is due to us training the model with data that comes with feature names, but now we try to provide it with a new sample that comes without. To get rid of the warning, we can always provide feature names, or never provide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4944.06477799]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1: always provide feature names by using dataframes throughout\n",
    "ols.fit(auto[[\"mpg\",\"weight\"]],auto[[\"price\"]])\n",
    "df = pd.DataFrame([[10,2000]],columns=[\"mpg\",\"weight\"])\n",
    "ols.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4944.06477799]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: never provide feature names by using values throughout\n",
    "ols.fit(auto[[\"mpg\",\"weight\"]].values,auto[[\"price\"]])\n",
    "ols.predict([[10,2000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a machine-learning library, ```scikit-learn``` makes it very easy to build models by hiding a lot of the details behind the background. Here is how to fetch the regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "print(\"Coefficients:\",ols.coef_)\n",
    "print(\"Intecept:\",ols.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with using OLS as a predictive model is that its predictions have very high variance, meaning that predictions vary significantly across models trained with different samples.\n",
    "\n",
    "To see this, we will:\n",
    "1. Repeatedly draw samples of data.\n",
    "2. Train the model.\n",
    "3. Make predictions base on the model.\n",
    "4. Calculate the mean and standard deviation of predictions from different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample #for resampling\n",
    "\n",
    "# Whole data\n",
    "x_pop = auto[[\"mpg\",\"weight\"]]\n",
    "y_pop = auto[\"price\"]\n",
    "\n",
    "# Empty list to contain predictions\n",
    "predict_list = []\n",
    "\n",
    "# Draw random samples and fit model for 30 times\n",
    "for i in range(30):\n",
    "    \n",
    "    # Draw 20 random samples\n",
    "    y,x = resample(y_pop,x_pop,n_samples=20) \n",
    "    model = LinearRegression() #OLS\n",
    "    model.fit(x,y)\n",
    "    \n",
    "    xb = model.predict([[10,2000]]) #Predict price\n",
    "    predict_list.append(xb) #Append the predicted price to predict_list \n",
    "\n",
    "print(\"Mean of predictions:\", np.mean(predict_list)) #mean of predictions    \n",
    "print(\"S.D. of predictions:\", np.std(predict_list)) #standard deviation of predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression** have the following objective function:\n",
    "$$\n",
    "\\min_{a, \\beta}{\\left\\{ \\sum_{i}{[y_i-(a+x_i\\beta)]^2}+\\alpha \\sum_{k}{\\beta_k^2} \\right\\} }\n",
    "$$\n",
    "$\\alpha$ is the strength of regularization. All else equal, higher $\\alpha$ would result in smaller coefficients.\n",
    "\n",
    "Scikit-learn's ridge regression is ```Ridge()```. Play around with different values of ```alpha```&mdash;e.g. 50, 5000, 5000000&mdash;and see how the estimated coefficients change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Run a ridge regression\n",
    "\n",
    "\n",
    "# Coefficients\n",
    "print(\"Coefficients:\",ridge.coef_)\n",
    "print(\"Intecept:\",ridge.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso Regression** have the following objective function:\n",
    "$$\n",
    "\\min_{a, \\beta}{\\left\\{ \\sum_{i}{[y_i-(a+x_i\\beta)]^2}+\\alpha \\sum_{k}{\\left|\\beta_k\\right|} \\right\\} }\n",
    "$$\n",
    "$\\alpha$ is the strength of regularization. All else equal, higher $\\alpha$ would result more coefficients becoming zero. As before, play around with ```alpha``` to get an idea of how the coefficients change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso \n",
    "\n",
    "# Run a lasso regression\n",
    "\n",
    "\n",
    "# Coefficients\n",
    "print(\"Coefficients:\",lasso.coef_)\n",
    "print(\"Intecept:\",lasso.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Standardizing Data\n",
    "\n",
    "Because regularization penalizes large coefficients, standardizing data is usually necessary, otherwise variables with small units of measure&mdash;thus large coefficients&mdash;will get unfairly penalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Regularized vs Non-Regularized: Coefficient Estimates\n",
    "\n",
    "For a more systematic examination, let us pick a list of regularization strengths and use a loop to run the three types of regressions. We will store the estimated coefficients and plot them in two diagrams with the ```matplotlib``` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphas to go through\n",
    "alphas = [0.1,1,5,50,500,5000,50000,500000,5000000,50000000]\n",
    "\n",
    "# Empty lists to store coefficients\n",
    "mpg_ols = []\n",
    "mpg_ridge = [] \n",
    "mpg_lasso = []\n",
    "weight_ols = []\n",
    "weight_ridge = []\n",
    "weight_lasso = []\n",
    "\n",
    "# Run regressions for each alpha and save coefficients \n",
    "for a in alphas:\n",
    "    # OLS\n",
    "    scaler = StandardScaler()\n",
    "    ols = LinearRegression()\n",
    "    ols_pipe = Pipeline(steps=[(\"scaler\",scaler),\n",
    "                               (\"ols\",ols)])\n",
    "    ols_pipe.fit(auto[[\"mpg\",\"weight\"]],auto[\"price\"])\n",
    "    mpg_ols.append(ols_pipe[\"ols\"].coef_[0]) # Append coef. of mpg to list mpg_ols\n",
    "    weight_ols.append(ols_pipe[\"ols\"].coef_[1]) #A ppend coef. of weight to list weight_ols\n",
    "    \n",
    "    # Ridge\n",
    "    scaler1 = StandardScaler()\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge_pipe = Pipeline(steps=[(\"scaler\", scaler1),\n",
    "                           (\"ridge\", ridge)])\n",
    "    ridge_pipe.fit(auto[[\"mpg\",\"weight\"]],auto[\"price\"])\n",
    "    mpg_ridge.append(ridge_pipe[\"ridge\"].coef_[0])\n",
    "    weight_ridge.append(ridge_pipe[\"ridge\"].coef_[1])\n",
    "    \n",
    "    # Lasso\n",
    "    scaler2 = StandardScaler()\n",
    "    lasso = Lasso(alpha=a)\n",
    "    lasso_pipe = Pipeline(steps=[(\"scaler\", scaler2),\n",
    "                           (\"lasso\", lasso)])\n",
    "    lasso_pipe.fit(auto[[\"mpg\",\"weight\"]],auto[\"price\"])\n",
    "    mpg_lasso.append(lasso_pipe[\"lasso\"].coef_[0])\n",
    "    weight_lasso.append(lasso_pipe[\"lasso\"].coef_[1])\n",
    "    \n",
    "# Import library for graphing    \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot mpg graph\n",
    "fig, (gm, gw) = plt.subplots(ncols=2,figsize=(10,5))\n",
    "gm.set_title(\"mpg\")\n",
    "gm.set_ylabel(\"estimated coefficient\")\n",
    "gm.set_xlabel(\"alpha\")\n",
    "gm.set_xscale(\"log\")\n",
    "gm.plot(alphas, mpg_ols, label='OLS')\n",
    "gm.plot(alphas, mpg_ridge, label='Ridge')\n",
    "gm.plot(alphas, mpg_lasso, label='Lasso')\n",
    "\n",
    "# Plot weight graph\n",
    "gw.set_title(\"weight\")\n",
    "gw.set_ylabel(\"estimated coefficient\")\n",
    "gw.set_xlabel(\"alpha\")\n",
    "gw.set_xscale(\"log\")\n",
    "gw.plot(alphas, weight_ols, label='OLS')\n",
    "gw.plot(alphas, weight_ridge, label='Ridge')\n",
    "gw.plot(alphas, weight_lasso, label='Lasso')\n",
    "gw.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Regularized vs Non-Regularized: Prediction Variance\n",
    "\n",
    "How does the regularized regressions help with the problem of variance? To investigate, we will repeat what we did previously:\n",
    "1. Repeatedly draw samples of data.\n",
    "2. Train various models.\n",
    "3. Make predictions base on the models.\n",
    "4. Calculate the mean and standard deviation of predictions from different models and different samples.\n",
    "\n",
    "For ridge and lasso, we additionally need to try different values of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphas to go through\n",
    "alphas = [0.1,1,5,50,500,5000,50000,500000,5000000,50000000]\n",
    "\n",
    "# An sample/observation of independent variables to be used for prediction\n",
    "query = [[10,2000]]\n",
    "\n",
    "# Empty lists to contain mean predictions\n",
    "ols_mean = []\n",
    "ridge_mean = []\n",
    "lasso_mean = []\n",
    "\n",
    "# Empty lists to contain standard deviations\n",
    "ols_std = []\n",
    "ridge_std = []\n",
    "lasso_std = []\n",
    "\n",
    "# Loop through alphas\n",
    "for a in alphas: \n",
    "    \n",
    "    # Empty lists to contain predictions\n",
    "    ols_predict = []\n",
    "    ridge_predict = []\n",
    "    lasso_predict = []\n",
    "\n",
    "    # Draw 20 random samples and fit models for 50 times\n",
    "    for i in range(50):\n",
    "        \n",
    "        y,x = resample(y_pop,x_pop,n_samples=20) \n",
    "\n",
    "        # OLS\n",
    "        ols = LinearRegression()\n",
    "        ols.fit(x.values,y)\n",
    "        ols_xb = ols.predict(query) \n",
    "        ols_predict.append(ols_xb)\n",
    "        \n",
    "        # Ridge\n",
    "        scaler1 = StandardScaler()\n",
    "        ridge = Ridge(alpha=a)\n",
    "        ridge_pipe = Pipeline(steps=[(\"scaler\", scaler1),\n",
    "                               (\"ridge\", ridge)])\n",
    "        ridge_pipe.fit(x.values,y)\n",
    "        ridge_xb = ridge_pipe.predict(query)\n",
    "        ridge_predict.append(ridge_xb)\n",
    "\n",
    "        # Lasso\n",
    "        scaler2 = StandardScaler()\n",
    "        lasso = Lasso(alpha=a)\n",
    "        lasso_pipe = Pipeline(steps=[(\"scaler\", scaler2),\n",
    "                               (\"lasso\", lasso)])\n",
    "        lasso_pipe.fit(x.values,y)\n",
    "        lasso_xb = lasso_pipe.predict(query)\n",
    "        lasso_predict.append(lasso_xb)         \n",
    "\n",
    "    # Calculate and store mean prediction\n",
    "    ols_mean.append(np.mean(ols_predict))\n",
    "    ridge_mean.append(np.mean(ridge_predict))\n",
    "    lasso_mean.append(np.mean(lasso_predict))        \n",
    "        \n",
    "    # Calculate and store standard deviations\n",
    "    ols_std.append(np.std(ols_predict))\n",
    "    ridge_std.append(np.std(ridge_predict))\n",
    "    lasso_std.append(np.std(lasso_predict))\n",
    "\n",
    "# Import library for graphing    \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt    \n",
    "\n",
    "# Plot mean\n",
    "fig, (mn, sd) = plt.subplots(ncols=2,figsize=(10,5))\n",
    "mn.set_title(\"Mean of Prediction from Different Samples \\n (mpg=10, weight=2000)\")\n",
    "mn.set_xlabel(\"alpha\")\n",
    "mn.set_xscale(\"log\")\n",
    "mn.plot(alphas, ols_mean, label='OLS')\n",
    "mn.plot(alphas, ridge_mean, label='Ridge')\n",
    "mn.plot(alphas, lasso_mean, label='Lasso')\n",
    "mn.legend(loc=0)  \n",
    "\n",
    "# Plot standard deviations\n",
    "sd.set_title(\"S.D. of Prediction from Different Samples \\n (mpg=10, weight=2000)\")\n",
    "sd.set_xlabel(\"alpha\")\n",
    "sd.set_xscale(\"log\")\n",
    "sd.plot(alphas, ols_std, label='OLS')\n",
    "sd.plot(alphas, ridge_std, label='Ridge')\n",
    "sd.plot(alphas, lasso_std, label='Lasso')\n",
    "sd.legend(loc=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two thihngs you should have noticed:\n",
    "- Ridge and lasso have much smaller variance than OLS when ```alpha``` is large.\n",
    "- Even though OLS is an unbiased estimator, it does not appear to produce more accurate predictions than ridge or lasso when the sample size is small. You can verify this by changing the number of samples drawn each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Conclusion\n",
    "\n",
    "In this notebook, we have investigated how regularization can help reduce the variance of predictions. This ensures that the model we trained on a particular sample of data will be reasonably accurate when applied to other samples. \n",
    "\n",
    "A question that we have not yet touch on is, how do we choose the strength of regularization, ```alpha```? Conceptually, what we want to do is to pick a value that gives us the most accurate prediction when applied to other samples. In order to do so we would need to conduct out-of-sample test, which is the topic of the next notebook."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
