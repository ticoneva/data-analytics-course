{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "In this notebook, we will learn how to run a regression in Python and investigate the effect of regularization. We will be using several Python libraries: ```pandas``` for organizing data, ```statsmodel```, ```scikit-learn``` for analysis and ```numpy``` for calculating mean and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #organize data\n",
    "import numpy as np #calculate mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Loading Data\n",
    "\n",
    "We first load the data we need. ```auto.csv``` contains the information of 74 vehicle models from 1978. We can load it with panda's ```read_csv()``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data. Specify the full path if your data \n",
    "#is not in the same directory as the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch columns just like if we are using a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See more than one columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default ```pandas``` returns a lot of rows. If you just want to get an idea how the data looks lke, it might be useful just to fetch a few rows from the top or the bottom of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top\n",
    "\n",
    "#bottom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Ordinary Least Squares Regression\n",
    "\n",
    "Now that we have data, we can run a simple **Ordinary Least Squares** (OLS) regression. \n",
    "\n",
    "\n",
    "Let $y_i$ be the target/dependent variable and \n",
    "$x_i$ a vector of features/independent variables of observation $i$.\n",
    "OLS have the following objective function:\n",
    "$$\n",
    "\\min_{a, \\beta}{\\sum_{i}{[y_i-(a+x_i\\beta)]^2}}\n",
    "$$\n",
    "$a$ is called the *intecept* and $\\beta$ *coefficients*. These are parameters that needs to be estimated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS is common enough that it is included in more than one library. The choices are:\n",
    "- ```statsmodels.api.OLS```: formatted output, statistics\n",
    "- ```sklearn.linear_model.LinearRegression```: no output, machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Statsmodel``` gives formatted output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodel OLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# statsmodel does not add the constant by default, so add manually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model in ```scikit-learn```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Run OLS: price on mpg and weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did above is common to all ```scikit-learn``` models:\n",
    "1. Specify the model we need.\n",
    "2. Train the model with ```fit()```.\n",
    "\n",
    "To get an idea how well the model does we can use the ```score()``` method.\n",
    "For regressions this returns the $R^2$ statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#R-squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitted values can be obtained by calling ```predict()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the price of a car with mpg of 10 and weight of 2000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a machine-learning library, ```scikit-learn``` makes it very easy to build models by hiding a lot of the details behind the background. Here is how to fetch the regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficients\n",
    "print(\"Coefficients:\",ols.coef_)\n",
    "print(\"Intecept:\",ols.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression** have the following objective function:\n",
    "$$\n",
    "\\min_{a, \\beta}{\\left\\{ \\sum_{i}{[y_i-(a+x_i\\beta)]^2}+\\alpha \\sum_{k}{\\beta_k^2} \\right\\} }\n",
    "$$\n",
    "$\\alpha$ is the strength of regularization. All else equal, higher $\\alpha$ would result in smaller coefficients.\n",
    "\n",
    "Scikit-learn's ridge regression is ```Ridge()```. Play around with different values of ```alpha```&mdash;e.g. 50, 5000, 5000000&mdash;and see how the estimated coefficients change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#Run a ridge regression\n",
    "\n",
    "\n",
    "#Coefficients\n",
    "print(\"Coefficients:\",ridge.coef_)\n",
    "print(\"Intecept:\",ridge.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso Regression** have the following objective function:\n",
    "$$\n",
    "\\min_{a, \\beta}{\\left\\{ \\sum_{i}{[y_i-(a+x_i\\beta)]^2}+\\alpha \\sum_{k}{\\left|\\beta_k\\right|} \\right\\} }\n",
    "$$\n",
    "$\\alpha$ is the strength of regularization. All else equal, higher $\\alpha$ would result more coefficients becoming zero. As before, play around with ```alpha``` to get an idea of how the coefficients change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso \n",
    "\n",
    "#Run a lasso regression\n",
    "\n",
    "\n",
    "#Coefficients\n",
    "print(\"Coefficients:\",lasso.coef_)\n",
    "print(\"Intecept:\",lasso.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more systematic examination, let us pick a list of regularization strengths and use a loop to run the three types of regressions. We will store the estimated coefficients and plot them in two diagrams with the ```matplotlib``` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alphas to go through\n",
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000]\n",
    "\n",
    "#Empty lists to store coefficients\n",
    "mpg_ols = []\n",
    "mpg_ridge = [] \n",
    "mpg_lasso = []\n",
    "weight_ols = []\n",
    "weight_ridge = []\n",
    "weight_lasso = []\n",
    "\n",
    "#Run regressions for each alpha and save coefficients \n",
    "for a in alphas:\n",
    "    #OLS\n",
    "    ols = LinearRegression()\n",
    "    ols.fit(auto[[\"mpg\",\"weight\"]],auto[\"price\"])\n",
    "    mpg_ols.append(ols.coef_[0]) #Append coef. of mpg to list mpg_ols\n",
    "    weight_ols.append(ols.coef_[1]) #Append coef. of weight to list weight_ols\n",
    "    \n",
    "    #Ridge\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(auto[[\"mpg\",\"weight\"]],auto[\"price\"])\n",
    "    mpg_ridge.append(ridge.coef_[0])\n",
    "    weight_ridge.append(ridge.coef_[1])\n",
    "    \n",
    "    #Lasso\n",
    "    lasso = Lasso(alpha=a)\n",
    "    lasso.fit(auto[[\"mpg\",\"weight\"]],auto[\"price\"])\n",
    "    mpg_lasso.append(lasso.coef_[0])\n",
    "    weight_lasso.append(lasso.coef_[1])\n",
    "    \n",
    "#Import library for graphing    \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot mpg graph\n",
    "fig, (gm, gw) = plt.subplots(ncols=2,figsize=(10,5))\n",
    "gm.set_title(\"mpg\")\n",
    "gm.set_ylabel(\"estimated coefficient\")\n",
    "gm.set_xlabel(\"alpha\")\n",
    "gm.set_xscale(\"log\")\n",
    "gm.plot(alphas, mpg_ols, label='OLS')\n",
    "gm.plot(alphas, mpg_ridge, label='Ridge')\n",
    "gm.plot(alphas, mpg_lasso, label='Lasso')\n",
    "\n",
    "#Plot weight graph\n",
    "gw.set_title(\"weight\")\n",
    "gw.set_ylabel(\"estimated coefficient\")\n",
    "gw.set_xlabel(\"alpha\")\n",
    "gw.set_xscale(\"log\")\n",
    "gw.plot(alphas, weight_ols, label='OLS')\n",
    "gw.plot(alphas, weight_ridge, label='Ridge')\n",
    "gw.plot(alphas, weight_lasso, label='Lasso')\n",
    "gw.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. A Comparison between Different Regression Models\n",
    "\n",
    "How does the regularized regressions help with the problem of variance? To investigate, for each regression model we will:\n",
    "1. Repeatedly draw samples of data.\n",
    "2. Train the model.\n",
    "3. Make predictions base on the model.\n",
    "4. Calculate the mean and standard deviation of predictions from different samples.\n",
    "\n",
    "For ridge and lasso, you should once again try different values of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample #for resampling\n",
    "\n",
    "#Whole data\n",
    "x_pop = auto[[\"mpg\",\"weight\"]]\n",
    "y_pop = auto[\"price\"]\n",
    "\n",
    "#Empty list to contain predictions\n",
    "predict_list = []\n",
    "\n",
    "#Draw random samples and fit model for 30 times\n",
    "   \n",
    "    #Draw 20 random samples\n",
    "    \n",
    "    #Predict price\n",
    "    \n",
    "    #Append the predicted price to predict_list \n",
    "\n",
    "    \n",
    "#Print mean and standard deviation of predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "\n",
    "x_pop = auto[[\"mpg\",\"weight\"]]\n",
    "y_pop = auto[\"price\"]\n",
    "\n",
    "predict_list = []\n",
    "\n",
    "for i in range(30):\n",
    "    y,x = resample(y_pop,x_pop,n_samples=20) \n",
    "    model = Ridge(alpha=500000) #Ridge\n",
    "    model.fit(x,y)\n",
    "    \n",
    "    xb = model.predict([[10,2000]])\n",
    "    predict_list.append(xb)\n",
    "\n",
    "print(\"Mean of predictions:\", np.mean(predict_list))  \n",
    "print(\"S.D. of predictions:\", np.std(predict_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "\n",
    "x_pop = auto[[\"mpg\",\"weight\"]]\n",
    "y_pop = auto[\"price\"]\n",
    "\n",
    "predict_list = []\n",
    "\n",
    "for i in range(30):\n",
    "    y,x = resample(y_pop,x_pop,n_samples=20) \n",
    "    model = Lasso(alpha=500000) #Lasso\n",
    "    model.fit(x,y)\n",
    "    \n",
    "    xb = model.predict([[10,2000]]) \n",
    "    predict_list.append(xb) \n",
    "\n",
    "print(\"Mean of predictions:\", np.mean(predict_list))  \n",
    "print(\"S.D. of predictions:\", np.std(predict_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things you should have noticed:\n",
    "- OLS predictions have very high variance, meaning that predictions vary significantly across models trained with different samples.\n",
    "- Ridge and lasso have much smaller variance than OLS when ```alpha``` is large.\n",
    "- Even though OLS is an unbiased estimator, it does not appear to produce more accurate predictions than ridge or lasso.\n",
    "\n",
    "As before, we can examine the effect of regularization by loop through a list of alphas and plot their associated standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alphas to go through\n",
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000]\n",
    "\n",
    "#An sample/observation of independent variables to be used for prediction\n",
    "query = [[10,2000]]\n",
    "\n",
    "#Empty lists to contain standard deviations\n",
    "ols_std = []\n",
    "ridge_std = []\n",
    "lasso_std = []\n",
    "\n",
    "#Loop through alphas\n",
    "for a in alphas: \n",
    "    \n",
    "    #Empty lists to contain predictions\n",
    "    ols_predict = []\n",
    "    ridge_predict = []\n",
    "    lasso_predict = []\n",
    "\n",
    "    #Draw 20 random samples and fit models for 30 times\n",
    "    for i in range(30):\n",
    "        \n",
    "        y,x = resample(y_pop,x_pop,n_samples=20) \n",
    "\n",
    "        #ols\n",
    "        ols = LinearRegression()\n",
    "        ols.fit(x,y)\n",
    "        ols_xb = ols.predict(query) \n",
    "        ols_predict.append(ols_xb)\n",
    "        \n",
    "        #Ridge\n",
    "        ridge = Ridge(alpha=a)\n",
    "        ridge.fit(x,y)\n",
    "        ridge_xb = ridge.predict(query)\n",
    "        ridge_predict.append(ridge_xb)\n",
    "        \n",
    "        #Lasso\n",
    "        lasso = Lasso(alpha=a)\n",
    "        lasso.fit(x,y)\n",
    "        lasso_xb = lasso.predict(query)\n",
    "        lasso_predict.append(lasso_xb)        \n",
    "\n",
    "    #Calculate and store standard deviations\n",
    "    ols_std.append(np.std(ols_predict))\n",
    "    ridge_std.append(np.std(ridge_predict))\n",
    "    lasso_std.append(np.std(lasso_predict))\n",
    "\n",
    "#Import library for graphing    \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt    \n",
    "    \n",
    "#Plot standard deviations\n",
    "fig, sd = plt.subplots(ncols=1,figsize=(5,5))\n",
    "sd.set_title(\"S.D. of Prediction from Different Samples \\n (mpg=10, weight=2000)\")\n",
    "sd.set_xlabel(\"alpha\")\n",
    "sd.set_xscale(\"log\")\n",
    "sd.plot(alphas, ols_std, label='OLS')\n",
    "sd.plot(alphas, ridge_std, label='Ridge')\n",
    "sd.plot(alphas, lasso_std, label='Lasso')\n",
    "sd.legend(loc=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Conclusion\n",
    "\n",
    "In this notebook, we have investigated how regularization can help reduce the variance of predictions. This ensures that the model we trained on a particular sample of data will be reasonably accurate when applied to other samples. \n",
    "\n",
    "A question that we have not yet touch on is, how do we choose the strength of regularization, ```alpha```? Conceptually, what we want to do is to pick a value that gives us the most accurate prediction when applied to other samples. In order to do so we would need to conduct out-of-sample test, which is the topic of the next notebook."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
