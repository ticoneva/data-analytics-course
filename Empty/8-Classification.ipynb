{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The need to group data arises naturally in many scenario. For example, a marketing officer would need to identify which group of customers are the most worthwhile to target, and a researcher might want to categorize households based on their sociodemographic background. \n",
    "\n",
    "Techniques used in grouping data are separated into two broad categories:\n",
    "- **Classification** - Group data based on examples you provide.\n",
    "- **Clustering** - Group data without any examples. \n",
    "\n",
    "The former is a form of *supervised learning*, while the latter is *unsupervised learning*. Which type of techniques to use depends on your need. In the case of marketing, perhaps you already have experience on which type of customers is the most profitable, so you will go for classification. On the other hand, when it comes to categorizing households, you might want the program to automatically separate the households into groups, which would be clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Classification\n",
    "Suppose you have the following data:\n",
    "\n",
    "| Customer |   Address   | Spending |\n",
    "|:--------:|:-----------:|:--------:|\n",
    "|     1    |   Central   |   High   |\n",
    "|     2    |  Admiralty  |   High   |\n",
    "|     3    | North Point |    Low   |\n",
    "|     4    |    Shatin   |   High   |\n",
    "|     5    |    Fo Tan   |    Low   |\n",
    "|     6    |  Ma On Shan |    Low   |\n",
    "\n",
    "And you need to predict the spending of the following customer:\n",
    "\n",
    "| Customer |   Address   | Spending |\n",
    "|:--------:|:-----------:|:--------:|\n",
    "|     7    |   Chai Wan  |     ?    |\n",
    "\n",
    "How should you do so? \n",
    "\n",
    "Before we proceed further, let us first go over the terminology commonly used in classification. In economics we usually call each data point an *observation*, with \"Spending\" being a *dependent variable* and \"Address\" an *independent variable*. In classification it is more common to call each data point a *sample*, with \"Spending\" being a *class variable* and \"Address\" a *feature*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "raw_data = [\n",
    "            [1,1,'Central',22.2819,114.1581,1],\n",
    "            [2,1,'Admiralty',22.2796,114.1655,1],\n",
    "            [3,0,'North Point',22.2871,114.1917,1],\n",
    "            [4,1,'Shatin',22.3771,114.1974,0],\n",
    "            [5,0,'Fo Tan',22.3969,114.1959,0],\n",
    "            [6,0,'Ma On Shan',22.4221,114.2324,0],\n",
    "            ]\n",
    "labels = ['customer','hi_spending','address','latitude','longitude','hk_island']\n",
    "data = pd.DataFrame.from_records(raw_data,columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Square\n",
    "There are multiple ways to approach this problem. As an economics major, the first technique that comes to mind is probably the ordinary least square (OLS). Why is OLS not suitable for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit\n",
    "\n",
    "A seasoned economist will likely use a logit regression to handle this task. First let us consider the district of each customer's address:\n",
    "\n",
    "| Customer |   Address   | Hong Kong Island | High Spending |\n",
    "|:--------:|:-----------:|:----------------:|:-------------:|\n",
    "|     1    |   Central   |         1        |       1       |\n",
    "|     2    |  Admiralty  |         1        |       1       |\n",
    "|     3    | North Point |         1        |       0       |\n",
    "|     4    |    Shatin   |         0        |       1       |\n",
    "|     5    |    Fo Tan   |         0        |       0       |\n",
    "|     6    |  Ma On Shan |         0        |       0       |\n",
    "|     7    |   Chai Wan  |         1        |       ?       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Get data\n",
    "\n",
    "\n",
    "#Train model\n",
    "\n",
    "\n",
    "#Predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question we might have from the above is, why is estimated probabilities not 1/3 and 2/3? This is because in data mining, constraints are added to to penalize extreme estimates. This technique is called *regularization*. Regularization is done to prevent overfitting, the phenomenon of closely fitting existing data but producing good predictions for unseen samples.\n",
    "\n",
    "If we tune down the regularization parameter in the logistic regression, we will get predictions closer to (1/3,2/3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with weak regularization (high C)\n",
    "\n",
    "\n",
    "#Predict\n",
    "print(\"Prediction (high spending = 1):\".ljust(35),logit.predict([[1]]))\n",
    "print(\"Pr(High) Pr(Low):\".ljust(35),logit.predict_proba([[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is in effect already categorized. What if we have latitudinal and longitudinal data instead?\n",
    "\n",
    "| Customer |   Address   | Latitude | Longitude | High Spending |\n",
    "|:--------:|:-----------:|:--------:|:---------:|:-------------:|\n",
    "|     1    |   Central   |  22.2819 |  114.1581 |       1       |\n",
    "|     2    |  Admiralty  |  22.2796 |  114.1655 |       1       |\n",
    "|     3    | North Point |  22.2871 |  114.1917 |       0       |\n",
    "|     4    |    Shatin   |  22.3771 |  114.1974 |       1       |\n",
    "|     5    |    Fo Tan   |  22.3969 |  114.1959 |       0       |\n",
    "|     6    |  Ma On Shan |  22.4221 |  114.2324 |       0       |\n",
    "|     7    |   Chai Wan  |   22.27  |   114.24  |       ?       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "\n",
    "\n",
    "#Test data\n",
    "\n",
    "\n",
    "#Train Model\n",
    "\n",
    "\n",
    "#Predict\n",
    "tw = 50\n",
    "print(\"Model Accuracy:\".ljust(tw),model.score(X2,y))\n",
    "print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan))\n",
    "print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan))\n",
    "print(\"Shatin Prediction (high spending = 1):\".ljust(tw),model.predict(X_Shatin))\n",
    "print(\"Shatin Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Shatin))\n",
    "print(\"North Point Prediction (high spending = 1):\".ljust(tw),model.predict(X_NorthPoint))\n",
    "print(\"North Point Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_NorthPoint))\n",
    "print(\"\")\n",
    "print(\"Fo Tan Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Fotan))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule\n",
    "The Bayes Rule says that\n",
    "\n",
    "$$\n",
    "P(y \\mid \\vec{x}) = \\frac{P(\\vec{x} \\mid y)P(y)}{P(\\vec{x})}\n",
    "$$\n",
    "Applying to our current problem,\n",
    "\n",
    "$$\n",
    "P(spending \\mid location) = \\frac{P(location \\mid spending)P(spending)}{P(location)}\n",
    "$$\n",
    "Our task is to pick a value for $spending$ that maximizes this probability:\n",
    "\n",
    "$$\n",
    "\\hat{spending} = \\underset{spending}{\\operatorname{argmax}}  \\left \\{ \\frac{P(location \\mid spending)P(spending)}{P(location)} \\right \\}\n",
    "$$\n",
    "\n",
    "Notice that $P(location)$ is constant for any given location, so we can eliminate it and get\n",
    "\n",
    "$$\n",
    "\\hat{spending} = \\underset{spending}{\\operatorname{argmax}}  \\left \\{ P(location \\mid spending)P(spending) \\right \\}\n",
    "$$\n",
    "\n",
    "To solve this maximization problem we need $P(location \\mid spending)$, and there are two common ways to get that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Native Bayes\n",
    "Native Bayes assumes that all the elements of $\\vec{x}$ are independent, so\n",
    "\n",
    "$$\n",
    "P(y \\mid \\vec{x}) = P(y \\mid x_1) \\cdot P(y \\mid x_2) \\cdot P(y \\mid x_3) ... \n",
    "$$\n",
    "\n",
    "Each $P(y \\mid x_i)$ is assumed to be normally distributed. The mean and standard deviations are estimated by the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Native Bayes\n",
    "from sklearn.naive_bayes import *\n",
    "model = GaussianNB()\n",
    "model.fit(X2,y)\n",
    "\n",
    "print(\"Model Accuracy:\".ljust(tw),model.score(X2,y))\n",
    "print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan))\n",
    "print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan))\n",
    "print(\"Shatin Prediction (high spending = 1):\".ljust(tw),model.predict(X_Shatin))\n",
    "print(\"Shatin Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Shatin))\n",
    "print(\"North Point Prediction (high spending = 1):\".ljust(tw),model.predict(X_NorthPoint))\n",
    "print(\"North Point Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_NorthPoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Linear Discriminant Analysis (LDA)\n",
    "LDA assumes that the features are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(X2,y)\n",
    "\n",
    "print(\"Model Accuracy:\".ljust(tw),model.score(X2,y))\n",
    "print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan))\n",
    "print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan))\n",
    "print(\"Shatin Prediction (high spending = 1):\".ljust(tw),model.predict(X_Shatin))\n",
    "print(\"Shatin Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Shatin))\n",
    "print(\"North Point Prediction (high spending = 1):\".ljust(tw),model.predict(X_NorthPoint))\n",
    "print(\"North Point Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_NorthPoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVD)\n",
    "\n",
    "SVD looks for a boundary that separate two classes while allowing for a buffer zone where mistakes are tolerated.\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_svm_margin_001.png\">\n",
    "Source: <a href=\"http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html\">scikit learn</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(probability=True)\n",
    "model.fit(X2,y)\n",
    "\n",
    "print(\"Model Accuracy:\".ljust(tw),model.score(X2,y))\n",
    "print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan))\n",
    "print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan))\n",
    "print(\"Shatin Prediction (high spending = 1):\".ljust(tw),model.predict(X_Shatin))\n",
    "print(\"Shatin Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Shatin))\n",
    "print(\"North Point Prediction (high spending = 1):\".ljust(tw),model.predict(X_NorthPoint))\n",
    "print(\"North Point Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_NorthPoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor\n",
    "Another method we could use is to look at samples that have similar characters as the one we are trying to predict. This method is called *nearest neighbor*.\n",
    "\n",
    "In the simpliest case, we will use the closest sample as a predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nearest Neigbhor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Only consider the nearest neighbor\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X2,y)\n",
    "\n",
    "print(\"Model Accuracy:\".ljust(tw),model.score(X2,y))\n",
    "print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan))\n",
    "print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan))\n",
    "print(\"Shatin Prediction (high spending = 1):\".ljust(tw),model.predict(X_Shatin))\n",
    "print(\"Shatin Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Shatin))\n",
    "print(\"North Point Prediction (high spending = 1):\".ljust(tw),model.predict(X_NorthPoint))\n",
    "print(\"North Point Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_NorthPoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike previous methods, when we try to predict the outcome of a pre-existing sample such as Shatin, we will get the correct answer. Naturally, this is because a pre-existing sample's closest neighbor is itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of neighbors is crucial. For example, suppose we use the three closest neighbors instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider the three closest neighbor\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X2,y)\n",
    "print(\"Model Accuracy:\".ljust(tw),model.score(X2,y))\n",
    "print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan))\n",
    "print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree\n",
    "Classification Tree repeatedly look for cutoffs that give the best prediction at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Tree\n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(X2,y)\n",
    "\n",
    "print(\"Model Accuracy:\".ljust(tw),model.score(X2,y))\n",
    "print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan))\n",
    "print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan))\n",
    "print(\"Shatin Prediction (high spending = 1):\".ljust(tw),model.predict(X_Shatin))\n",
    "print(\"Shatin Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Shatin))\n",
    "print(\"Fo Tan Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Shatin))\n",
    "print(\"North Point Prediction (high spending = 1):\".ljust(tw),model.predict(X_NorthPoint))\n",
    "print(\"North Point Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_NorthPoint))\n",
    "\n",
    "#Export tree structure to PNG format\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydot\n",
    "dotfile = StringIO()\n",
    "tree.export_graphviz(model, out_file=dotfile)\n",
    "pydot.graph_from_dot_data(dotfile.getvalue())[0].write_png(\"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the tree structure as shown in tree.png:\n",
    "\n",
    "<img src=\"http://www.ticoneva.com/econ/econ4130/images/8-tree.png\" width=\"300\">\n",
    "\n",
    "We can plot the cutoffs on a map to see how the tree works. This will take a couple of seconds to generate. Skip to the attached image below if you do not want to wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "lat = data['latitude'].values\n",
    "lon = data['longitude'].values\n",
    "\n",
    "#Map size\n",
    "padding = 0.15 # padding\n",
    "lat_min = min(lat) - padding\n",
    "lat_max = max(lat) + padding\n",
    "lon_min = min(lon) - padding\n",
    "lon_max = max(lon) + padding\n",
    "\n",
    "#Create map using Basemap\n",
    "plt.figure(figsize=(8,8))\n",
    "m = Basemap(llcrnrlon=lon_min,\n",
    "            llcrnrlat=lat_min,\n",
    "            urcrnrlon=lon_max,\n",
    "            urcrnrlat=lat_max,\n",
    "            lat_0=(lat_max - lat_min)/2,\n",
    "            lon_0=(lon_max-lon_min)/2,\n",
    "            resolution = 'h',\n",
    "            )\n",
    "m.drawcoastlines()\n",
    "m.drawmapboundary(fill_color='#46bcec')\n",
    "m.fillcontinents(color = 'white',lake_color='#46bcec')\n",
    "\n",
    "# plot points and cutoffs\n",
    "m.scatter(lons, lats, marker = 'o', color='r', zorder=5)\n",
    "m.plot([114.179,114.179],[lat_min,lat_max], 'k-', zorder=5)\n",
    "m.plot([114.197,114.197],[lat_min,lat_max], 'b-', zorder=5)\n",
    "m.plot([lon_min,lon_max],[22.4,22.4], 'g-', zorder=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated image should look the same as this one. The black, blue and green lines are the first, second and third cutoff respectively.\n",
    "\n",
    "<img src=\"http://www.ticoneva.com/econ/econ4130/images/8-map-tree.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Clustering\n",
    "\n",
    "Clustering algorithms group data without supervision. To do so, they minimize some measure of distance between data within the same group. For example, this could be simple distance as measured by the difference between values, or it could be variation as measured by variance.\n",
    "\n",
    "| Customer |   Address   | Latitude | Longitude |\n",
    "|:--------:|:-----------:|:--------:|:---------:|\n",
    "|     1    |   Central   |  22.2819 |  114.1581 |\n",
    "|     2    |  Admiralty  |  22.2796 |  114.1655 |\n",
    "|     3    | North Point |  22.2871 |  114.1917 |\n",
    "|     4    |    Shatin   |  22.3771 |  114.1974 |\n",
    "|     5    |    Fo Tan   |  22.3969 |  114.1959 |\n",
    "|     6    |  Ma On Shan |  22.4221 |  114.2324 |\n",
    "|     7    |   Chai Wan  |   22.27  |   114.24  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "raw_data2 = [\n",
    "            [1,'Central',22.2819,114.1581],\n",
    "            [2,'Admiralty',22.2796,114.1655],\n",
    "            [3,'North Point',22.2871,114.1917],\n",
    "            [4,'Shatin',22.3771,114.1974],\n",
    "            [5,'Fo Tan',22.3969,114.1959],\n",
    "            [6,'Ma On Shan',22.4221,114.2324],\n",
    "            [7,'Chai Wan',22.27,114.24]\n",
    "            ]\n",
    "labels = ['customer','address','latitude','longitude']\n",
    "data2 = pd.DataFrame.from_records(raw_data2,columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = data2[[\"latitude\",\"longitude\"]]\n",
    "\n",
    "#K-Means\n",
    "from sklearn.cluster import *\n",
    "\n",
    "#Two clusters\n",
    "\n",
    "\n",
    "#Three clusters\n",
    "\n",
    "\n",
    "#Four clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agglomerative Clustering\n",
    "ac = AgglomerativeClustering(n_clusters=2)\n",
    "y_ac = ac.fit_predict(X3)\n",
    "print(y_ac)\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=3)\n",
    "y_ac = ac.fit_predict(X3)\n",
    "print(y_ac)\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=4)\n",
    "y_ac = ac.fit_predict(X3)\n",
    "print(y_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the labelling is random."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
