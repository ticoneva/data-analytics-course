{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The need to group data arises naturally in many scenario. For example, a marketing officer might need to identify which group of customers are the most worthwhile to target, and a researcher might want to categorize households based on their sociodemographic background. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Classification\n",
    "Suppose you have the following data:\n",
    "\n",
    "| Customer |   Address   | Spending |\n",
    "|:--------:|:-----------:|:--------:|\n",
    "|     1    |   Central   |   High   |\n",
    "|     2    |  Admiralty  |   High   |\n",
    "|     3    | North Point |    Low   |\n",
    "|     4    |    Shatin   |   High   |\n",
    "|     5    |    Fo Tan   |    Low   |\n",
    "|     6    |  Ma On Shan |    Low   |\n",
    "\n",
    "And you need to predict the spending of the following customer:\n",
    "\n",
    "| Customer |   Address   | Spending |\n",
    "|:--------:|:-----------:|:--------:|\n",
    "|     7    |   Chai Wan  |     ?    |\n",
    "\n",
    "How should you do so? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Data\n",
    "raw_data = [\n",
    "            [1,1,'Central',22.2819,114.1581,1],\n",
    "            [2,1,'Admiralty',22.2796,114.1655,1],\n",
    "            [3,0,'North Point',22.2871,114.1917,1],\n",
    "            [4,1,'Shatin',22.3771,114.1974,0],\n",
    "            [5,0,'Fo Tan',22.3969,114.1959,0],\n",
    "            [6,0,'Ma On Shan',22.4221,114.2324,0],\n",
    "            ]\n",
    "labels = ['customer','hi_spending','address','latitude','longitude','hk_island']\n",
    "data = pd.DataFrame.from_records(raw_data,columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visual the data on a map of Hong Kong:\n",
    "\n",
    "<img src=\"../Images/8-map.png\" width=\"300\">\n",
    "\n",
    "This map is generated with the ```basemap``` library. You can find the corresponding script below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Square\n",
    "There are multiple ways to approach this problem. If you have learnt statistics, the first technique that comes to mind is probably the ordinary least square (OLS). Why is OLS not suitable for classification?\n",
    "\n",
    "It is unsuitable because OLS gives continuous predictions. If ```1``` is high income and ```0``` is low income, should we interpret a prediction of  ```2``` as *really high income*? Worse, how should we interpret a negative prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit\n",
    "\n",
    "Logistic regression assumes that \n",
    "$$\n",
    "P(y=1\\mid \\vec{x}) = \\frac{e^{\\vec{x}\\vec{\\beta}}}{1+e^{\\vec{x}\\vec{\\beta}}}\n",
    "$$\n",
    "\n",
    "\n",
    "First let us consider the district of each customer's address:\n",
    "\n",
    "| Customer |   Address   | Hong Kong Island | High Spending |\n",
    "|:--------:|:-----------:|:----------------:|:-------------:|\n",
    "|     1    |   Central   |         1        |       1       |\n",
    "|     2    |  Admiralty  |         1        |       1       |\n",
    "|     3    | North Point |         1        |       0       |\n",
    "|     4    |    Shatin   |         0        |       1       |\n",
    "|     5    |    Fo Tan   |         0        |       0       |\n",
    "|     6    |  Ma On Shan |         0        |       0       |\n",
    "|     7    |   Chai Wan  |         1        |       ?       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "y = data[\"hi_spending\"]\n",
    "X = data[\"hk_island\"].values.reshape(-1, 1)\n",
    "\n",
    "# Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question we might have from the above is, why is estimated probabilities not 1/3 and 2/3? This is because in data mining, constraints are added to to penalize extreme estimates. This technique is called *regularization*. Regularization is done to prevent overfitting, the phenomenon of closely fitting existing data but producing good predictions for unseen samples.\n",
    "\n",
    "If we tune down the regularization parameter in the logistic regression, we will get predictions closer to (1/3,2/3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression with weak regularization (high C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Performance Metrics\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "How should we evaluate the performance of the model? The simplest metrics is **accuracy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "Despite its simplicity, accuracy does not give a complete picture. \n",
    "Consider a certain procedure that can detect cancer, with the following results:\n",
    "\n",
    "|<a>                | Cancer Detected | Cancer Not Detected |\n",
    "|-------------------|-----------------|---------------------|\n",
    "| Really Have Cancer | 9               | 1                   |\n",
    "| No Cancer         | 100             | 900                 |\n",
    "    \n",
    "The procedure has an overall accuracy of 90%, but it is obviously not useful in detecting cancer because the vast majority of detection are *false positives*. \n",
    "\n",
    "To get a better picture of the usefulness of the procedure, we can calculate two additional metrics: *precision* and *recall*. \n",
    "- **Precision**: true positives / total positives\n",
    "- **Recall**: true positives / (true positives + false negatives)\n",
    "\n",
    "Wikipedia has a nice picture of these two metrics:\n",
    "<img width=\"250\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png\">\n",
    "\n",
    "No we can see why the procedure is bad---while the procedure has high recall ($9/(9+1)=90\\%$), it has low precision ($9/(9+100)=8.26\\% $). There are simply too many false positives.\n",
    "\n",
    "To calculate precision, use ```sklearn.metrics.precision_score```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, use ```sklearn.metrics.recall_score``` for recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two metrics happens to be the same as accuracy in our data, but as we have seen in the example above, it need not be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "For simiplicity, many studies report a single metrics called the **F1 score**, which is a weighted average of precision and recall:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\left( \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\right)\n",
    "$$\n",
    "\n",
    "To compute F1, use ```sklearn.metrics.f1_score```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The cancer detection table we have above is called a **confusion matrix**. It is often useful to generate such a table to get an idea on what is going on with the model. \n",
    "\n",
    "To generate the matrix use ```sklearn.metrics.confusion_matrix```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver operating characteristic (ROC) Curve\n",
    "\n",
    "When scikit-learn's model generates predictions by default it uses a threshold of 0.5, \n",
    "which means any sample with a estimated probability >= 0.5 will get a predicted value of 1.\n",
    "We might wonder if this threshold is ideal. \n",
    "A lower threshold means more samples will get a predicted value of 1, meaning that the number \n",
    "of true positives and the number of false positives will both go up.\n",
    "\n",
    "ROC curve gives us a graphical representation of this trade off. \n",
    "It plots the **True Positive Rates (TPR)** against **False Positive Rates (FPR)**.\n",
    "$$\n",
    "TPR = \\frac{\\text{true positives}}{\\text{true positives + false negatives}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "FPR = \\frac{\\text{false positives}}{\\text{false positives + true negatives}}\n",
    "$$\n",
    "\n",
    "ROC curve can be computed by using ```sklearn.metrics.roc_curve```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is simple enough that the default threshold of 0.5 is already the best. \n",
    "The thresholds used to plot the curve is stored in the third array returned by ```roc_curve```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under the Curve (AUC)\n",
    "\n",
    "Because ROC is a curve, it is made up of a series of numbers. \n",
    "To summarize the curve in a single number, we can compute the area under the curve,\n",
    "or **AUC** for short. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUC\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher AUC is the better: \n",
    "- In the best-case scenario, we get a 100% true positive rate regardless of the threshold, resulting in a AUC of **1**.\n",
    "- In the worst-case scenario, true positive rate equals false positive rate, resulting in a AUC of **0.5**.\n",
    "\n",
    "Wait, why is not the worse classification one that has false positive rate increasing much faster than true positive rate? Do you know why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Common Classification Methods\n",
    "\n",
    "Before we proceed, let us make the task more difficult.\n",
    "The above example is in effect already categorized. What if we have latitudinal and longitudinal data instead?\n",
    "\n",
    "| Customer |   Address   | Latitude | Longitude | High Spending |\n",
    "|:--------:|:-----------:|:--------:|:---------:|:-------------:|\n",
    "|     1    |   Central   |  22.2819 |  114.1581 |       1       |\n",
    "|     2    |  Admiralty  |  22.2796 |  114.1655 |       1       |\n",
    "|     3    | North Point |  22.2871 |  114.1917 |       0       |\n",
    "|     4    |    Shatin   |  22.3771 |  114.1974 |       1       |\n",
    "|     5    |    Fo Tan   |  22.3969 |  114.1959 |       0       |\n",
    "|     6    |  Ma On Shan |  22.4221 |  114.2324 |       0       |\n",
    "|     7    |   Chai Wan  |   22.27  |   114.24  |       ?       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going run the same tests for all the models in this notebook, it would be helpful to write a function for that so we will not need to type the same code repeatedly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate model accuracy and generate some predictions\n",
    "def summarize(model,data):\n",
    "    tw = 50 # title column width\n",
    "    \n",
    "    # Data\n",
    "    y = data[\"hi_spending\"]\n",
    "    X = data[[\"latitude\",\"longitude\"]]    \n",
    "    \n",
    "    # Accuracy\n",
    "    y_predict = model.predict(X)\n",
    "    print(\"Model Accuracy:\".ljust(tw),f1_score(y,y_predict))\n",
    "    \n",
    "    # Test data\n",
    "    X_ChaiWan = [[22.27,114.24]]\n",
    "    print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan)[0])\n",
    "    print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also write a function to highlight the decision boundaries with different colors. This function uses the ```basemap``` library, which you can install with ```conda install -c conda-forge basemap```. The function is a bit complicated, so it is okay if you do not understand how it works right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "if \"PROJ_LIB\" not in os.environ:\n",
    "    os.environ[\"PROJ_LIB\"] = \"../Others/\"\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def drawBoundaryOnMap(model,data):\n",
    "    \n",
    "    # Fetch latitudinal and longtitudinal data\n",
    "    lat = data['latitude'].values\n",
    "    lon = data['longitude'].values\n",
    "\n",
    "    # Map size\n",
    "    padding = 0.15 # padding\n",
    "    lat_min = min(lat) - padding\n",
    "    lat_max = max(lat) + padding\n",
    "    lon_min = min(lon) - padding\n",
    "    lon_max = max(lon) + padding\n",
    "\n",
    "    # 6x6 matplotlib figure\n",
    "    plt.figure(figsize=(6,6))\n",
    "\n",
    "    # Create map using Basemap\n",
    "    m = Basemap(llcrnrlon=lon_min,\n",
    "                llcrnrlat=lat_min,\n",
    "                urcrnrlon=lon_max,\n",
    "                urcrnrlat=lat_max,\n",
    "                lat_0=(lat_max - lat_min)/2,\n",
    "                lon_0=(lon_max-lon_min)/2,\n",
    "                resolution = 'h',\n",
    "                )\n",
    "    m.drawcoastlines()\n",
    "    m.drawmapboundary(fill_color='#46bcec')\n",
    "    m.fillcontinents(color = 'white',lake_color='#46bcec')\n",
    "\n",
    "    # Heatmap highlighting decision boundaries\n",
    "    xy = np.mgrid[lat_min:lat_max:0.001,lon_min:lon_max:0.001]\n",
    "    x_count = xy.shape[1]\n",
    "    y_count = xy.shape[2]\n",
    "    xy = xy.reshape(2,-1).T\n",
    "    a = model.predict(xy)\n",
    "    a = np.asarray(a).reshape((x_count,y_count))\n",
    "    m.imshow(a, cmap='PiYG', interpolation='nearest', \n",
    "               extent=(lon_min,lon_max,lat_min,lat_max),\n",
    "               alpha=0.8,zorder=5)\n",
    "    m.scatter(lon, lat, marker = 'o', color='r', zorder=6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the two functions on the logit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logit model will label anything within the purple area as low income and that within the green area as high income. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule\n",
    "The Bayes Rule says that\n",
    "\n",
    "$$\n",
    "P(y \\mid \\vec{x}) = \\frac{P(\\vec{x} \\mid y)P(y)}{P(\\vec{x})}\n",
    "$$\n",
    "Applying to our current problem,\n",
    "\n",
    "$$\n",
    "P(spending \\mid location) = \\frac{P(location \\mid spending)P(spending)}{P(location)}\n",
    "$$\n",
    "Our task is to pick a value for $spending$ that maximizes this probability:\n",
    "\n",
    "$$\n",
    "\\hat{spending} = \\underset{spending}{\\operatorname{argmax}}  \\left \\{ \\frac{P(location \\mid spending)P(spending)}{P(location)} \\right \\}\n",
    "$$\n",
    "\n",
    "Notice that $P(location)$ is constant for any given location, so we can eliminate it and get\n",
    "\n",
    "$$\n",
    "\\hat{spending} = \\underset{spending}{\\operatorname{argmax}}  \\left \\{ P(location \\mid spending)P(spending) \\right \\}\n",
    "$$\n",
    "\n",
    "To solve this maximization problem we need $P(location \\mid spending)$, and there are two common ways to get that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Native Bayes\n",
    "Native Bayes assumes that all the elements of $\\vec{x}$ are independent, so\n",
    "\n",
    "$$\n",
    "P( \\vec{x} \\mid y) = P(x_1 \\mid y) \\cdot P(x_2 \\mid y) \\cdot P( x_3 \\mid y) ... \n",
    "$$\n",
    "\n",
    "Each $P(x_i \\mid y)$ is assumed to be normally distributed. The mean and standard deviations are estimated by the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Gaussian Native Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Model here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Linear Discriminant Analysis (LDA)\n",
    "LDA assumes that the features are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Linear Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#Model here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVD)\n",
    "\n",
    "SVD looks for a boundary that separate two classes while allowing for a buffer zone where mistakes are tolerated.\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_svm_margin_001.png\">\n",
    "Source: <a href=\"http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html\">scikit learn</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine Classifier\n",
    "from sklearn.svm import SVC\n",
    "#Model here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Decision Tree repeatedly look for cutoffs that give the best prediction at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#Model here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export tree structure to PNG format\n",
    "import io\n",
    "import pydot\n",
    "import sklearn\n",
    "dotfile = io.StringIO()\n",
    "sklearn.tree.export_graphviz(dt, out_file=dotfile)\n",
    "pydot.graph_from_dot_data(dotfile.getvalue())[0].write_png(\"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above export the tree structure in an image filed named ```tree.png```. You might get a different try structure each time you run the script. Here is one example:\n",
    "\n",
    "<img src=\"http://www.ticoneva.com/econ/econ4130/images/8-tree.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor\n",
    "Another method we could use is to look at samples that have similar characters as the one we are trying to predict. This method is called *nearest neighbor*.\n",
    "\n",
    "In the simpliest case, we will use the closest sample as a predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nearest Neigbhor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Model here. Only consider the nearest neighbor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike previous methods, when we try to predict the outcome of a pre-existing sample such as Shatin, we will get the correct answer. Naturally, this is because a pre-existing sample's closest neighbor is itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of neighbors is crucial. For example, suppose we use the three closest neighbors instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider the three closest neighbor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model estimates that Chai Wan has 1/3 chance of being low income and 2/3 chance of being high income because out of the three Hong Kong Island districts in our training data, one has low income (North Point) and two have high income (Central and Admiralty).\n",
    "\n",
    "The following diagram sums up the estimates. The green circle indicates Chai Wan's closest neighbor, while the purple circle indicates the closest three neighbors.\n",
    "\n",
    "<img src=\"../Images/8-map-knn.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Clustering\n",
    "\n",
    "Clustering algorithms group data without supervision. To do so, they minimize some measure of distance between data within the same group. For example, this could be simple distance as measured by the difference between values, or it could be variation as measured by variance.\n",
    "\n",
    "| Customer |   Address   | Latitude | Longitude |\n",
    "|:--------:|:-----------:|:--------:|:---------:|\n",
    "|     1    |   Central   |  22.2819 |  114.1581 |\n",
    "|     2    |  Admiralty  |  22.2796 |  114.1655 |\n",
    "|     3    | North Point |  22.2871 |  114.1917 |\n",
    "|     4    |    Shatin   |  22.3771 |  114.1974 |\n",
    "|     5    |    Fo Tan   |  22.3969 |  114.1959 |\n",
    "|     6    |  Ma On Shan |  22.4221 |  114.2324 |\n",
    "|     7    |   Chai Wan  |   22.27  |   114.24  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "raw_data2 = [\n",
    "            [1,'Central',22.2819,114.1581],\n",
    "            [2,'Admiralty',22.2796,114.1655],\n",
    "            [3,'North Point',22.2871,114.1917],\n",
    "            [4,'Shatin',22.3771,114.1974],\n",
    "            [5,'Fo Tan',22.3969,114.1959],\n",
    "            [6,'Ma On Shan',22.4221,114.2324],\n",
    "            [7,'Chai Wan',22.27,114.24]\n",
    "            ]\n",
    "labels = ['customer','address','latitude','longitude']\n",
    "data2 = pd.DataFrame.from_records(raw_data2,columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X3 = data2[[\"latitude\",\"longitude\"]]\n",
    "\n",
    "#K-Means\n",
    "from sklearn.cluster import *\n",
    "\n",
    "#Two clusters\n",
    "\n",
    "\n",
    "#Three clusters\n",
    "\n",
    "\n",
    "#Four clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the labelling is random and could change between runs.\n",
    "\n",
    "A different clustering algorithm could give different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Agglomerative Clustering\n",
    "ac = AgglomerativeClustering(n_clusters=2)\n",
    "y_ac = ac.fit_predict(X3)\n",
    "print(y_ac)\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=3)\n",
    "y_ac = ac.fit_predict(X3)\n",
    "print(y_ac)\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=4)\n",
    "y_ac = ac.fit_predict(X3)\n",
    "print(y_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very nice diagram on scikit-learn's website showing the prediction of different clustering algorithms:\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
