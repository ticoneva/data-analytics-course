{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data\n",
    "\n",
    "Version: 2024-10-14\n",
    "\n",
    "Trying to construct models that understand text falls under the field of *natural language processing*. This is a field of enormous practical importance: chatbot, automated translation and generated new articles area few notable applications. In this notebook we will look into some basic ways of processing text data.\n",
    "\n",
    "Below is what you might get in a typical dataset of review data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text data\n",
    "corpus = [\n",
    "    \"This is good.\",\n",
    "    \"This is bad.\",\n",
    "    \"This is very good.\",\n",
    "    \"This is not good.\",\n",
    "    \"This is not bad.\",\n",
    "    \"This is...is bad.\"\n",
    "]\n",
    "\n",
    "ratings = [\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing review data the typical goal is to predict a single value, the rating, from the written text. In the case of chatbot and automated translation, where one single value is not sufficient to represent the meaning of text, a vector is outputed by the model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. N-gram\n",
    "\n",
    "Let us count the number of times each word appears in a sample. This is called *unigram* in natural language processing. To do so, we will use ```CountVectorizer``` of scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```get_feature_names_out()``` to see which word each column represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word-count vector can now be used with a suitable model to conduct language processing. Here we will simply use a logit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which phrases do our model have a difficulty understanding? Why might that be the case?\n",
    "\n",
    "Now let us take a look at the estimated coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the coefficients of each word. Can you see what is wrong with our model? One thing you might notice is that 'is' has a very negative coefficient while 'very' has very a positive coefficient, even though these words do not have such connotations themselves.  \n",
    "\n",
    "When we start counting combination of words instead of individual words, what we have is *n-gram*. ```CountVectorizer``` allows us to specify the range of words we wish to consider via the option ```ngram_range```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try running the logistic regression again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. IMDB Movie Review\n",
    "\n",
    "Now let us try something real. We will analyse a sample of <a href=\"https://www.imdb.com/\">IMDB</a> movie reviews, trying to predict the rating a user gives based on his written review. For speed reasons we will be using a subsample, but the original text data can be found <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">here</a>.\n",
    "\n",
    "First let us import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "imdb_train = pd.read_csv(\"../Data/imdb_train.csv\",\n",
    "                         names=['label','text'])\n",
    "imdb_test = pd.read_csv(\"../Data/imdb_test.csv\",\n",
    "                         names=['label','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many samples do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(imdb_train.shape)\n",
    "print(imdb_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inside each sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Words are encoded by their frequency-of-apperance ranking in the data. This allows us to easily delete words that either\n",
    "- appear frequently but add little to the meaning of the text (e.g. articles, conjunctions and prepositions), or\n",
    "- appear too infrequently to be of use.-->\n",
    "\n",
    "We will now repeat what we have done previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = imdb_train['label']\n",
    "y_test = imdb_test['label']\n",
    "\n",
    "# N-gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "print(model.score(x_train,y_train))\n",
    "print(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Lemmatization\n",
    "\n",
    "Consider the following corpus of text, modified from the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text data\n",
    "corpus2 = [\n",
    "    \"Apple is good.\",\n",
    "    \"Apple was bad.\",\n",
    "    \"Apples are good.\",\n",
    "    \"Apples were not good.\",\n",
    "    \"Apple is not bad.\",\n",
    "    \"Apples were...are bad.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having plurals complicates our analysis: `CountVectorizer` will treat 'Apple' and 'Apples' as two distinct words, unncessarily splitting the samples for apples. Similarly, 'is' and 'are' are both forms of the verb 'to be', so they should be considered as one word. What we need is *lemmatization*, which is the process of grouping together the inflected forms of a word for use in analysis.\n",
    "\n",
    "We will be using <a href=\"https://textblob.readthedocs.io/en/dev/index.html\">TextBlob</a>, a library for processing textual data. TextBlob in turn relies on <a href=\"http://www.nltk.org/\">NLTK</a> (short for *Natural Language ToolKit*) to do some of the heavy lifting. Since NLTK does not come with all packages installed, we will need to first download the ones we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process goes as follows:\n",
    "1. First convert each string to a `TextBlob` object. \n",
    "2. Split each string into sentences with the `.sentences` property if needed.\n",
    "3. Split each string (or sentence) into words with the `.words` property.\n",
    "4. Lemmatize each word with the `lemmatize()` method. \n",
    "\n",
    "Note that `lemmatize()` expects words to be in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TextBlob to lemmatize the corpus\n",
    "from textblob import TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above successfully grouped 'apples' with 'apple', but it failed to group 'is' and 'are'. The second sample gives us some hint as to what went wrong---'was' was somehow converted to 'wa'. What happened was that `lemmatize()` by default treats all words as nouns. To ensure proper conversion, we will need to provide it with each word's part of speech (POS).\n",
    "\n",
    "First, we generate part-of-speech tags by using the `.tags` property of the `TextBlob` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Penn Treebank POS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then providing `lemmatize()` with part-of-speech tags. Unfortunately it is not as simple as passing the POS tags from above. The reason is that NLTK generates tags base on the <a href=\"https://catalog.ldc.upenn.edu/LDC99T42\">Penn Treebank</a> corpus, which uses different <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">POS</a> tags than the <a href=\"https://wordnet.princeton.edu/documentation/wndb5wn\">Wordnet</a> corpus that `lemmatize()` is based on. \n",
    "\n",
    "We therefore need to map the two POS systems before lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map Penn Treebank POS to Wordnet POS\n",
    "\n",
    "# Convert Penn Treebank POS to Wordnet POS\n",
    "\n",
    "# Lemmatize with POS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob and NLTK have many other useful features such as spelling correction and translation that you can explore on your own. One particularly useful feature is pre-trained sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis with TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Chinese Text\n",
    "\n",
    "One major issue with Chinese text is that there is no space between words. Unsurprisingly then, this is a major focus for Chinese natural language processing research.\n",
    "\n",
    "They are multiple libraries for Chinese NLP. Here we will try out `jieba` and `pkuseg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = '我愛吃北京餃子。'\n",
    "\n",
    "# jieba default\n",
    "\n",
    "\n",
    "# jieba + paddle\n",
    "\n",
    "\n",
    "# pkuseg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are much easier once we have the individual words. For example, we could immediately use ngram on the text.\n",
    "\n",
    "We can also fetch POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba\n",
    "\n",
    "\n",
    "# jieba + paddle\n",
    "\n",
    "\n",
    "# pkuseg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Neural Network\n",
    "\n",
    "Below is a simple LSTM neural network model that runs sentiment analysis on the IMDB data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.utils import resample\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train,y_train,x_test,y_test = resample(x_train,y_train,x_test,y_test,\n",
    "                                         n_samples=1000)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that training a neural network is several orders of magnitude slower than a n-gram model. Furthermore, the neural network model above is not more accurate than our simple n-gram model. One reason is that with so many parameters, neural network models need more than a thousand sample to achieve good results if you are training one from scratch. You can try running the same script with more data on a computer with GPU and see whether you get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Large Language Models\n",
    "\n",
    "A much better way to incorporate a neural network models is to use a *pre-trained* language model, \n",
    "which has been trained to understand language based on an enormous amount of text data.\n",
    "The main reason why the models we have tried so far do not work well is that they have to learn\n",
    "English from scratch based on the relatively small number of samples we provide. \n",
    "The use of a pre-trained model circumvent this issue. \n",
    "\n",
    "We will go into the details of such pre-trained models in a later lecture.\n",
    "Here we will simply have a demo. As running these models are very computationally intensive,\n",
    "you should run the following code with access to GPU, otherwise it is going to be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with GPU! This is very slow on CPU.\n",
    "\n",
    "# Data\n",
    "x_test = imdb_test['text'].tolist()\n",
    "y_test = imdb_test['label'].tolist()\n",
    "\n",
    "# Use a pre-trained text classifier through Hugging Face transformer library\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', device=0) # device=0 means use (first) GPU\n",
    "tokenizer_kwargs = {'truncation':True,'max_length':512} # Truncate to 512 tokens\n",
    "results = classifier(x_test,**tokenizer_kwargs) # Returns a text label\n",
    "y_predicted = [ 1 if x['label']=='POSITIVE' else 0 for x in results] # Convert to 1 or 0\n",
    "\n",
    "# Measure accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Accessing Language Models through API\n",
    "\n",
    "Instead of running your own copy pf a pre-trained language model,\n",
    "you can defer the task to a remotely-hosted model. The most well-known\n",
    "remotely-hosted models are OpenAI's GPTs and Anthropic's Claude.\n",
    "Here we will use Meta's Llama 3.1, hosted on CUHK Department of Economics'\n",
    "servers. \n",
    "\n",
    "Note that while this is the most powerful and convenient method of conducting \n",
    "text classification, it is also the slowest&mdash;you pay a price for being able \n",
    "to give the model instuctions in plain language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification through API\n",
    "\n",
    "# Need to supply your own API key\n",
    "api_key = 'your-key-here'\n",
    "\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Data - using only 30 samples for speed reasons\n",
    "x_test = imdb_test['text'].head(30).tolist()\n",
    "y_test = imdb_test['label'].head(30).tolist()\n",
    "\n",
    "# OpenAI API\n",
    "client = OpenAI(\n",
    "    base_url = 'https://scrp-chat.econ.cuhk.edu.hk/api',\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "# Function for running the inference\n",
    "def f(x):\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"llama3.1:8b-instruct-q5_K_M\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"\"\"Please classify if the given text's sentiment is positive or negative.\n",
    "                       If it is positive, return 1. Otherwise return 0. \n",
    "                       Show only the finally answer, do not show your reasoning.\n",
    "                    \"\"\"},        \n",
    "        {\"role\": \"user\", \"content\": x}\n",
    "      ],\n",
    "      temperature=0  \n",
    "    )\n",
    "    return int(response.choices[0].message.content)\n",
    "\n",
    "# Loop through data\n",
    "y_predicted = [f(x) for x in x_test]\n",
    "\n",
    "# Measure accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Readings\n",
    "- <a href=\"https://github.com/dipanjanS/text-analytics-with-python\">Text Analytics with Python</a> (or the <a href=\"https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\">free tutorial</a> by the same author on Towards Data Science.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
