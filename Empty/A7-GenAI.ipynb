{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301b2556-3c62-4c06-9ae6-0ee028c660ea",
   "metadata": {},
   "source": [
    "# Accessing a Generative AI Model through OpenAI API\n",
    "\n",
    "Version: 2025-10-13\n",
    "\n",
    "Generative AI models are machine learning models that are capable of generating content.\n",
    "This notebook is going to focus on the most commonly-used type of generative AI models&mdash;the \n",
    "ones that generate text. These models are called *large language models*, or 'LLM' for short.\n",
    "We will not be going into the technical details of how a LLM works at this moment, but rather focus\n",
    "on how to use an LLM as an end user.\n",
    "\n",
    "## A. Accessing an LLM through Python\n",
    "Although it is very convenient to access an LLM through a browser interface for one-off task, \n",
    "for repetitive task you will generating want to access the model using a program. Model providers \n",
    "will generally provide such access through an *application programming interface*, or 'API' for short.\n",
    "An API defines what sorts of interactions are possible with the model, and how input and output data \n",
    "should look like. \n",
    "\n",
    "The API used by OpenAI, the provider of ChatGPT, is the de facto standard supported by most models.\n",
    "\n",
    "The first step is to set up an OpenAI client. The client requires two piece of information:\n",
    "1. `base_url`: this is the web address of the model provider. If you use OpenAI's model, this can be omited.\n",
    "2. `api_key`: this is a string of text unique to your user account with the model provider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff1848-d882-403f-888d-3799ef0bf34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03fd7e0-4945-4c8a-b480-6898c797bd8a",
   "metadata": {},
   "source": [
    "Note that whoever has your API key has access to your AI service account. In a production environment, it is therefore best to save the API key as an environment variable, so that it will not be saved in your notebook. \n",
    "\n",
    "The easiest way to do so when working in a Jupyter notebook is to use the `dotenv` library to load environment variables from a file. The library defaults to loading the variables from a file named `.env` in your current working directory, but you can load from any file by providing a suitable path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa489c-056f-4e24-ae3a-32ab7946d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the scrp-chat.env file containing environment variables.\n",
    "# Default is to look for a file named .env in the current working directory.\n",
    "\n",
    "\n",
    "# The API key should be saved as an environment variable OPENAI_API_KEY\n",
    "# The base_url should be saved as OPENAI_BASE_URL in the same file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd43909-469a-4975-9980-b1a4e1d1660b",
   "metadata": {},
   "source": [
    "## B. Model List\n",
    "\n",
    "To see what models the provider support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d76bd6-d461-41e9-aaa7-d7403713eec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e206c5-cc8c-46b9-8297-f63097b21ce7",
   "metadata": {},
   "source": [
    "## C. Calling the Model\n",
    "\n",
    "To call a model, use \n",
    "```python\n",
    "response = client.chat.completions.create(...)\n",
    "```\n",
    "You will need to provide a few pieces of information:\n",
    "- `model`: the name of the model.\n",
    "- `messages`: a list containing the conversation history. Each message should be dictionary with a `role` and a `content`. `role` can be one of:\n",
    "    - `system`: system prompt. Use this to give the model general guidelines that it should follow strictly.\n",
    "    - `user`: prompt entered by the user. \n",
    "    - `assistant`: response from the model. \n",
    "    - `tool`: response from tools the model can use.\n",
    "- Additional settings such as temperature and number of samples:\n",
    "    - `temperature`: Adjust the randomness of the response. 0 is the lowest setting.\n",
    "      Default is model specfic, around 0.6-0.8.\n",
    "    - `n`: number of responses you want the model to generate. Default is 1.\n",
    "\n",
    "The response will be recorded in\n",
    "```python\n",
    "response.choices[0].message.content\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e23de-39e8-43a4-8171-29c645e69996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "client = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3c5b33-7762-4625-aecb-e5d5d140566f",
   "metadata": {},
   "source": [
    "## D. Example: Scrapping Webpage and Extract Information with LLM\n",
    "\n",
    "We first scrape the webpage with Selenium + Beautiful Soup,\n",
    "but unlike before, we do not manually find the elements we need.\n",
    "Instead, we export a stripped version of the webpage's source code,\n",
    "then uses an LLM to extract the information we need. \n",
    "\n",
    "Because LLMs can only process a limited amount of text input, called \n",
    "*context length*, and slows down as the context length goes up, \n",
    "it is usually wise to use Beautiful Soup to locate\n",
    "as precious as possible what you need, before passing the source code\n",
    "to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69785aa2-92dc-4c8e-8647-88ff2a14d216",
   "metadata": {},
   "source": [
    "### D1. Fetch the Page Source in Full\n",
    "\n",
    "Scrape the webpage in full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48133c6-545a-4fa0-ba8a-e049eda315eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only works with static content.\n",
    "# See A6-Data-Scrping-Completed for a version that works with dynamic content.\n",
    "import requests\n",
    "def get_page_source(url):\n",
    "    page = requests.get(url)\n",
    "    return page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ffea03-c586-4daa-92bb-451fbcb5d6c1",
   "metadata": {},
   "source": [
    "Now we feed the source code to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c243c3-ed62-4837-a169-b2ac2e6cd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80410e65-5d5f-4091-92ba-d41e55b19d10",
   "metadata": {},
   "source": [
    "### D2. Strip the Page Source\n",
    "\n",
    "You can strip white space by setting `strip = True` in `get_page_source()`, \n",
    "but it appears to drastically lower the quality of the LLM's output.\n",
    "The likely reason for this is that during training, the LLM has learnt to \n",
    "use the leading and trailing white space in understanding the content. \n",
    "Removing the whitespace therefore make the content harder to understand,\n",
    "just like it would be for an actual human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442ecee-cda6-4c92-9ce5-82c69af901f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page source with white space stripped\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_stripped_page_source(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    return soup.get_text(strip=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca3a66-c255-41ab-8498-dc18340662c8",
   "metadata": {},
   "source": [
    "Let us take a look at the stripped source code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24db9ad-3750-46a3-b72c-a1c43a065617",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_source = get_stripped_page_source('http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/20151213/ST')\n",
    "page_source[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5658970-08d1-4339-9750-7fef836afb88",
   "metadata": {},
   "source": [
    "Now let us provide the page source to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81755146-1f7f-407f-9ac4-d3fd299e9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "page_source = get_stripped_page_source('http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/20151213/ST',\n",
    "                              strip=True)\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "start = time.time()\n",
    "response = client.chat.completions.create(\n",
    "  model=\"default\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"The following webpage contains horse racing results. \n",
    "                                Please extract the result table and return it in comma-delimited\n",
    "                                CSV format. No explanation is needed.\n",
    "                                {page_source}\n",
    "                                \"\"\"},\n",
    "  ],\n",
    ")\n",
    "output = response.choices[0].message.content\n",
    "print(\"Processing took: {:.2f}\".format(time.time() - start))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406271e-0726-4a5d-be16-e4c211093c3c",
   "metadata": {},
   "source": [
    "### D3. Target Specific Elements\n",
    "\n",
    "To speed up the processing and make the model's output more accurate, you can use Beauitful Soup to locate the table, and provide only the source code of the table to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9609d26e-d36e-4d2f-88b5-3591cb836fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_table_source(url):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf98104-4828-49c8-97fe-9ceb382c4202",
   "metadata": {},
   "source": [
    "Let us try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8c9c2-b97a-49f4-8d38-48b9c5fb3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_source = get_table_source('http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/20151213/ST',\n",
    "                              strip=True)\n",
    "start = time.time()\n",
    "response = client.chat.completions.create(\n",
    "  model=\"default\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"The following webpage contains horse racing results. \n",
    "                                Please extract the result table and return it in comma-delimited\n",
    "                                CSV format. No explanation is needed.\n",
    "                                {page_source}\n",
    "                                \"\"\"},\n",
    "  ],\n",
    ")\n",
    "output = response.choices[0].message.content\n",
    "print(\"Processing took: {:.2f}\".format(time.time() - start))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa751c-3734-489f-95ee-15f77c988107",
   "metadata": {},
   "source": [
    "Finally, we write the output to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae1ba1-2ad7-4150-9b2b-3c1651b26031",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result.csv\", \"w\") as f:\n",
    "  f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3f309-01ff-4d4d-8ba6-74c47208ac51",
   "metadata": {},
   "source": [
    "### E. Outputing to File\n",
    "\n",
    "Because the LLM has done the formatting, we directly write\n",
    "the output to file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b355f1-4277-4988-aa72-7782e1a9b024",
   "metadata": {},
   "source": [
    "### F. Scrape Multiple Pages\n",
    "\n",
    "We can put everything together in one function\n",
    "and use it in a loop to save the information we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad9173-f885-4a57-b560-3bcf152c22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape results\n",
    "\n",
    "def scrape_webpage(url, prompt):\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf2993-9d6b-4521-bd90-a68834c67db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first part of the URL of data source\n",
    "url_front = \"http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/\"\n",
    "prompt = \"\"\"The following webpage may contain horse racing results in a table. \n",
    "            If it does, please extract the result table and return it in \n",
    "            comma-delimited CSV format. If not, just return an empty string.\n",
    "            No explanation is needed.\n",
    "            \"\"\"\n",
    "\n",
    "# Copy the loop from above and incorporate the csv-saving code\n",
    "for year in range(2017,2018):\n",
    "    for month in range(1,2):\n",
    "        for day in range(1,15):\n",
    "            \n",
    "            # Convert month and day to 2-digit representation\n",
    "            month_2d = '{:02d}'.format(month)\n",
    "            day_2d = '{:02d}'.format(day)\n",
    "            \n",
    "            # Full URL of data source\n",
    "            url = url_front + str(year) + month_2d + day_2d\n",
    "            \n",
    "            # Print the URL so we know the progress so far\n",
    "            print(\"Trying:\",url)\n",
    "            \n",
    "            # Call our function to fetch and process data given the URL\n",
    "            output = scrape_webpage(url, prompt)\n",
    "            \n",
    "            # Only save if there is something in content\n",
    "            if len(output) > 0:\n",
    "                filepath = str(year)+month_2d+day_2d+\".csv\"\n",
    "                \n",
    "                # Save to file\n",
    "                with open(filepath, \"w\") as f:\n",
    "                    f.write(output)\n",
    "                    print(f\"{filepath} saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
