{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a78b9572",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "Recurrent neural networks (RNN) have autoregressive layers. They are suitable for natural language processing (NLP) and time series modelling, though for the former they have been supplanted by Transformer-based models in recent years.\n",
    "\n",
    "Several things to note when using RNN:\n",
    "1. **Samples must have the same number of features**. \n",
    "   Truncate or pad each sample as needed.\n",
    "2. **RNN is slow to train**. \n",
    "   Start with a small subsample of data\n",
    "   move to the full data only after you verify that your model is working.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6817c071",
   "metadata": {},
   "source": [
    "## Text Data\n",
    "\n",
    "We will use the IMDB to illustrate how to use a RNN for natural language processing. \n",
    "\n",
    "### A. Load Data\n",
    "\n",
    "We first load the IMDB data then process it. The two most important processing we will apply are:\n",
    "1. *How many unique words to keep?* Words that are too infrequent should be ignored because there will not be enough data to figure out their meaning. All such words will be converted to a special out-of-vocabulary character.\n",
    "2. *How many features?* In the context of NLP, this translates to how many words each sample are allowed to have. Longer sequences are truncated while shorter ones are padded with a special character, usually `0`.\n",
    "\n",
    "We will also take a random subsample of data to speed up training in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
    "\n",
    "# Resample\n",
    "x_train,y_train,x_test,y_test = resample(x_train,y_train,\n",
    "                                         x_test,y_test,\n",
    "                                         n_samples=1000)\n",
    "\n",
    "# Data processing\n",
    "\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88286679",
   "metadata": {},
   "source": [
    "### B. Model\n",
    "\n",
    "Now we build our model. The model has the following structure:\n",
    "\n",
    "1. Input\n",
    "2. Embedding layer\n",
    "3. Recurrent layers\n",
    "4. Fully-connected layers\n",
    "5. Output\n",
    "\n",
    "An embedding layer translates each word into a vector, allowing much richer representation of the meaning of each word than just a single number. The initial translation is random, but the layer will learn through back propagation just like any other layer in the model.\n",
    "\n",
    "#### Standard RNN\n",
    "\n",
    "First, let us try standard RNN using Keras' `SimpleRNN` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c31459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171944c",
   "metadata": {},
   "source": [
    "#### Long-Short Term Memory (LSTM)\n",
    "\n",
    "Next we will try out the LSTM layer. We simply need to change `SimpleRNN` to `LSTM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc0654",
   "metadata": {},
   "source": [
    "#### Bidirectional RNN\n",
    "\n",
    "Finally, let us try out bi-directional LSTM. This can be done by enclosing the LSTM layer with `Bidirectional()`. \n",
    "\n",
    "Note that this will *double* the number of neurons in the targeted layer. Not only does this make training slower, it is also not a fair comparison with above because the number of parameters have increased by a lot. You should cut the number of neurons if you want a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47072400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08e428",
   "metadata": {},
   "source": [
    "### C. Hyperparameters Tuning\n",
    "\n",
    "Hyperparameter tuning is necessary in order to get good performance.\n",
    "The major hyperparameters you need to consider are:\n",
    "- Size of the embedding (i.e. How long a vector do you need to represent each word?)\n",
    "- Number of recurrent neurons\n",
    "- Number of recurrent layers\n",
    "- Number of fully-connected neurons\n",
    "- Number of fully-connected layers\n",
    "- Dropout rate\n",
    "- Optimzer\n",
    "- Number of epochs\n",
    "\n",
    "Putting everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Embedding\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "\n",
    "# Settings\n",
    "max_features = 20000 # How many words to keep?\n",
    "maxlen = 80  # cut texts after this number of words\n",
    "n_samples = 1000 # Running with full data takes a lot of time\n",
    "batch_size = 32\n",
    "\n",
    "# Load data\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "if (n_samples != None):\n",
    "    x_train,y_train,x_test,y_test = resample(x_train,y_train,\n",
    "                                             x_test,y_test,\n",
    "                                             n_samples=n_samples)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "# Data processing\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "# Model\n",
    "print('Build model...')\n",
    "inputs = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, 128)(inputs)\n",
    "x = Bidirectional(LSTM(128, dropout=0.2))(x)\n",
    "x = Dense(128)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Training\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11af3c",
   "metadata": {},
   "source": [
    "## Time Series Data\n",
    "\n",
    "In this part will use RNN to predict stock index. \n",
    "\n",
    "First, load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import data\n",
    "hsi = pd.read_csv(\"../Data/hsi.csv\")\n",
    "hsi = hsi.dropna()\n",
    "hsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069479bd",
   "metadata": {},
   "source": [
    "Next, we need to process our data. For each target $y_t$, the corresponding features are \n",
    "$x_t = \\left[ y_{t-1},y_{t-2},...,y_{t-n} \\right]$.\n",
    "We can generate this by using pandas' `df.shift()` or Keras' `timeseries_dataset_from_array()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453da74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "\n",
    "data = hsi[\"Close\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8c3ad",
   "metadata": {},
   "source": [
    "Finally, our model. Two main difference when compared with NLP modelling:\n",
    "1. No embedding layer. It is usually impossible to put one in because time series data is often continuously distributed.\n",
    "2. No bidirectional layer. A bidirectional layer has access to both the past and the future, the latter we have no access to when it comes to actual inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
