{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Network\n",
    "\n",
    "Neural network is fundamentally numeric computation, so any software with decent numeric computation capabilities can be used to construct and train a neural network. That said, while in theory you can construct a neural network in Excel, in practice it will be very troublesome since Excel is not designed with neural network in mind. Libraries are that specifically geared toward neural network include:\n",
    "- Google's <a href=\"https://www.tensorflow.org/\">Tensorflow</a>\n",
    "- Microsoft's <a href=\"https://github.com/Microsoft/CNTK\">CNTK</a>\n",
    "- Facebook's <a href=\"http://pytorch.org/\">PyTorch</a> and <a href=\"https://caffe2.ai/\">Caffe2</a>\n",
    "- Intel's <a href=\"https://ai.intel.com/neon/\">neon</a>\n",
    "- <a href=\"http://deeplearning.net/software/theano/\">Theano</a> and <a href=\"http://caffe.berkeleyvision.org/\">Caffe</a>\n",
    "\n",
    "In this course we will focus on using <a href=\"https://keras.io/\">```keras```</a>, which is a high-level library for constructing neural networks. Keras runs on top of a numerical computation library of your choice, defaulting to ```tensorflow```. A library such as Keras significantly simplify the workflow of constructing and training neural networks. \n",
    "\n",
    "<img src=\"http://www.ticoneva.com/econ/econ4130/images/nn_libraries.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example: Binary Neural Network Classifier\n",
    "\n",
    "As a first example, we will train a neural network to the following classification task:\n",
    "\n",
    "|y|x1|x2|\n",
    "|-|-|-|\n",
    "|0|1|0|\n",
    "|1|0|1|\n",
    "\n",
    "To be clear: there is absolutely no need to use neural network for such as simple task. A simpler model will train a lot faster and potentially with better accuracy.\n",
    "\n",
    "We first generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Generate 2000 samples. [1,0] -> 0, [0,1] -> 1\n",
    "X = np.repeat([[1,0]], 1000, axis=0)\n",
    "y = np.repeat([0], 1000, axis=0)\n",
    "X = np.append(X,np.repeat([[0,1]], 1000, axis=0),axis=0)\n",
    "y = np.append(y,np.repeat([1], 1000, axis=0),axis=0)\n",
    "\n",
    "#Shuffle and split data into train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a neural network classifier. Below is the simplest neural network one can come up with, with only one hidden neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Set up layers \n",
    "\n",
    "\n",
    "# Set up model\n",
    "\n",
    "\n",
    "#start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-sample test can be conducted with ```model.evaluate()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number is the model's loss while the subsequent numbers are the metrics we specified. In our case, they are ```binary_crossentropy``` and ```accuracy``` respectively.\n",
    "\n",
    "Unlike OLS, a neural network's performance could vary across runs. Run the code a few more times and see how the performance vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction (this is called *inference* in machine learning) with ```model.predict()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "Different activation can have profound impact on model performance. Besides ```sigmoid```, which is just a different name for the logistic function, there are other activation function such as ```tanh``` and ```relu```. ```relu```, which stands for **RE**ctified **L**inear **U**nit, is a particular common choice due to its good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Replace 'sigmoid' with 'relu' for the hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Regression\n",
    "\n",
    "Next we are going use a neural network in a regression task. The true data generating process (DGP) is as follows:\n",
    "\n",
    "$$\n",
    "y = x^5 -2x^3 + 6x^2 + 10x - 5\n",
    "$$\n",
    "\n",
    "The model does not know the true DGP, so it needs to figure out the relationship between $y$ and $x$ from the data.\n",
    "\n",
    "First we generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 1000 samples\n",
    "X = np.random.rand(1000,1)\n",
    "y = X**5 - 2*X**3 + 6*X**2 + 10*X - 5\n",
    "\n",
    "#Shuffle and split data into train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layers\n",
    "\n",
    "\n",
    "#Model\n",
    "\n",
    "\n",
    "#Training\n",
    "\n",
    "\n",
    "#Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to run the model through different settings. The function contains everything we have coded previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyNN(data,hidden_count=100,epochs=200):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    \n",
    "    #Layers\n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(hidden_count, activation='relu')(inputs)\n",
    "    predictions = Dense(1, activation='linear')(x)\n",
    "\n",
    "    #Model\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mean_squared_error')\n",
    "    model.fit(X_train,y_train,epochs=epochs,verbose=0) #Do not display progress\n",
    "    print(\"Hidden count:\",str(hidden_count).ljust(5),\n",
    "          \"Parameters:\",str(model.count_params()).ljust(8),\n",
    "          \"loss:\",model.evaluate(x=X_test,y=y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily try out different settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = X_train, X_test, y_train, y_test\n",
    "data = train_test_split(X,y)\n",
    "\n",
    "#Try various neuron count\n",
    "polyNN(data,hidden_count=1)\n",
    "polyNN(data,hidden_count=10)\n",
    "polyNN(data,hidden_count=50)\n",
    "polyNN(data,hidden_count=100)\n",
    "polyNN(data,hidden_count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the universal approximation theory in work: the more neurons we have the better the fit.\n",
    "\n",
    "One trick that can often improve performance: *standardizing* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Standardize X\n",
    "\n",
    "\n",
    "#Run the model again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "*Deep learning* is the stacking of multiple layers of neurons. Holding the number of parameters constant, this often performs better than having only a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function polyDNN() that have a variable number of layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the model through various settings. I have chosen the neuron count such that the number of parameters is roughly the same as in the single-layer case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyDNN(data,hidden_count=1,layers=2)\n",
    "polyDNN(data,hidden_count=10,layers=2)\n",
    "polyDNN(data,hidden_count=15,layers=2)\n",
    "polyDNN(data,hidden_count=36,layers=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
