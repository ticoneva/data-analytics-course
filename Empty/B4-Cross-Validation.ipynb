{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "In the previous notebook, we see that regularization is crucial to training a good model. The strength of regularization is controlled by a **hyperparameter** ```alpha```. How should we pick such hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # organize data\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso # regressions\n",
    "import numpy as np # calculate mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data. \n",
    "auto = pd.read_csv(\"../Data/auto.csv\")\n",
    "\n",
    "# Check data\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. A simple out-of-sample test\n",
    "Let's start with a simple out-of-sample test: we will divide our data into two parts, one for training the model and the other for testing the model's out-of-sample performance. The former is commonly called **training set** while the latter **test set** or **holdout set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick variables\n",
    "\n",
    "\n",
    "# Use about 3/4 of data for training: 60 samples\n",
    "\n",
    "\n",
    "# In-sample data for training model\n",
    "\n",
    "\n",
    "# Out-of-sample data for testing model\n",
    "\n",
    "\n",
    "# Train Ridge model and show R-Squared values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's loop through different values of alpha and see how it affects the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphas to go through\n",
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000,500000000]\n",
    "\n",
    "# Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While regularization helps us get more consistent performance, our model simply isn't really good. What could be the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Shuffling data\n",
    "\n",
    "If the data is sorted, splitting the data sequentially would give us unrepresentative sets of data. To deal with that, we can shuffle our data before splitting it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function for shuffling\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Shuffle observations\n",
    "\n",
    "\n",
    "# Use about 3/4 of data for training: 60 samples\n",
    "\n",
    "\n",
    "# In-sample data for training model\n",
    "\n",
    "\n",
    "# Out-of-sample data for testing model\n",
    "\n",
    "\n",
    "# Train Ridge model with different alphas and show R-Squared values\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)\n",
    "    print(str(a).ljust(10), # left-justified, width=10\n",
    "          str(round(ridge.score(x_in,y_in),2)).ljust(8), # left justified, width=5\n",
    "          str(round(ridge.score(x_out,y_out),1)).rjust(5)) # right justified, width=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you shuffle your data before splitting it? It depends. For cross-section data it is probably a good idea, but for time-series data it would be a bad idea, since you are introducing *hindsight bias* if you can train with data that is generated after some of your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. train_test_split\n",
    "\n",
    "In practice, you will probably use sckit-learn's ```train_test_split``` method to split the data. ```train_test_split``` shuffles the data by default, so there is no need to call ```shuffle``` separately. The default is a 75/25 split, which you can change by providing a different ```train_size``` or ```test_size```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "y_in,y_out,x_in,x_out = train_test_split(auto[\"price\"],\n",
    "                                         auto[[\"mpg\",\"weight\"]],\n",
    "                                         train_size=0.8)\n",
    "\n",
    "# Train Ridge model with different alphas and show R-Squared values\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)\n",
    "    print(str(a).ljust(10),\n",
    "          str(round(ridge.score(x_in,y_in),2)).ljust(8),\n",
    "          str(round(ridge.score(x_out,y_out),1)).rjust(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Validation\n",
    "\n",
    "So we try out different values of ```alpha``` and pick the one that give us the highest out-of-sample score. Do so is actually problematic: since ```alpha``` is a parameter of our model, we are effectively training our model with the supposingly out-of-sample data, which means the test set no longer gives us truly out-of-sample results. In particular, there is a real chance of overfitting our model to the test set via ```alpha```.\n",
    "\n",
    "\n",
    "The correct approach is to split the data into three parts: besides the train set and test set, we have an additional **validation set** for picking the model's hyperparameters. It is common to use around 60% of the data for training and 20% each for validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64% for training, 16% for validation and 20% for out-of-sample test\n",
    "\n",
    "\n",
    "# Try different alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After picking the best alpha based on validation data, the final step is to test the model's out-of-sample performance with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. K-Fold Cross Validation\n",
    "\n",
    "A problem with dividing the data into three parts is that we are using a lot less data for training. **K-Fold Cross Validation** is a method to overcome that problem: instead of having a separate validation set, we divide our training set into $K$ equal parts. We use $K-1$ parts for training and validate with the remaining part. This process can be repeated for $K$ times, each time using a different part for validation. We then take the average score from these $K$ runs to pick our hyperparameters.\n",
    "\n",
    "<img src=\"../Images/cross_validation.png\" width=\"80%\">\n",
    "Source: <a href=\"https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\">\n",
    "Towards Data Science</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can loop through different alphas and pick the one that works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = 60\n",
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000,500000000]\n",
    "\n",
    "score_list = [] # List for saving scores\n",
    "\n",
    "# Splitting the data\n",
    "y_in,y_out,x_in,x_out = train_test_split(auto[\"price\"],\n",
    "                                         auto[[\"mpg\",\"weight\"]],\n",
    "                                         train_size=0.8)\n",
    "\n",
    "# Loop through different alphas\n",
    "best_alpha = None\n",
    "best_score = -99\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    scores = cross_val_score(ridge,x,y,cv=5)\n",
    "    avg_score = np.mean(scores)\n",
    "    print(str(a).ljust(10),\n",
    "          str(round(avg_score,4)).rjust(5))\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_alpha = a\n",
    "\n",
    "# Check model performance with test data\n",
    "best_model = Ridge(alpha=best_alpha)\n",
    "best_model.fit(x_in,y_in)\n",
    "print(\"Best alpha value:\",best_alpha)\n",
    "print(\"Test R-Squared:\",round(best_model.score(x_test,y_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold cross-validation trades a data with training time. This might be a worthwhile tradeoff when data is limited and the model is relatively simple. For models such neural networks that are time-consuming to train, the simple train-validation-test split is often the only feasible way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. GridSearchCV\n",
    "\n",
    "In practice, you should use scikit-learn's ```GridSearchCV``` instead of writing your own loop. This is particularly true if the model has multiple hyperparameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use a dictionary to specify the parameters we need to go through\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing hyperparameter(s) and the best score are recorded in ```best_params_``` and ```best_score_``` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Best parameter(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Best score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best_estimator is saved in ```best_estimator_```. We can use that for out-of-sample test or making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
