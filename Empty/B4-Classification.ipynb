{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The need to group data arises naturally in many scenario. For example, a marketing officer would need to identify which group of customers are the most worthwhile to target, and a researcher might want to categorize households based on their sociodemographic background. \n",
    "\n",
    "Techniques used in grouping data are separated into two broad categories:\n",
    "- **Classification** - Group data based on examples you provide.\n",
    "- **Clustering** - Group data without any examples. \n",
    "\n",
    "The former is a form of *supervised learning*, while the latter is *unsupervised learning*. Which type of techniques to use depends on your need. In the case of marketing, perhaps you already have experience on which type of customers is the most profitable, so you will go for classification. On the other hand, when it comes to categorizing households, you might want the program to automatically separate the households into groups, which would be clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Classification\n",
    "Suppose you have the following data:\n",
    "\n",
    "| Customer |   Address   | Spending |\n",
    "|:--------:|:-----------:|:--------:|\n",
    "|     1    |   Central   |   High   |\n",
    "|     2    |  Admiralty  |   High   |\n",
    "|     3    | North Point |    Low   |\n",
    "|     4    |    Shatin   |   High   |\n",
    "|     5    |    Fo Tan   |    Low   |\n",
    "|     6    |  Ma On Shan |    Low   |\n",
    "\n",
    "And you need to predict the spending of the following customer:\n",
    "\n",
    "| Customer |   Address   | Spending |\n",
    "|:--------:|:-----------:|:--------:|\n",
    "|     7    |   Chai Wan  |     ?    |\n",
    "\n",
    "How should you do so? \n",
    "\n",
    "Before we proceed further, let us first go over the terminology commonly used in classification. In economics we usually call each data point an *observation*, with \"Spending\" being a *dependent variable* and \"Address\" an *independent variable*. In classification it is more common to call each data point a *sample*, with \"Spending\" being a *class variable* and \"Address\" a *feature*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Data\n",
    "raw_data = [\n",
    "            [1,1,'Central',22.2819,114.1581,1],\n",
    "            [2,1,'Admiralty',22.2796,114.1655,1],\n",
    "            [3,0,'North Point',22.2871,114.1917,1],\n",
    "            [4,1,'Shatin',22.3771,114.1974,0],\n",
    "            [5,0,'Fo Tan',22.3969,114.1959,0],\n",
    "            [6,0,'Ma On Shan',22.4221,114.2324,0],\n",
    "            ]\n",
    "labels = ['customer','hi_spending','address','latitude','longitude','hk_island']\n",
    "data = pd.DataFrame.from_records(raw_data,columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visual the data on a map of Hong Kong:\n",
    "\n",
    "<img src=\"http://www.ticoneva.com/econ/econ4130/images/8-map.png\" width=\"300\">\n",
    "\n",
    "This map is generated with the ```basemap``` library. You can find the corresponding script below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Square\n",
    "There are multiple ways to approach this problem. As an economics major, the first technique that comes to mind is probably the ordinary least square (OLS). Why is OLS not suitable for classification?\n",
    "\n",
    "It is unsuitable because OLS gives continuous predictions. If ```1``` is high income and ```0``` is low income, should we interpret a prediction of  ```2``` as *really high income*? Worse, how should we interpret a negative prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit\n",
    "\n",
    "A seasoned economist will likely use a logit regression to handle this task. Logistic regression assumes that \n",
    "$$\n",
    "P(y=1\\mid \\vec{x}) = \\frac{e^{\\vec{x}\\vec{\\beta}}}{1+e^{\\vec{x}\\vec{\\beta}}}\n",
    "$$\n",
    "\n",
    "\n",
    "First let us consider the district of each customer's address:\n",
    "\n",
    "| Customer |   Address   | Hong Kong Island | High Spending |\n",
    "|:--------:|:-----------:|:----------------:|:-------------:|\n",
    "|     1    |   Central   |         1        |       1       |\n",
    "|     2    |  Admiralty  |         1        |       1       |\n",
    "|     3    | North Point |         1        |       0       |\n",
    "|     4    |    Shatin   |         0        |       1       |\n",
    "|     5    |    Fo Tan   |         0        |       0       |\n",
    "|     6    |  Ma On Shan |         0        |       0       |\n",
    "|     7    |   Chai Wan  |         1        |       ?       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "y = data[\"hi_spending\"]\n",
    "X = data[\"hk_island\"].values.reshape(-1, 1)\n",
    "\n",
    "#Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question we might have from the above is, why is estimated probabilities not 1/3 and 2/3? This is because in data mining, constraints are added to to penalize extreme estimates. This technique is called *regularization*. Regularization is done to prevent overfitting, the phenomenon of closely fitting existing data but producing good predictions for unseen samples.\n",
    "\n",
    "If we tune down the regularization parameter in the logistic regression, we will get predictions closer to (1/3,2/3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with weak regularization (high C)\n",
    "\n",
    "\n",
    "#Predict\n",
    "print(\"Prediction (high spending = 1):\".ljust(35),model.predict([[1]]))\n",
    "print(\"Pr(Low) Pr(High):\".ljust(35),model.predict_proba([[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is in effect already categorized. What if we have latitudinal and longitudinal data instead?\n",
    "\n",
    "| Customer |   Address   | Latitude | Longitude | High Spending |\n",
    "|:--------:|:-----------:|:--------:|:---------:|:-------------:|\n",
    "|     1    |   Central   |  22.2819 |  114.1581 |       1       |\n",
    "|     2    |  Admiralty  |  22.2796 |  114.1655 |       1       |\n",
    "|     3    | North Point |  22.2871 |  114.1917 |       0       |\n",
    "|     4    |    Shatin   |  22.3771 |  114.1974 |       1       |\n",
    "|     5    |    Fo Tan   |  22.3969 |  114.1959 |       0       |\n",
    "|     6    |  Ma On Shan |  22.4221 |  114.2324 |       0       |\n",
    "|     7    |   Chai Wan  |   22.27  |   114.24  |       ?       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "\n",
    "\n",
    "#Test data\n",
    "\n",
    "\n",
    "#Train model\n",
    "\n",
    "\n",
    "#Predict\n",
    "tw = 50 #title column width\n",
    "print(\"Model Accuracy:\".ljust(tw),logit.score(X2,y))\n",
    "print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),logit.predict(X_ChaiWan))\n",
    "print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),logit.predict_proba(X_ChaiWan))\n",
    "print(\"Shatin Prediction (high spending = 1):\".ljust(tw),logit.predict(X_Shatin))\n",
    "print(\"Shatin Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),logit.predict_proba(X_Shatin))\n",
    "print(\"North Point Prediction (high spending = 1):\".ljust(tw),logit.predict(X_NorthPoint))\n",
    "print(\"North Point Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),logit.predict_proba(X_NorthPoint))\n",
    "print(\"Fo Tan Prediction (high spending = 1):\".ljust(tw),logit.predict(X_Fotan))\n",
    "print(\"Fo Tan Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),logit.predict_proba(X_Fotan))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going run the same tests for all the models in this notebook, it would be helpful to write a function for that so we will not need to type the same code repeatedly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to evaluate model accuracy and generate some predictions\n",
    "def summarize(model,data):\n",
    "    #Data for calculating r-squared\n",
    "    y = data[\"hi_spending\"]\n",
    "    X = data[[\"latitude\",\"longitude\"]]    \n",
    "    \n",
    "    #Test data\n",
    "    X_ChaiWan = [[22.27,114.24]]\n",
    "    X_Shatin = [[22.3771,114.1974]]\n",
    "    X_NorthPoint = [[22.2871,114.1917]]\n",
    "    X_Fotan = [[22.3969,114.1959]]    \n",
    "    \n",
    "    tw = 50 #title column width\n",
    "    print(\"Model Accuracy:\".ljust(tw),model.score(X,y))\n",
    "    print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan)[0])\n",
    "    print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan)[0])\n",
    "    print(\"North Point Prediction (high spending = 1):\".ljust(tw),model.predict(X_NorthPoint)[0])\n",
    "    print(\"North Point Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_NorthPoint)[0])    \n",
    "    print(\"Shatin Prediction (high spending = 1):\".ljust(tw),model.predict(X_Shatin)[0])\n",
    "    print(\"Shatin Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Shatin)[0])\n",
    "    print(\"Fo Tan Prediction (high spending = 1):\".ljust(tw),model.predict(X_Fotan)[0])\n",
    "    print(\"Fo Tan Est. Prob. - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_Fotan)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also write a function to highlight the decision boundaries with different colors. This function uses the ```basemap``` library, which you can install with ```conda install -c conda-forge basemap```. The function is a bit complicated, so it is okay if you do not understand how it works right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.cbook\n",
    "warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import numpy as np\n",
    "\n",
    "def drawBoundaryOnMap(model,data):\n",
    "    \n",
    "    #Fetch latitudinal and longtitudinal data\n",
    "    lat = data['latitude'].values\n",
    "    lon = data['longitude'].values\n",
    "\n",
    "    #Map size\n",
    "    padding = 0.15 # padding\n",
    "    lat_min = min(lat) - padding\n",
    "    lat_max = max(lat) + padding\n",
    "    lon_min = min(lon) - padding\n",
    "    lon_max = max(lon) + padding\n",
    "\n",
    "    #6x6 matplotlib figure\n",
    "    plt.figure(figsize=(6,6))\n",
    "\n",
    "    #Create map using Basemap\n",
    "    m = Basemap(llcrnrlon=lon_min,\n",
    "                llcrnrlat=lat_min,\n",
    "                urcrnrlon=lon_max,\n",
    "                urcrnrlat=lat_max,\n",
    "                lat_0=(lat_max - lat_min)/2,\n",
    "                lon_0=(lon_max-lon_min)/2,\n",
    "                resolution = 'h',\n",
    "                )\n",
    "    m.drawcoastlines()\n",
    "    m.drawmapboundary(fill_color='#46bcec')\n",
    "    m.fillcontinents(color = 'white',lake_color='#46bcec')\n",
    "\n",
    "    #Heatmap highlighting decision boundaries\n",
    "    xy = np.mgrid[lat_min:lat_max:0.001,lon_min:lon_max:0.001]\n",
    "    x_count = xy.shape[1]\n",
    "    y_count = xy.shape[2]\n",
    "    xy = xy.reshape(2,-1).T\n",
    "    a = model.predict(xy)\n",
    "    #a = model.predict_proba(xy)[:,1]\n",
    "    a = np.asarray(a).reshape((x_count,y_count))\n",
    "    m.imshow(a, cmap='PiYG', interpolation='nearest', \n",
    "               extent=(lon_min,lon_max,lat_min,lat_max),\n",
    "               alpha=0.8,zorder=5)\n",
    "    m.scatter(lon, lat, marker = 'o', color='r', zorder=6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the graphing function on the logit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawBoundaryOnMap(logit,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logit model will label anything within the purple area as low income and that within the green area as high income. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule\n",
    "The Bayes Rule says that\n",
    "\n",
    "$$\n",
    "P(y \\mid \\vec{x}) = \\frac{P(\\vec{x} \\mid y)P(y)}{P(\\vec{x})}\n",
    "$$\n",
    "Applying to our current problem,\n",
    "\n",
    "$$\n",
    "P(spending \\mid location) = \\frac{P(location \\mid spending)P(spending)}{P(location)}\n",
    "$$\n",
    "Our task is to pick a value for $spending$ that maximizes this probability:\n",
    "\n",
    "$$\n",
    "\\hat{spending} = \\underset{spending}{\\operatorname{argmax}}  \\left \\{ \\frac{P(location \\mid spending)P(spending)}{P(location)} \\right \\}\n",
    "$$\n",
    "\n",
    "Notice that $P(location)$ is constant for any given location, so we can eliminate it and get\n",
    "\n",
    "$$\n",
    "\\hat{spending} = \\underset{spending}{\\operatorname{argmax}}  \\left \\{ P(location \\mid spending)P(spending) \\right \\}\n",
    "$$\n",
    "\n",
    "To solve this maximization problem we need $P(location \\mid spending)$, and there are two common ways to get that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Native Bayes\n",
    "Native Bayes assumes that all the elements of $\\vec{x}$ are independent, so\n",
    "\n",
    "$$\n",
    "P(y \\mid \\vec{x}) = P(y \\mid x_1) \\cdot P(y \\mid x_2) \\cdot P(y \\mid x_3) ... \n",
    "$$\n",
    "\n",
    "Each $P(y \\mid x_i)$ is assumed to be normally distributed. The mean and standard deviations are estimated by the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Native Bayes\n",
    "from sklearn.naive_bayes import *\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X2,y)\n",
    "\n",
    "summarize(gnb,data)\n",
    "drawBoundaryOnMap(gnb,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Linear Discriminant Analysis (LDA)\n",
    "LDA assumes that the features are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Discriminant Analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X2,y)\n",
    "\n",
    "summarize(lda,data)\n",
    "drawBoundaryOnMap(lda,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVD)\n",
    "\n",
    "SVD looks for a boundary that separate two classes while allowing for a buffer zone where mistakes are tolerated.\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_svm_margin_001.png\">\n",
    "Source: <a href=\"http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html\">scikit learn</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Support Vector Machine Classifier\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(probability=True)\n",
    "svc.fit(X2,y)\n",
    "\n",
    "summarize(svc,data)\n",
    "drawBoundaryOnMap(svc,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor\n",
    "Another method we could use is to look at samples that have similar characters as the one we are trying to predict. This method is called *nearest neighbor*.\n",
    "\n",
    "In the simpliest case, we will use the closest sample as a predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nearest Neigbhor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Only consider the nearest neighbor\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X2,y)\n",
    "\n",
    "summarize(knn,data)\n",
    "drawBoundaryOnMap(knn,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike previous methods, when we try to predict the outcome of a pre-existing sample such as Shatin, we will get the correct answer. Naturally, this is because a pre-existing sample's closest neighbor is itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of neighbors is crucial. For example, suppose we use the three closest neighbors instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider the three closest neighbor\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X2,y)\n",
    "print(\"Model Accuracy:\".ljust(tw),model.score(X2,y))\n",
    "print(\"Chai Wan Prediction (high spending = 1):\".ljust(tw),model.predict(X_ChaiWan))\n",
    "\n",
    "print(\"Chai Wan Est. Prob - Pr(Low) Pr(High):\".ljust(tw),model.predict_proba(X_ChaiWan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model estimates that Chai Wan has 1/3 chance of being low income and 2/3 chance of being high income because out of the three Hong Kong Island districts in our training data, one has low income (North Point) and two have high income (Central and Admiralty).\n",
    "\n",
    "The following diagram sums up the estimates. The green circle indicates Chai Wan's closest neighbor, while the purple circle indicates the closest three neighbors.\n",
    "\n",
    "<img src=\"http://www.ticoneva.com/econ/econ4130/images/8-map-knn.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree\n",
    "Classification Tree repeatedly look for cutoffs that give the best prediction at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Tree\n",
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(X2,y)\n",
    "\n",
    "summarize(dt,data)\n",
    "drawBoundaryOnMap(dt,data)\n",
    "\n",
    "#Export tree structure to PNG format\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydot\n",
    "dotfile = StringIO()\n",
    "tree.export_graphviz(dt, out_file=dotfile)\n",
    "pydot.graph_from_dot_data(dotfile.getvalue())[0].write_png(\"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above export the tree structure in an image filed named ```tree.png```. You might get a different try structure each time you run the script. Here is one example:\n",
    "\n",
    "<img src=\"http://www.ticoneva.com/econ/econ4130/images/8-tree.png\" width=\"300\">\n",
    "\n",
    "We can trace the cutoffs on a map to see how the tree works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lat = data['latitude'].values\n",
    "lon = data['longitude'].values\n",
    "\n",
    "#Map size\n",
    "padding = 0.15 # padding\n",
    "lat_min = min(lat) - padding\n",
    "lat_max = max(lat) + padding\n",
    "lon_min = min(lon) - padding\n",
    "lon_max = max(lon) + padding\n",
    "\n",
    "#Create map using Basemap\n",
    "plt.figure(figsize=(6,6))\n",
    "m = Basemap(llcrnrlon=lon_min,\n",
    "            llcrnrlat=lat_min,\n",
    "            urcrnrlon=lon_max,\n",
    "            urcrnrlat=lat_max,\n",
    "            lat_0=(lat_max - lat_min)/2,\n",
    "            lon_0=(lon_max-lon_min)/2,\n",
    "            resolution = 'h',\n",
    "            )\n",
    "m.drawcoastlines()\n",
    "m.drawmapboundary(fill_color='#46bcec')\n",
    "m.fillcontinents(color = 'white',lake_color='#46bcec')\n",
    "\n",
    "# plot points and cutoffs\n",
    "m.scatter(lon, lat, marker = 'o', color='r', zorder=5)\n",
    "m.plot([114.179,114.179],[lat_min,lat_max], 'k-', zorder=5)\n",
    "m.plot([114.197,114.197],[lat_min,lat_max], 'b-', zorder=5)\n",
    "m.plot([lon_min,lon_max],[22.4,22.4], 'g-', zorder=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated image should look the same as this one. The black, blue and green lines are the first, second and third cutoff respectively.\n",
    "\n",
    "<img src=\"http://www.ticoneva.com/econ/econ4130/images/8-map-tree.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Clustering\n",
    "\n",
    "Clustering algorithms group data without supervision. To do so, they minimize some measure of distance between data within the same group. For example, this could be simple distance as measured by the difference between values, or it could be variation as measured by variance.\n",
    "\n",
    "| Customer |   Address   | Latitude | Longitude |\n",
    "|:--------:|:-----------:|:--------:|:---------:|\n",
    "|     1    |   Central   |  22.2819 |  114.1581 |\n",
    "|     2    |  Admiralty  |  22.2796 |  114.1655 |\n",
    "|     3    | North Point |  22.2871 |  114.1917 |\n",
    "|     4    |    Shatin   |  22.3771 |  114.1974 |\n",
    "|     5    |    Fo Tan   |  22.3969 |  114.1959 |\n",
    "|     6    |  Ma On Shan |  22.4221 |  114.2324 |\n",
    "|     7    |   Chai Wan  |   22.27  |   114.24  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "raw_data2 = [\n",
    "            [1,'Central',22.2819,114.1581],\n",
    "            [2,'Admiralty',22.2796,114.1655],\n",
    "            [3,'North Point',22.2871,114.1917],\n",
    "            [4,'Shatin',22.3771,114.1974],\n",
    "            [5,'Fo Tan',22.3969,114.1959],\n",
    "            [6,'Ma On Shan',22.4221,114.2324],\n",
    "            [7,'Chai Wan',22.27,114.24]\n",
    "            ]\n",
    "labels = ['customer','address','latitude','longitude']\n",
    "data2 = pd.DataFrame.from_records(raw_data2,columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = data2[[\"latitude\",\"longitude\"]]\n",
    "\n",
    "#K-Means\n",
    "from sklearn.cluster import *\n",
    "\n",
    "#Two clusters\n",
    "\n",
    "\n",
    "#Three clusters\n",
    "\n",
    "\n",
    "#Four clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the labelling is random and could change between runs.\n",
    "\n",
    "A different clustering algorithm could give different results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Agglomerative Clustering\n",
    "ac = AgglomerativeClustering(n_clusters=2)\n",
    "y_ac = ac.fit_predict(X3)\n",
    "print(y_ac)\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=3)\n",
    "y_ac = ac.fit_predict(X3)\n",
    "print(y_ac)\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=4)\n",
    "y_ac = ac.fit_predict(X3)\n",
    "print(y_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very nice diagram on scikit-learn's website showing the prediction of different clustering algorithms:\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
