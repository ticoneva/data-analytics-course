{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4e6f22",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Transformer-based models are the current state of the art in the field of natural language processing. It is the basis of some of the most advanced AI currently in existence, including the Starcraft-playing [AlphaStar](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii) and text-generating [GPT-3](https://en.wikipedia.org/wiki/GPT-3).\n",
    "\n",
    "Training a Transformer-based model from scratch is very expensive, due to the large number of parameters and the huge volume of data involved. The cost of training GPT-3 was [estimated](https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/) to be in the range of tens of millions of U.S. dollars. Fortunately, many pre-trained models are available. Pre-trained models can be fine-tuned to specific needs by training them further with domain-specific data.\n",
    "\n",
    "In this notebook, we will use the `transformers` library developed by [Hugging Face](https://huggingface.co/), a startup \"on a mission to democratize good machine learning.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b394f14",
   "metadata": {},
   "source": [
    "## A. Using Pre-Trained Models\n",
    "\n",
    "The `transformers` library makes it very easy to download pre-trained models. Downloaded models are saved in a cache folder, which is by default under your home directory at `$HOME/.cache/huggingface`. Because Transformer models requires a lot of disk space&mdash;larger ones can run into hundreds of GB's&mdash;we will change the cache folder to a shared one, where I have already downloaded some models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc3d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face's Default cache directory is $HOME/.cache/huggingface\n",
    "# To change it, set the environment variable HF_HOME\n",
    "# BEFORE importing Hugging Face libraries\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/data/huggingface/\"\n",
    "\n",
    "# Hugging Face Transformers\n",
    "# Either PyTorch or Tensorflow must be installed\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cbd8c",
   "metadata": {},
   "source": [
    "Next we have to decide what model to download. Models are categorized by attributes, including:\n",
    "\n",
    "#### Model architecture\n",
    "- BERT, GPT-2, ALBERT, RoBERTa,...\n",
    "\n",
    "#### Fine-tuned task\n",
    "- Default is whatever the model is trained on. \n",
    "e.g. BERT is trained to fill in missing words, \n",
    "while GPT-2 is trained to predict next words.\n",
    "- [*text-generation*](https://huggingface.co/models?pipeline_tag=text-generation) models are fine-tuned for text generation.\n",
    "- [*question-anwsering*](https://huggingface.co/models?pipeline_tag=question-answering) models are fine-tuned to answer questions based on a user-provided context.\n",
    "- [*text-classification*](https://huggingface.co/models?pipeline_tag=text-classification) covers sentiment analysis and topic classification.\n",
    "\n",
    "There are also models for [summarization](https://huggingface.co/models?pipeline_tag=summarization), [conversation](https://huggingface.co/models?pipeline_tag=conversational), [sentence comparison](https://huggingface.co/models?pipeline_tag=sentence-similarity) and [translation](https://huggingface.co/models?pipeline_tag=translation). You can search for available models on Hugging Face's [website](https://huggingface.co/). \n",
    "\n",
    "#### Language\n",
    "- Models are usually trained on English data, but you can search for other languages, e.g. [Chinese](https://huggingface.co/models?search=chinese).\n",
    "\n",
    "### A1. Question Answering\n",
    "\n",
    "Let us start by loading the default Q&A model.  `transformers` provide the `pipline` class for this purpose. The syntax is:\n",
    "```python\n",
    "model = pipline(task,[model])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32d14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Question answering with default model.\n",
    "# This will download the model if not already present\n",
    "question_answerer = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de8b11",
   "metadata": {},
   "source": [
    "Once the model is loaded, we need to provide it with a `question` and a `context` in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aedf81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9862246513366699, 'start': 42, 'end': 45, 'answer': '8th'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\n",
    "'question': 'What is the ranking of CUHK in Asia?',\n",
    "'context': 'The Chinese University of Hong Kong ranks 8th in Asia and 48th in the world in the field of Economics and Econometrics (QS World University Rankings by Subject 2021).'\n",
    "}\n",
    "\n",
    "question_answerer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8339732",
   "metadata": {},
   "source": [
    "Try different questions and context and see what you get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c23996",
   "metadata": {},
   "source": [
    "### A2. Text Generation\n",
    "\n",
    "For text generation, we will specify that we want the GPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2da03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation with GPT-2\n",
    "text_generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2479df",
   "metadata": {},
   "source": [
    "We need to provide the model with a text prompt. \n",
    "The model will then predict what words should follow.\n",
    "We can also specify the maximum length of the generated text with `max_length`\n",
    "and how many sequences of text we want with `num_return_sequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fbb3844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I major in economics, the only role for me is as an economics teacher.\" And he added:'},\n",
       " {'generated_text': \"I major in economics, including financial science and law, which I did in high school and I'm\"},\n",
       " {'generated_text': 'I major in economics, and can be reached at (202) 694-4161.'},\n",
       " {'generated_text': \"I major in economics, but as a graduate student or professor, I've always wondered if we can\"},\n",
       " {'generated_text': 'I major in economics, philosophy, science, economics, etc.)\\n\\nGran Turismo Sport'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate five sequences of 20 words each.\n",
    "text_generator(\"I major in economics,\", \n",
    "               max_length=20, \n",
    "               num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08dc3d9",
   "metadata": {},
   "source": [
    "Try changing `max_length` and note how the quality of the generated text varies with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5847241",
   "metadata": {},
   "source": [
    "### A3. Sentiment Analysis\n",
    "\n",
    "Finally, let us try a sentiment analysis model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5979f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e42a0f503f84f30876c3fa3037f8714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472ed3ed5ecc49909b6b04f5fbe424b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c88ccf3a7340d8a85bdac5d1a1c3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5ee228d6d84e4ab8022719f7d42900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = pipeline('text-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0935b5",
   "metadata": {},
   "source": [
    "For sentiment analysis we only need to provide a string of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ee27a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9992952346801758}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am very sad today.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c42bf",
   "metadata": {},
   "source": [
    "## B. Tokenizer\n",
    "\n",
    "If you want to fine-tune a model, you will need to convert your text data\n",
    "into a suitable format. This is the job of a model's *tokenizer*. \n",
    "Because different models have different designs, \n",
    "you need to use the tokenizer that comes with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d098f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer for DistilBERT\n",
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b0f059",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1327, 1110, 1103, 5662, 1104, 140, 2591, 3048, 2428, 1107, 3165, 136, 102, 5192, 1107, 3165, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the tokenizer. Question and text can be arrays rather than one sample.\n",
    "question, text = \"What is the ranking of CUHK in Asia?\",\"8th in Asia\"\n",
    "encodings = tokenizer(question,text)\n",
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f5ca30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What is the ranking of CUHK in Asia? [SEP] 8th in Asia [SEP]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e69132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(encodings))\n",
    "print(type(dict(encodings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a7e7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d22c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adceb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
