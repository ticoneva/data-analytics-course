{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data\n",
    "\n",
    "Trying to construct models that understand text falls under the field of *natural language processing*. This is a field of enormous practical importance: chatbot, automated translation and generated new articles area few notable applications. In this notebook we will look into some basic ways of processing text data.\n",
    "\n",
    "Below is what you might get in a typical dataset of review data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text data\n",
    "corpus = [\n",
    "    \"This is good.\",\n",
    "    \"This is bad.\",\n",
    "    \"This is very good.\",\n",
    "    \"This is not good.\",\n",
    "    \"This is not bad.\",\n",
    "    \"This is...is bad.\"\n",
    "]\n",
    "\n",
    "ratings = [\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing review data the typical goal is to predict a single value, the rating, from the written text. This is a form of *sentiment analysis*. In the case of chatbot and automated translation, where one single value is not sufficient to represent the meaning of text, a vector is outputed by the model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. N-gram\n",
    "\n",
    "Let us count the number of times each word appears in a sample. This is called *unigram* in natural language processing. To do so, we will use ```CountVectorizer``` of scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0]\n",
      " [1 0 1 0 1 0]\n",
      " [0 1 1 0 1 1]\n",
      " [0 1 1 1 1 0]\n",
      " [1 0 1 1 1 0]\n",
      " [1 0 2 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```get_feature_names()``` to see which word each column represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'good', 'is', 'not', 'this', 'very']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word-count vector can now be used with a suitable model to conduct language processing. Here we will simply use a logit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "[1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = ratings\n",
    "\n",
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "print(model.score(X,y))\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which phrases do our model have a difficulty understanding? Why might that be the case?\n",
    "\n",
    "Now let us take a look at the estimated coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.37601408e-01  2.37590746e-01 -3.55854219e-01  1.81681559e-06\n",
      "  -1.06620709e-05  3.55804904e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the coefficients of each word. Can you see what is wrong with our model? One thing you might notice is that 'is' has a very negative coefficient while 'very' has very a positive coefficient, even though these words do not have such connotations themselves.  \n",
    "\n",
    "When we start counting combination of words instead of individual words, what we have is *n-gram*. ```CountVectorizer``` allows us to specify the range of words we wish to consider via the option ```ngram_range```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 1]\n",
      " [0 0 0 1 0 0 1 1 0]\n",
      " [0 0 0 1 0 1 0 1 0]\n",
      " [1 0 1 0 0 0 0 1 0]]\n",
      "['is bad', 'is good', 'is is', 'is not', 'is very', 'not bad', 'not good', 'this is', 'very good']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try running the logistic regression again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[-6.65709533e-01  3.78667208e-01 -2.99691500e-01 -3.25311016e-02\n",
      "   3.19576438e-01  3.84884164e-01 -4.17415266e-01  3.01182347e-06\n",
      "   3.19576438e-01]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "print(model.score(X,y))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. IMDB Movie Review\n",
    "\n",
    "Now let us try something real. We will analyse a sample of <a href=\"https://www.imdb.com/\">IMDB</a> movie reviews, trying to predict the rating a user gives based on his written review. We will be using a pre-processed version of the data, but the original text data can be found <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">here</a>.\n",
    "\n",
    "First let us import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "imdb = np.load(\"../Data/imdb.npz\",allow_pickle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look what is inside this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test\n",
      "x_train\n",
      "y_train\n",
      "y_test\n"
     ]
    }
   ],
   "source": [
    "for d in imdb:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many samples do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(imdb[\"x_train\"].shape)\n",
    "print(imdb[\"x_test\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inside each X sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23022,\n",
       " 309,\n",
       " 6,\n",
       " 3,\n",
       " 1069,\n",
       " 209,\n",
       " 9,\n",
       " 2175,\n",
       " 30,\n",
       " 1,\n",
       " 169,\n",
       " 55,\n",
       " 14,\n",
       " 46,\n",
       " 82,\n",
       " 5869,\n",
       " 41,\n",
       " 393,\n",
       " 110,\n",
       " 138,\n",
       " 14,\n",
       " 5359,\n",
       " 58,\n",
       " 4477,\n",
       " 150,\n",
       " 8,\n",
       " 1,\n",
       " 5032,\n",
       " 5948,\n",
       " 482,\n",
       " 69,\n",
       " 5,\n",
       " 261,\n",
       " 12,\n",
       " 23022,\n",
       " 73935,\n",
       " 2003,\n",
       " 6,\n",
       " 73,\n",
       " 2436,\n",
       " 5,\n",
       " 632,\n",
       " 71,\n",
       " 6,\n",
       " 5359,\n",
       " 1,\n",
       " 25279,\n",
       " 5,\n",
       " 2004,\n",
       " 10471,\n",
       " 1,\n",
       " 5941,\n",
       " 1534,\n",
       " 34,\n",
       " 67,\n",
       " 64,\n",
       " 205,\n",
       " 140,\n",
       " 65,\n",
       " 1232,\n",
       " 63526,\n",
       " 21145,\n",
       " 1,\n",
       " 49265,\n",
       " 4,\n",
       " 1,\n",
       " 223,\n",
       " 901,\n",
       " 29,\n",
       " 3024,\n",
       " 69,\n",
       " 4,\n",
       " 1,\n",
       " 5863,\n",
       " 10,\n",
       " 694,\n",
       " 2,\n",
       " 65,\n",
       " 1534,\n",
       " 51,\n",
       " 10,\n",
       " 216,\n",
       " 1,\n",
       " 387,\n",
       " 8,\n",
       " 60,\n",
       " 3,\n",
       " 1472,\n",
       " 3724,\n",
       " 802,\n",
       " 5,\n",
       " 3521,\n",
       " 177,\n",
       " 1,\n",
       " 393,\n",
       " 10,\n",
       " 1238,\n",
       " 14030,\n",
       " 30,\n",
       " 309,\n",
       " 3,\n",
       " 353,\n",
       " 344,\n",
       " 2989,\n",
       " 143,\n",
       " 130,\n",
       " 5,\n",
       " 7804,\n",
       " 28,\n",
       " 4,\n",
       " 126,\n",
       " 5359,\n",
       " 1472,\n",
       " 2375,\n",
       " 5,\n",
       " 23022,\n",
       " 309,\n",
       " 10,\n",
       " 532,\n",
       " 12,\n",
       " 108,\n",
       " 1470,\n",
       " 4,\n",
       " 58,\n",
       " 556,\n",
       " 101,\n",
       " 12,\n",
       " 23022,\n",
       " 309,\n",
       " 6,\n",
       " 227,\n",
       " 4187,\n",
       " 48,\n",
       " 3,\n",
       " 2237,\n",
       " 12,\n",
       " 9,\n",
       " 215]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"x_train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are encoded by their frequency-of-apperance ranking in the data. This allows us to easily delete words that either\n",
    "- appear frequently but add little to the meaning of the text (e.g. articles, conjunctions and prepositions), or\n",
    "- appear too infrequently to be of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inside each y sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"y_train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now repeat what we have done previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "x_raw_train,y_train,x_raw_test,y_test = resample(imdb[\"x_train\"],imdb[\"y_train\"],\n",
    "                                         imdb[\"x_test\"],imdb[\"y_test\"],\n",
    "                                         n_samples=1000)\n",
    "\n",
    "x_train = [' '.join(str(e) for e in x) for x in x_raw_train]\n",
    "x_test = [' '.join(str(e) for e in x) for x in x_raw_test]\n",
    "vectorizer = CountVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.806\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "print(model.score(x_train,y_train))\n",
    "print(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Lemmatization\n",
    "\n",
    "Consider the following corpus of text, modified from the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text data\n",
    "corpus2 = [\n",
    "    \"Apple is good.\",\n",
    "    \"Apple was bad.\",\n",
    "    \"Apples are good.\",\n",
    "    \"Apples were not good.\",\n",
    "    \"Apple is not bad.\",\n",
    "    \"Apples were...are bad.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having plurals complicates our analysis: `CountVectorizer` will treat 'Apple' and 'Apples' as two distinct words, unncessarily splitting the samples for apples. Similarly, 'is' and 'are' are both forms of the verb 'to be', so they should be considered as one word. What we need is *lemmatization*, which is the process of grouping together the inflected forms of a word for use in analysis.\n",
    "\n",
    "We will be using <a href=\"https://textblob.readthedocs.io/en/dev/index.html\">TextBlob</a>, a library for processing textual data. TextBlob in turn relies on <a href=\"http://www.nltk.org/\">NLTK</a> (short for *Natural Language ToolKit*) to do some of the heavy lifting. Since NLTK does not come with all packages installed, we will need to first download the ones we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process goes as follows:\n",
    "1. First convert each string to a `TextBlob` object. \n",
    "2. Split each string into sentences with the `.sentences` property if needed.\n",
    "3. Split each string (or sentence) into words with the `.words` property.\n",
    "4. Lemmatize each word with the `lemmatize()` method. \n",
    "\n",
    "Note that `lemmatize()` expects words to be in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['apple', 'is', 'good']),\n",
       " WordList(['apple', 'wa', 'bad']),\n",
       " WordList(['apple', 'are', 'good']),\n",
       " WordList(['apple', 'were', 'not', 'good']),\n",
       " WordList(['apple', 'is', 'not', 'bad']),\n",
       " WordList(['apple', 'were', 'are', 'bad'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use TextBlob to lemmatize the corpus\n",
    "from textblob import TextBlob\n",
    "\n",
    "tb = [TextBlob(c.lower()) for c in corpus2]\n",
    "wordlist = [t.words for t in tb]\n",
    "data = [w.lemmatize() for w in wordlist]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above successfully grouped 'apples' with 'apple', but it failed to group 'is' and 'are'. The second sample gives us some hint as to what went wrong---'was' was somehow converted to 'wa'. What happened was that `lemmatize()` by default treats all words as nouns. To ensure proper conversion, we will need to provide it with each word's part of speech (POS).\n",
    "\n",
    "First, we generate part-of-speech tags by using the `.tags` property of the `TextBlob` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('apple', 'NN'), ('is', 'VBZ'), ('good', 'JJ')],\n",
       " [('apple', 'NN'), ('was', 'VBD'), ('bad', 'JJ')],\n",
       " [('apples', 'NNS'), ('are', 'VBP'), ('good', 'JJ')],\n",
       " [('apples', 'NNS'), ('were', 'VBD'), ('not', 'RB'), ('good', 'JJ')],\n",
       " [('apple', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('bad', 'JJ')],\n",
       " [('apples', 'NNS'), ('were', 'VBD'), ('are', 'VBP'), ('bad', 'JJ')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Penn Treebank POS\n",
    "tags = [t.tags for t in tb]\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then providing `lemmatize()` with part-of-speech tags. Unfortunately it is not as simple as passing the POS tags from above. The reason is that NLTK generates tags base on the <a href=\"https://catalog.ldc.upenn.edu/LDC99T42\">Penn Treebank</a> corpus, which uses different <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">POS</a> tags than the <a href=\"https://wordnet.princeton.edu/documentation/wndb5wn\">Wordnet</a> corpus that `lemmatize()` is based on. \n",
    "\n",
    "We therefore need to map the two POS systems before lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'be', 'good'],\n",
       " ['apple', 'be', 'bad'],\n",
       " ['apple', 'be', 'good'],\n",
       " ['apple', 'be', 'not', 'good'],\n",
       " ['apple', 'be', 'not', 'bad'],\n",
       " ['apple', 'be', 'be', 'bad']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to map Penn Treebank POS to Wordnet POS\n",
    "def pos_conv(pos):\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}    \n",
    "    return tag_dict.get(pos[0], 'n')\n",
    "\n",
    "# Convert Penn Treebank POS to Wordnet POS\n",
    "wordnet_tags = [[[w, pos_conv(pos)] for w, pos in t] for t in tags]\n",
    "\n",
    "# Lemmatize with POS\n",
    "data = [[w.lemmatize(t) for w,t in s] for s in wordnet_tags]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob and NLTK have many other useful features such as spelling correction and translation that you can explore on your own. One particularly useful feature is pre-trained sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentiment(polarity=0.7, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=0.7, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=-0.35, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=0.3499999999999999, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment analysis with TextBlob\n",
    "sentiment =  [t.sentiment for t in tb]\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Chinese Text\n",
    "\n",
    "One major issue with Chinese text is that there is no space between words. Unsurprisingly then, this is a major focus for Chinese natural language processing research.\n",
    "\n",
    "They are multiple libraries for Chinese NLP. Here we will try out `jieba`, `pkuseg` and `snownlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我愛吃', '北京', '餃子', '。']\n",
      "['我', '愛', '吃', '北京', '餃', '子', '。']\n",
      "['我', '愛', '吃', '北京', '餃子', '。']\n",
      "['我', '愛', '吃', '北京', '餃子', '。']\n"
     ]
    }
   ],
   "source": [
    "text = '我愛吃北京餃子。'\n",
    "\n",
    "# jieba default\n",
    "import jieba\n",
    "seg_list = jieba.cut(text) \n",
    "print([w for w in seg_list])\n",
    "\n",
    "# jieba cut all mode\n",
    "import jieba\n",
    "seg_list = jieba.cut(text, cut_all=True) \n",
    "print([w for w in seg_list])\n",
    "\n",
    "# pkuseg\n",
    "import pkuseg\n",
    "seg = pkuseg.pkuseg() \n",
    "seg_list = seg.cut(text)\n",
    "print(seg_list)\n",
    "\n",
    "# SnowNLP\n",
    "from snownlp import SnowNLP\n",
    "snow = SnowNLP(text)\n",
    "print(snow.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are much easier once we have the individual words. For example, we could immediately use ngram on the text.\n",
    "\n",
    "We can also fetch POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('我', 'r'), ('愛', 'v'), ('吃', 'v'), ('北京', 'ns'), ('餃子', 'n'), ('。', 'x')]\n",
      "[('我', 'r'), ('愛', 'v'), ('吃', 'v'), ('北京', 'ns'), ('餃子', 'n'), ('。', 'w')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('我', 'r'), ('愛', 'Ng'), ('吃', 'v'), ('北京', 'ns'), ('餃子', 'y'), ('。', 'w')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jieba\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(text)\n",
    "print([(w,f) for w,f in words])\n",
    "\n",
    "# pkuseg\n",
    "seg = pkuseg.pkuseg(postag=True)\n",
    "text = seg.cut(text)\n",
    "print(text)\n",
    "\n",
    "# SnowNLP\n",
    "list(snow.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tags for `pkugseg`:\n",
    "https://github.com/lancopku/pkuseg-python/blob/master/tags.txt\n",
    "\n",
    "For `jieba`:\n",
    "https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`snownlp` is capable of sentiment analysis, trained on product reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8645420008368975"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow.sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Neural Network\n",
    "\n",
    "Below is a simple LSTM neural network model that runs sentiment analysis on the IMDB data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "1000 train sequences\n",
      "1000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (1000, 80)\n",
      "x_test shape: (1000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.6880 - acc: 0.5470 - val_loss: 0.6755 - val_acc: 0.5210\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4602 - acc: 0.7940 - val_loss: 0.6251 - val_acc: 0.6870\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.1288 - acc: 0.9560 - val_loss: 0.8618 - val_acc: 0.6880\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0254 - acc: 0.9930 - val_loss: 0.9426 - val_acc: 0.6990\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.0071 - acc: 0.9990 - val_loss: 1.1436 - val_acc: 0.7110\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 9.9471e-04 - acc: 1.0000 - val_loss: 1.2642 - val_acc: 0.7110\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.0342e-04 - acc: 1.0000 - val_loss: 1.3591 - val_acc: 0.7140\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.0508e-04 - acc: 1.0000 - val_loss: 1.3817 - val_acc: 0.7150\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 5.8263e-05 - acc: 1.0000 - val_loss: 1.4128 - val_acc: 0.7190\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 4.4111e-05 - acc: 1.0000 - val_loss: 1.4411 - val_acc: 0.7200\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.9674e-05 - acc: 1.0000 - val_loss: 1.4642 - val_acc: 0.7210\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.7676e-05 - acc: 1.0000 - val_loss: 1.4845 - val_acc: 0.7210\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.8061e-05 - acc: 1.0000 - val_loss: 1.5058 - val_acc: 0.7200\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.3179e-05 - acc: 1.0000 - val_loss: 1.5180 - val_acc: 0.7220\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.1029e-05 - acc: 1.0000 - val_loss: 1.5236 - val_acc: 0.7190\n",
      "1000/1000 [==============================] - 0s 489us/step\n",
      "Test score: 1.52359737206\n",
      "Test accuracy: 0.719\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train,y_train,x_test,y_test = resample(x_train,y_train,x_test,y_test,\n",
    "                                         n_samples=1000)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that training a neural network is several orders of magnitude slower than a n-gram model. Furthermore, the neural network model above is not more accurate than our simple n-gram model. One reason is that with so many parameters, neural network models need more than a thousand sample to achieve good results. You can try running the same script with more data on a computer with GPU and see whether you get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Readings\n",
    "- <a href=\"https://github.com/dipanjanS/text-analytics-with-python\">Text Analytics with Python</a> (or the <a href=\"https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\">free tutorial</a> by the same author on Towards Data Science.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
