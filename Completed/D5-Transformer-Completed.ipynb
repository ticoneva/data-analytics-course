{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4e6f22",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Version 2022-12-1\n",
    "\n",
    "Transformer-based models are the current state of the art in the field of natural language processing. It is the basis of some of the most advanced AI currently in existence, including image-generator Stable Diffusion [AlphaStar](https://stability.ai/blog/stable-diffusion-v2-release) and text-generator [GPT-3](https://openai.com/api/).\n",
    "\n",
    "Training a Transformer-based model from scratch is very expensive, due to the large number of parameters and the huge volume of data involved. The cost of training GPT-3 was [estimated](https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/) to be in the range of tens of millions of U.S. dollars. Fortunately, many pre-trained models are available. Pre-trained models can be fine-tuned to specific needs by training them further with domain-specific data.\n",
    "\n",
    "In this notebook, we will use the `transformers` library developed by [Hugging Face](https://huggingface.co/), a startup \"on a mission to democratize good machine learning.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b394f14",
   "metadata": {},
   "source": [
    "## A. Using Pre-Trained Models\n",
    "\n",
    "The `transformers` library makes it very easy to download pre-trained models. Downloaded models are saved in a cache folder, which is by default under your home directory at `$HOME/.cache/huggingface`. Because Transformer models requires a lot of disk space&mdash;larger ones can run into hundreds of GB's&mdash;we will change the cache folder to a shared one, where I have already downloaded some models. You should change it to a folder that you control when you work on your own projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc3d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face's Default cache directory is $HOME/.cache/huggingface\n",
    "# To change it, set the environment variable HF_HOME\n",
    "# BEFORE importing Hugging Face libraries\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/data/huggingface/\"\n",
    "\n",
    "# Hugging Face Transformers\n",
    "# Either PyTorch or Tensorflow must be installed\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cbd8c",
   "metadata": {},
   "source": [
    "Next we have to decide what model to download. Models are categorized by attributes, including:\n",
    "\n",
    "#### Model architecture\n",
    "- BERT, GPT-2, ALBERT, RoBERTa,...\n",
    "\n",
    "#### Fine-tuned task\n",
    "- Default is whatever the model is trained on. \n",
    "e.g. BERT is trained to fill in missing words, \n",
    "while GPT-2 is trained to predict next words.\n",
    "- [*text-generation*](https://huggingface.co/models?pipeline_tag=text-generation) models are fine-tuned for text generation.\n",
    "- [*question-anwsering*](https://huggingface.co/models?pipeline_tag=question-answering) models are fine-tuned to answer questions based on a user-provided context.\n",
    "- [*text-classification*](https://huggingface.co/models?pipeline_tag=text-classification) covers sentiment analysis and topic classification.\n",
    "\n",
    "There are also models for [summarization](https://huggingface.co/models?pipeline_tag=summarization), [conversation](https://huggingface.co/models?pipeline_tag=conversational), [sentence comparison](https://huggingface.co/models?pipeline_tag=sentence-similarity) and [translation](https://huggingface.co/models?pipeline_tag=translation). You can search for available models on Hugging Face's [website](https://huggingface.co/). \n",
    "\n",
    "#### Language\n",
    "- Models are usually trained on English data, but you can search for other languages, e.g. [Chinese](https://huggingface.co/models?search=chinese).\n",
    "\n",
    "### A1. Question Answering\n",
    "\n",
    "Let us start by loading the default Q&A model.  `transformers` provide the `pipline` class for this purpose. The syntax is:\n",
    "```python\n",
    "model = pipline(task,[model])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f32d14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Question answering with default model.\n",
    "# This will download the model if not already present\n",
    "question_answerer = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de8b11",
   "metadata": {},
   "source": [
    "Once the model is loaded, we need to provide it with a `question` and a `context` in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aedf81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9862250685691833, 'start': 42, 'end': 45, 'answer': '8th'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\n",
    "'question': 'What is the ranking of CUHK in Asia?',\n",
    "'context': 'The Chinese University of Hong Kong ranks 8th in Asia and 48th in the world in the field of Economics and Econometrics (QS World University Rankings by Subject 2021).'\n",
    "}\n",
    "\n",
    "question_answerer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8339732",
   "metadata": {},
   "source": [
    "Try different questions and context and see what you get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c23996",
   "metadata": {},
   "source": [
    "### A2. Text Generation\n",
    "\n",
    "For text generation, we will specify that we want the GPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2da03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation with GPT-2\n",
    "text_generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2479df",
   "metadata": {},
   "source": [
    "We need to provide the model with a text prompt. \n",
    "The model will then predict what words should follow.\n",
    "We can also specify the maximum length of the generated text with `max_length`\n",
    "and how many sequences of text we want with `num_return_sequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbb3844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I major in economics, and my career involves making money. In fact, it helped me realize why'},\n",
       " {'generated_text': 'I major in economics, is an adjunct professor of ecology and evolutionary biology from Columbia University. He and'},\n",
       " {'generated_text': 'I major in economics, and has previously done research for a small university in central London working with the'},\n",
       " {'generated_text': \"I major in economics, and I'm an economist. I'm a big believer that markets and incentives\"},\n",
       " {'generated_text': 'I major in economics, but my college was full of people with no experience or interest in government,\"'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate five sequences of 20 words each.\n",
    "text_generator(\"I major in economics,\", \n",
    "               max_length=20, \n",
    "               num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08dc3d9",
   "metadata": {},
   "source": [
    "Try changing `max_length` and note how the quality of the generated text varies with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5847241",
   "metadata": {},
   "source": [
    "### A3. Sentiment Analysis\n",
    "\n",
    "Finally, let us try a sentiment analysis model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5979f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('text-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0935b5",
   "metadata": {},
   "source": [
    "For sentiment analysis we only need to provide a string of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee27a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9992952346801758}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am very sad today.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd171de8",
   "metadata": {},
   "source": [
    "### A4. Multiple Samples\n",
    "\n",
    "`pipeline` allows you to provide multiple samples in a list, though if you want to go through a whole dataset, you might want to use the underlying model directly. How to do so will be covered in part C.\n",
    "\n",
    "**Do not use `pipeline` on GPU, as that combination is even slower than using `pipeline` on CPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b5eba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84.8 ms, sys: 0 ns, total: 84.8 ms\n",
      "Wall time: 8.34 ms\n",
      "CPU times: user 255 ms, sys: 0 ns, total: 255 ms\n",
      "Wall time: 15.7 ms\n",
      "CPU times: user 382 ms, sys: 0 ns, total: 382 ms\n",
      "Wall time: 22.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9992952346801758},\n",
       " {'label': 'NEGATIVE', 'score': 0.9992952346801758},\n",
       " {'label': 'NEGATIVE', 'score': 0.9992952346801758}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time the execution speed of one sample\n",
    "%time classifier(\"I am very sad today.\")\n",
    "\n",
    "# Time the execution speed of two samples\n",
    "%time classifier([\"I am very sad today.\",\"I am very sad today.\"])\n",
    "\n",
    "# Time the execution speed of three samples\n",
    "%time classifier([\"I am very sad today.\",\"I am very sad today.\",\"I am very sad today.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c42bf",
   "metadata": {},
   "source": [
    "## B. Tokenizer\n",
    "\n",
    "If you want to fine-tune a model, you will need to convert your text data\n",
    "into a suitable format. This is the job of a model's *tokenizer*. \n",
    "Because different models have different designs, \n",
    "you need to use the tokenizer that comes with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b0f059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1327, 1110, 1103, 5662, 1104, 140, 2591, 3048, 2428, 1107, 3165, 136, 102, 5192, 1107, 3165, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer for DistilBERT\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# Use the tokenizer. \n",
    "# Note that question and text can be arrays rather than one sample.\n",
    "question, text = \"What is the ranking of CUHK in Asia?\",\"8th in Asia\"\n",
    "encodings = tokenizer(question,text)\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a45a4",
   "metadata": {},
   "source": [
    "`input_ids` is the text we provide, with each word replaced by its numeric ID. We can use `tokenizer.decode()` to convert it back to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f5ca30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What is the ranking of CUHK in Asia? [SEP] 8th in Asia [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e81511",
   "metadata": {},
   "source": [
    "Note the special characters `[CLS]` and `[SEP]` added by the BERT tokenizer.\n",
    "\n",
    "Models such as BERT often use *sub-word tokens* to provide even more information to the model. We usually do not need to construct the sub-word tokens manually, but it can be done with \n",
    "```\n",
    "tokenizer.convert_ids_to_tokens(input_ids)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bde8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'ranking',\n",
       " 'of',\n",
       " 'C',\n",
       " '##U',\n",
       " '##H',\n",
       " '##K',\n",
       " 'in',\n",
       " 'Asia',\n",
       " '?',\n",
       " '[SEP]',\n",
       " '8th',\n",
       " 'in',\n",
       " 'Asia',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encodings['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569c305",
   "metadata": {},
   "source": [
    "Note how BERT separates 'CUHK' into four separate tokens.\n",
    "\n",
    "To convert sub-word tokens back to string, use `tokenizer.convert_tokens_to_string()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c370bc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What is the ranking of CUHK in Asia? [SEP] 8th in Asia [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def87990",
   "metadata": {},
   "source": [
    "Let use try another example. This time we will use GPT-2's tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3739735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [40, 1688, 287, 12446], 'attention_mask': [1, 1, 1, 1]}\n",
      "I major in economics\n",
      "['I', 'Ġmajor', 'Ġin', 'Ġeconomics']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer for GPT-2\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "text = \"I major in economics\"\n",
    "encodings = tokenizer(text)\n",
    "print(encodings)\n",
    "print(tokenizer.decode(encodings['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(encodings['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd004a0",
   "metadata": {},
   "source": [
    "The `Ġ` character in tokens stands for whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a7e7d",
   "metadata": {},
   "source": [
    "## C. Using the Underlying Model\n",
    "\n",
    "If you want the model to process a lot of samples, you need to use the underlying model directly instead of using `pipeline`. \n",
    "\n",
    "First, load the appropriate model and tokenizer. Will use the DistilBERT question and answer model as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2128def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 10:15:55.874938: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-01 10:15:55.874964: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (scrp-login-2): /proc/driver/nvidia/version does not exist\n",
      "2022-12-01 10:15:55.875602: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFDistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-cased-distilled-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForQuestionAnswering\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import tensorflow as tf # Need to import either Tensorflow or PyTorch\n",
    "import numpy as np\n",
    "\n",
    "# Set up model. Tensorflow models starts with 'TF'\n",
    "model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374be68",
   "metadata": {},
   "source": [
    "Next, feed the data to the model and process the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0a5a66a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hong Kong', 'orange']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "question = [\"Where is CUHK?\", \n",
    "            \"What is an apple?\"]\n",
    "text = [\"CUHK is a university in Hong Kong.\", \n",
    "        \"Apple and orange are examples of fruits.\"]\n",
    "inputs = tokenizer(question, text, \n",
    "                   return_tensors='tf', \n",
    "                   truncation=True, \n",
    "                   padding=True)\n",
    "\n",
    "# Feed data through the model\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Q&A model outputs the two logit scores for each word.\n",
    "# One for its chance of being the start of the answer\n",
    "# and one for its chance of being the end\n",
    "start_logits = outputs.start_logits.numpy()\n",
    "end_logits = outputs.end_logits.numpy()\n",
    "\n",
    "# Find the words with the highest score\n",
    "start = np.argmax(start_logits, 1)\n",
    "end = np.argmax(end_logits, 1)\n",
    "\n",
    "# Return the answers\n",
    "tokens = [tokenizer.convert_ids_to_tokens(x) for x in inputs[\"input_ids\"].numpy()]\n",
    "ans_tokens = [x[start[i]:end[i]+1] for i,x in enumerate(tokens)]\n",
    "answers = [tokenizer.convert_tokens_to_string(x) for x in ans_tokens]\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dec1be",
   "metadata": {},
   "source": [
    "To time the script, let us wrap the code above in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ef1ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(question,text):\n",
    "    inputs = tokenizer(question, text, \n",
    "                       return_tensors='tf', \n",
    "                       truncation=True, \n",
    "                       padding=True)\n",
    "\n",
    "    # Feed data through the model\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Q&A model outputs the two logit scores for each word.\n",
    "    # One for its chance of being the start of the answer\n",
    "    # and one for its chance of being the end\n",
    "    start_logits = outputs.start_logits.numpy()\n",
    "    end_logits = outputs.end_logits.numpy()\n",
    "\n",
    "    # Find the words with the highest score\n",
    "    start = np.argmax(start_logits, 1)\n",
    "    end = np.argmax(end_logits, 1)\n",
    "\n",
    "    # Return the answers\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(x) for x in inputs[\"input_ids\"].numpy()]\n",
    "    return [tokenizer.convert_tokens_to_string(x[start[i]:end[i]+1]) for i,x in enumerate(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98014c64",
   "metadata": {},
   "source": [
    "Now we can use the magic command `%time` to time the function. This time, we feed the model with 1000 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eca68ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.4 s, sys: 27.9 s, total: 1min 20s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "question = [\"Where is CUHK?\" for i in range(1000)]\n",
    "text = [\"CUHK is a university in Hong Kong.\" for i in range(1000)]\n",
    "\n",
    "%time ans = batch_inference(question,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc7bd4",
   "metadata": {},
   "source": [
    "Compare to using `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f37ce44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 7s, sys: 73.7 ms, total: 3min 7s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "inputs={'question':question,'context':text}\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "%time ans = question_answerer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3379824",
   "metadata": {
    "tags": []
   },
   "source": [
    "## D. Running on Cluster\n",
    "\n",
    "We can speed up the process by using more CPU cores, but it will be even better if we use a GPU. Too see how much speed up we can get, let us put what we have above in a python script. This is available as `hf-batch-inference.py` under the 'Examples' folder.\n",
    "\n",
    "If you using the Department of Economics' SCRP HPC Cluster, you can run it on four CPU cores by typing the following commands in a terminal:\n",
    "\n",
    "```\n",
    "conda activate tensorflow\n",
    "compute python [path]/hf-batch-inference.py\n",
    "```\n",
    "\n",
    "This should take around six seconds to complete.\n",
    "\n",
    "\n",
    "To run on a GPU :\n",
    "\n",
    "```\n",
    "gpu python [path]/hf-batch-inference.py\n",
    "```\n",
    "\n",
    "This runs the script on the slowest available GPU on the cluster. This usually means a RTX 3060. You can expect the inference to complete in 0.35 seconds, excluding the time it takes to load the model and the tokenizer.\n",
    "\n",
    "The speed up is going to be much more impressive if we use the fastest GPU available:\n",
    "```\n",
    "gpu ---gpus=rtx3090:1 python [path]/hf-batch-inference.py\n",
    "```\n",
    "\n",
    "Inferencing 1000 samples should take less than 0.2 seconds, a 200x speed up over using `pipeline` on a login node.\n",
    "\n",
    "One thing to beware of is that GPU on-board memory is generally much smaller than main memory, and for that reason you could ran out of memory if you try to feed a large dataset to the model all at once. In that case you will have to feed data in batches. Both Tensorflow and Hugging Face have a `Dataset` class for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b28a7a-883b-43e1-ad15-9f99dcec628f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
