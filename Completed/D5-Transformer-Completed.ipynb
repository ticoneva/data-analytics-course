{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4e6f22",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Version 2023-11-27\n",
    "\n",
    "Transformer-based models are the current state of the art in the field of natural language processing. It is the basis of some of the most advanced AI currently in existence, including image-generator Stable Diffusion [AlphaStar](https://stability.ai/blog/stable-diffusion-v2-release) and text-generator [GPT-3](https://openai.com/api/).\n",
    "\n",
    "Training a Transformer-based model from scratch is very expensive, due to the large number of parameters and the huge volume of data involved. The cost of training GPT-3 was [estimated](https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/) to be in the range of tens of millions of U.S. dollars. Fortunately, many pre-trained models are available. Pre-trained models can be fine-tuned to specific needs by training them further with domain-specific data.\n",
    "\n",
    "In this notebook, we will use the `transformers` library developed by [Hugging Face](https://huggingface.co/), a startup \"on a mission to democratize good machine learning.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b394f14",
   "metadata": {},
   "source": [
    "## A. Using Pre-Trained Models\n",
    "\n",
    "The `transformers` library makes it very easy to download pre-trained models. Downloaded models are saved in a cache folder, which is by default under your home directory at `$HOME/.cache/huggingface`. Because Transformer models requires a lot of disk space&mdash;larger ones can run into hundreds of GB's&mdash;you might want to change the cache folder to a different location. This can be done by changing the `HF_HOME` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc3d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face's Default cache directory is $HOME/.cache/huggingface\n",
    "# To change it, set the environment variable HF_HOME\n",
    "# BEFORE importing Hugging Face libraries\n",
    "#import os\n",
    "#os.environ[\"HF_HOME\"] = \"/data/huggingface/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cbd8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we have to decide what model to download. Models are categorized by attributes, including:\n",
    "\n",
    "#### Model architecture\n",
    "- Small models: BERT, GPT-2, ALBERT, RoBERTa,...\n",
    "- Large models ($\\geq$ 7B parameters): Llama-2, MPT, Falcon, ...\n",
    "\n",
    "#### Fine-tuned task\n",
    "- Default is whatever the model is trained on. \n",
    "e.g. BERT is trained to fill in missing words, \n",
    "while GPT-2 is trained to predict next words.\n",
    "- [*text-generation*](https://huggingface.co/models?pipeline_tag=text-generation) models are fine-tuned for text generation.\n",
    "- [*question-anwsering*](https://huggingface.co/models?pipeline_tag=question-answering) models are fine-tuned to answer questions based on a user-provided context.\n",
    "- [*text-classification*](https://huggingface.co/models?pipeline_tag=text-classification) covers sentiment analysis and topic classification.\n",
    "\n",
    "There are also models for [summarization](https://huggingface.co/models?pipeline_tag=summarization), [conversation](https://huggingface.co/models?pipeline_tag=conversational), [sentence comparison](https://huggingface.co/models?pipeline_tag=sentence-similarity) and [translation](https://huggingface.co/models?pipeline_tag=translation). You can search for available models on Hugging Face's [website](https://huggingface.co/). \n",
    "\n",
    "#### Language\n",
    "- Models are usually trained on English data, but you can search for other languages, e.g. [Chinese](https://huggingface.co/models?search=chinese).\n",
    "\n",
    "### A1. Question Answering\n",
    "\n",
    "Let us start by loading the default Q&A model.  `transformers` provide the `pipline` class for this purpose. The syntax is:\n",
    "```python\n",
    "model = pipline(task,[model,settings])\n",
    "```\n",
    "Some important settings:\n",
    "- `device_map`: Set to `'auto'` to allow pipeline to use GPU when available.\n",
    "- `model_kwargs`: pass a dictionary of advanced settings to the underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f32d14b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Transformers\n",
    "# Either PyTorch or Tensorflow must be installed\n",
    "from transformers import pipeline\n",
    "\n",
    "# Question answering with default model.\n",
    "# This will download the model if not already present\n",
    "pipe = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de8b11",
   "metadata": {},
   "source": [
    "Once the model is loaded, we need to provide it with a `question` and a `context` in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aedf81a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9862250685691833, 'start': 42, 'end': 45, 'answer': '8th'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\n",
    "'question': 'What is the ranking of CUHK in Asia?',\n",
    "'context': 'The Chinese University of Hong Kong ranks 8th in Asia and 48th in the world in the field of Economics and Econometrics (QS World University Rankings by Subject 2021).'\n",
    "}\n",
    "\n",
    "pipe(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8339732",
   "metadata": {},
   "source": [
    "Try different questions and context and see what you get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5847241",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A2. Sentiment Analysis\n",
    "\n",
    "Next, let us try a sentiment analysis model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5979f31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('text-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0935b5",
   "metadata": {},
   "source": [
    "For sentiment analysis we only need to provide a string of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee27a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9992952346801758}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am very sad today.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c23996",
   "metadata": {},
   "source": [
    "### A3. Text Generation - Non-Conversational\n",
    "\n",
    "For general text generation, we will specify that we want the GPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2da03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text generation with GPT-2\n",
    "from transformers import pipeline\n",
    "text_generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2479df",
   "metadata": {},
   "source": [
    "We need to provide the model with a text prompt. \n",
    "The model will then predict what words should follow.\n",
    "\n",
    "Important settings that you can change:\n",
    "- `max_length`: The maximum length of the generated text, including the text you provide.\n",
    "- `num_return_sequences`: How many sequences of text we want.\n",
    "- `do_sample`: Whether to sample words based on their probability distribution.\n",
    "- `temperature`: A lower temperature sharpens the distribution.\n",
    "- `top_k`: Only sample from the top ùëò-most likely words.\n",
    "- `top_p`: Only sample words are within the top cumulative distribution of ùëù\n",
    "- `repetition_penalty`: A higher value lower the probability of sampling words that have already been sampled before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fbb3844",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I major in economics, and I'm in the business of making the most effective decisions that are best\"},\n",
       " {'generated_text': 'I major in economics, but I\\'m very serious about it,\" he said.\\n\\nIn a'},\n",
       " {'generated_text': \"I major in economics, as well as a couple of degrees in business administration. I've been writing\"},\n",
       " {'generated_text': \"I major in economics, and I'm a student-athlete. I have a bachelor's degree\"},\n",
       " {'generated_text': 'I major in economics, and I\\'ve been a huge proponent of free markets and the free market.\"'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate five sequences of 20 words each.\n",
    "text_generator(\"I major in economics,\", \n",
    "               max_length=20, \n",
    "               do_sample=True,\n",
    "               temperature=0.6,\n",
    "               num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08dc3d9",
   "metadata": {},
   "source": [
    "Try changing the settings and note how the quality of the generated text varies with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78edc6b3-5000-47fb-8d8e-11f523ef86f1",
   "metadata": {},
   "source": [
    "### A4. Text Generation - Conversational\n",
    "\n",
    "For conversational text generation, we need to specify a model that is fine-tuned on instruction following. Here we will try Meta's `opt-1.3b`. \n",
    "\n",
    "We will pass an advanced settings called `load_in_8bit`, which reduces memory usage by 3/4. As a rule of thumb, a 1B-parameter model loaded in 8-bit takes 1GB of memory. There is additionally another 1GB of overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eab87bf-05ea-47db-ba40-fae9cdbe73ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you tell me what is deep learning?\n",
      "It's a new type of AI that uses neural networks to learn from data. It's a lot like how a human brain learns.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('text-generation', \n",
    "                     model=\"facebook/opt-1.3b\",\n",
    "                     device_map=\"auto\", \n",
    "                     model_kwargs={\"load_in_8bit\": True})\n",
    "\n",
    "response = pipe(\"Can you tell me what is deep learning?\",\n",
    "          max_length=512 )\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7ed62-057e-41dc-9a01-65a1e14f3281",
   "metadata": {},
   "source": [
    "Next we will try `vicuna-13b-v1.5`. This is a 13B-parameter model, which means it will take more that 13GB of memory, more than what a RTX 3060 can provide. We can either move up to a GPU with more memory, or we can lower the memory usage further with the setting `load_in_4bit`. Note that a model's performance would goes down as we lower its parameters' precision, though the degradation might not be noticable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3951304c-5637-4da0-97ba-9abe34918a45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f6a64852464d7281b69d2a8a0d4fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you tell me what is deep learning?\n",
      "\n",
      "Deep learning is a subset of machine learning that uses artificial neural networks to model and solve complex problems. It is a type of machine learning that is designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn and make predictions by modeling complex patterns in large datasets.\n",
      "\n",
      "Deep learning algorithms are capable of learning and making predictions by modeling complex patterns in large datasets. They are designed to learn\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('text-generation', \n",
    "                model=\"lmsys/vicuna-13b-v1.5\",\n",
    "                device_map=\"auto\", \n",
    "                model_kwargs={\"load_in_4bit\": True})\n",
    "response = pipe(\"Can you tell me what is deep learning?\", \n",
    "               max_length=512)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ebf20-018d-459b-a7ac-16bf9daf98f0",
   "metadata": {},
   "source": [
    "For reasons that I am not sure about, some model would fail to load its corresponding tokenizer, requiring you to load it yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3283846b-2f7b-4985-8093-005e897b00bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206a963e46d247df8b8781b7cd8abd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Can you tell me what is deep learning?\\nDeep learning is a subset of machine learning that uses artificial neural networks to learn and make predictions. These networks are composed of layers of neurons that are trained on large datasets to recognize patterns and make decisions.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n",
    "pipe = pipeline('text-generation', \n",
    "                model=\"tiiuae/falcon-7b-instruct\",\n",
    "                tokenizer=tokenizer,\n",
    "                device_map=\"auto\", \n",
    "                model_kwargs={\"load_in_4bit\": True})\n",
    "pipe(\"Can you tell me what is deep learning?\", \n",
    "               max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd171de8",
   "metadata": {},
   "source": [
    "### A5. Multiple Samples\n",
    "\n",
    "`pipeline` allows you to provide multiple samples in a list, though if you want to go through a whole dataset, you might want to use the underlying model directly. How to do so will be covered in part C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b5eba4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.3 ms, sys: 1.13 ms, total: 99.4 ms\n",
      "Wall time: 35.5 ms\n",
      "CPU times: user 133 ms, sys: 6.66 ms, total: 140 ms\n",
      "Wall time: 36.7 ms\n",
      "CPU times: user 260 ms, sys: 12.2 ms, total: 272 ms\n",
      "Wall time: 76.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9992952346801758},\n",
       " {'label': 'NEGATIVE', 'score': 0.9992952346801758},\n",
       " {'label': 'NEGATIVE', 'score': 0.9992952346801758}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time the execution speed of one sample\n",
    "%time classifier(\"I am very sad today.\")\n",
    "\n",
    "# Time the execution speed of two samples\n",
    "%time classifier([\"I am very sad today.\",\"I am very sad today.\"])\n",
    "\n",
    "# Time the execution speed of three samples\n",
    "%time classifier([\"I am very sad today.\",\"I am very sad today.\",\"I am very sad today.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c42bf",
   "metadata": {},
   "source": [
    "## B. Tokenizer\n",
    "\n",
    "If you want to fine-tune a model, you will need to convert your text data\n",
    "into a suitable format. This is the job of a model's *tokenizer*. \n",
    "Because different models have different designs, \n",
    "you need to use the tokenizer that comes with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b0f059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1327, 1110, 1103, 5662, 1104, 140, 2591, 3048, 2428, 1107, 3165, 136, 102, 5192, 1107, 3165, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer for DistilBERT\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# Use the tokenizer. \n",
    "# Note that question and text can be arrays rather than one sample.\n",
    "question, text = \"What is the ranking of CUHK in Asia?\",\"8th in Asia\"\n",
    "encodings = tokenizer(question,text)\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a45a4",
   "metadata": {},
   "source": [
    "`input_ids` is the text we provide, with each word replaced by its numeric ID. We can use `tokenizer.decode()` to convert it back to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f5ca30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What is the ranking of CUHK in Asia? [SEP] 8th in Asia [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e81511",
   "metadata": {},
   "source": [
    "Note the special characters `[CLS]` and `[SEP]` added by the BERT tokenizer.\n",
    "\n",
    "Models such as BERT often use *sub-word tokens* to provide even more information to the model. We usually do not need to construct the sub-word tokens manually, but it can be done with \n",
    "```\n",
    "tokenizer.convert_ids_to_tokens(input_ids)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bde8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'What',\n",
       " 'is',\n",
       " 'the',\n",
       " 'ranking',\n",
       " 'of',\n",
       " 'C',\n",
       " '##U',\n",
       " '##H',\n",
       " '##K',\n",
       " 'in',\n",
       " 'Asia',\n",
       " '?',\n",
       " '[SEP]',\n",
       " '8th',\n",
       " 'in',\n",
       " 'Asia',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encodings['input_ids'])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569c305",
   "metadata": {},
   "source": [
    "Note how BERT separates 'CUHK' into four separate tokens.\n",
    "\n",
    "To convert sub-word tokens back to string, use `tokenizer.convert_tokens_to_string()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c370bc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What is the ranking of CUHK in Asia? [SEP] 8th in Asia [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def87990",
   "metadata": {},
   "source": [
    "Let use try another example. This time we will use GPT-2's tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3739735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [40, 1688, 287, 12446], 'attention_mask': [1, 1, 1, 1]}\n",
      "I major in economics\n",
      "['I', 'ƒ†major', 'ƒ†in', 'ƒ†economics']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer for GPT-2\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "text = \"I major in economics\"\n",
    "encodings = tokenizer(text)\n",
    "print(encodings)\n",
    "print(tokenizer.decode(encodings['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(encodings['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd004a0",
   "metadata": {},
   "source": [
    "The `ƒ†` character in tokens stands for whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a7e7d",
   "metadata": {},
   "source": [
    "## C. Using the Underlying Model\n",
    "\n",
    "If you want the model to process a lot of samples, you need to use the underlying model directly instead of using `pipeline`. \n",
    "\n",
    "First, load the appropriate model and tokenizer. Will use the DistilBERT question and answer model as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2128def6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import torch # Need to import either Tensorflow or PyTorch\n",
    "import numpy as np\n",
    "\n",
    "# Use GPU if available, otherwise use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Set up model. Tensorflow models starts with 'TF'\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374be68",
   "metadata": {},
   "source": [
    "Next, feed the data to the model and process the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0a5a66a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hong Kong', 'orange']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "question = [\"Where is CUHK?\", \n",
    "            \"What is an apple?\"]\n",
    "text = [\"CUHK is a university in Hong Kong.\", \n",
    "        \"Apple and orange are examples of fruits.\"]\n",
    "\n",
    "# return_tensors should be `tf' for Tensorflow\n",
    "inputs = tokenizer(question, text, \n",
    "                   return_tensors='pt', \n",
    "                   truncation=True, \n",
    "                   padding=True)\n",
    "# Move data to GPU\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Feed data through the model\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Q&A model outputs the two logit scores for each word.\n",
    "# One for its chance of being the start of the answer\n",
    "# and one for its chance of being the end\n",
    "start_logits = outputs.start_logits.to('cpu').detach().numpy()\n",
    "end_logits = outputs.end_logits.to('cpu').detach().numpy()\n",
    "\n",
    "# Find the words with the highest score\n",
    "start = np.argmax(start_logits, 1)\n",
    "end = np.argmax(end_logits, 1)\n",
    "\n",
    "# Return the answers\n",
    "tokens = [tokenizer.convert_ids_to_tokens(x) for x in inputs[\"input_ids\"].to('cpu').detach().numpy()]\n",
    "ans_tokens = [x[start[i]:end[i]+1] for i,x in enumerate(tokens)]\n",
    "answers = [tokenizer.convert_tokens_to_string(x) for x in ans_tokens]\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dec1be",
   "metadata": {},
   "source": [
    "To time the script, let us wrap the code above in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ef1ad12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_inference(question,text):\n",
    "    inputs = tokenizer(question, text, \n",
    "                       return_tensors='pt', \n",
    "                       truncation=True, \n",
    "                       padding=True)\n",
    "    # Move data to GPU\n",
    "    inputs = inputs.to(device)    \n",
    "\n",
    "    # Feed data through the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Q&A model outputs the two logit scores for each word.\n",
    "    # One for its chance of being the start of the answer\n",
    "    # and one for its chance of being the end\n",
    "    start_logits = outputs.start_logits.to('cpu').detach().numpy()\n",
    "    end_logits = outputs.end_logits.to('cpu').detach().numpy()\n",
    "\n",
    "    # Find the words with the highest score\n",
    "    start = np.argmax(start_logits, 1)\n",
    "    end = np.argmax(end_logits, 1)\n",
    "\n",
    "    # Return the answers\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(x) for x in inputs[\"input_ids\"].to('cpu').detach().numpy()]\n",
    "    return [tokenizer.convert_tokens_to_string(x[start[i]:end[i]+1]) for i,x in enumerate(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98014c64",
   "metadata": {},
   "source": [
    "Now we can use the magic command `%time` to time the function. This time, we feed the model with 1000 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eca68ece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 183 ms, sys: 12.6 ms, total: 195 ms\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "question = [\"Where is CUHK?\" for i in range(1000)]\n",
    "text = [\"CUHK is a university in Hong Kong.\" for i in range(1000)]\n",
    "\n",
    "%time ans = batch_inference(question,text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc7bd4",
   "metadata": {},
   "source": [
    "Compare to using `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f37ce44c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 986 ms, total: 1min 29s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "inputs={'question':question,'context':text}\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "%time ans = question_answerer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3379824",
   "metadata": {
    "tags": []
   },
   "source": [
    "## D. Running on Cluster\n",
    "\n",
    "We can speed up the process by using more CPU cores, but it will be even better if we use a GPU. Too see how much speed up we can get, let us put what we have above in a python script. This is available as `hf-batch-inference.py` under the 'Examples' folder.\n",
    "\n",
    "If you using the Department of Economics' SCRP HPC Cluster, you can run it on four CPU cores by typing the following commands in a terminal:\n",
    "\n",
    "```\n",
    "conda activate tensorflow\n",
    "compute python [path]/hf-batch-inference.py\n",
    "```\n",
    "\n",
    "This should take around six seconds to complete.\n",
    "\n",
    "\n",
    "To run on a GPU :\n",
    "\n",
    "```\n",
    "gpu python [path]/hf-batch-inference.py\n",
    "```\n",
    "\n",
    "This runs the script on the slowest available GPU on the cluster. This usually means a RTX 3060. You can expect the inference to complete in 0.35 seconds, excluding the time it takes to load the model and the tokenizer.\n",
    "\n",
    "The speed up is going to be much more impressive if we use the fastest GPU available:\n",
    "```\n",
    "gpu ---gpus=rtx3090:1 python [path]/hf-batch-inference.py\n",
    "```\n",
    "\n",
    "Inferencing 1000 samples should take less than 0.2 seconds, a 200x speed up over using `pipeline` on a login node.\n",
    "\n",
    "One thing to beware of is that GPU on-board memory is generally much smaller than main memory, and for that reason you could ran out of memory if you try to feed a large dataset to the model all at once. In that case you will have to feed data in batches. Both Tensorflow and Hugging Face have a `Dataset` class for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b28a7a-883b-43e1-ad15-9f99dcec628f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
