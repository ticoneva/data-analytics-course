{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data\n",
    "\n",
    "Version: 2023-10-18\n",
    "\n",
    "Trying to construct models that understand text falls under the field of *natural language processing*. This is a field of enormous practical importance: chatbot, automated translation and generated new articles area few notable applications. In this notebook we will look into some basic ways of processing text data.\n",
    "\n",
    "Below is what you might get in a typical dataset of review data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text data\n",
    "corpus = [\n",
    "    \"This is good.\",\n",
    "    \"This is bad.\",\n",
    "    \"This is very good.\",\n",
    "    \"This is not good.\",\n",
    "    \"This is not bad.\",\n",
    "    \"This is...is bad.\"\n",
    "]\n",
    "\n",
    "ratings = [\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing review data the typical goal is to predict a single value, the rating, from the written text. This is a form of *sentiment analysis*. In the case of chatbot and automated translation, where one single value is not sufficient to represent the meaning of text, a vector is outputed by the model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. N-gram\n",
    "\n",
    "Let us count the number of times each word appears in a sample. This is called *unigram* in natural language processing. To do so, we will use ```CountVectorizer``` of scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0]\n",
      " [1 0 1 0 1 0]\n",
      " [0 1 1 0 1 1]\n",
      " [0 1 1 1 1 0]\n",
      " [1 0 1 1 1 0]\n",
      " [1 0 2 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```get_feature_names_out()``` to see which word each column represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bad', 'good', 'is', 'not', 'this', 'very'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word-count vector can now be used with a suitable model to conduct language processing. Here we will simply use a logit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "[1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = ratings\n",
    "\n",
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "print(model.score(X,y))\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which phrases do our model have a difficulty understanding? Why might that be the case?\n",
    "\n",
    "Now let us take a look at the estimated coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.37601408e-01  2.37590746e-01 -3.55854219e-01  1.81681559e-06\n",
      "  -1.06620709e-05  3.55804904e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the coefficients of each word. Can you see what is wrong with our model? One thing you might notice is that 'is' has a very negative coefficient while 'very' has very a positive coefficient, even though these words do not have such connotations themselves.  \n",
    "\n",
    "When we start counting combination of words instead of individual words, what we have is *n-gram*. ```CountVectorizer``` allows us to specify the range of words we wish to consider via the option ```ngram_range```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 1]\n",
      " [0 0 0 1 0 0 1 1 0]\n",
      " [0 0 0 1 0 1 0 1 0]\n",
      " [1 0 1 0 0 0 0 1 0]]\n",
      "['is bad' 'is good' 'is is' 'is not' 'is very' 'not bad' 'not good'\n",
      " 'this is' 'very good']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try running the logistic regression again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[-6.65709533e-01  3.78667208e-01 -2.99691500e-01 -3.25311016e-02\n",
      "   3.19576438e-01  3.84884164e-01 -4.17415266e-01  3.01182347e-06\n",
      "   3.19576438e-01]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "print(model.score(X,y))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. IMDB Movie Review\n",
    "\n",
    "Now let us try something real. We will analyse a sample of <a href=\"https://www.imdb.com/\">IMDB</a> movie reviews, trying to predict the rating a user gives based on his written review. For speed reasons we will be using a subsample, but the original text data can be found <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">here</a>.\n",
    "\n",
    "First let us import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "imdb_train = pd.read_csv(\"../Data/imdb_train.csv\",\n",
    "                         names=['label','text'])\n",
    "imdb_test = pd.read_csv(\"../Data/imdb_test.csv\",\n",
    "                         names=['label','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many samples do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(imdb_train.shape)\n",
    "print(imdb_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inside each sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This was an absolutely terrible movie. Don't b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I have been known to fall asleep during films,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mann photographs the Alberta Rocky Mountains i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the kind of film for a snowy Sunday af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>As others have mentioned, all the women that g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  This was an absolutely terrible movie. Don't b...\n",
       "1      0  I have been known to fall asleep during films,...\n",
       "2      0  Mann photographs the Alberta Rocky Mountains i...\n",
       "3      1  This is the kind of film for a snowy Sunday af...\n",
       "4      1  As others have mentioned, all the women that g..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Words are encoded by their frequency-of-apperance ranking in the data. This allows us to easily delete words that either\n",
    "- appear frequently but add little to the meaning of the text (e.g. articles, conjunctions and prepositions), or\n",
    "- appear too infrequently to be of use.-->\n",
    "\n",
    "We will now repeat what we have done previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = imdb_train['label']\n",
    "y_test = imdb_test['label']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x_train = vectorizer.fit_transform(imdb_train['text'])\n",
    "x_test = vectorizer.transform(imdb_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.788\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train,y_train)\n",
    "print(model.score(x_train,y_train))\n",
    "print(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Lemmatization\n",
    "\n",
    "Consider the following corpus of text, modified from the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text data\n",
    "corpus2 = [\n",
    "    \"Apple is good.\",\n",
    "    \"Apple was bad.\",\n",
    "    \"Apples are good.\",\n",
    "    \"Apples were not good.\",\n",
    "    \"Apple is not bad.\",\n",
    "    \"Apples were...are bad.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having plurals complicates our analysis: `CountVectorizer` will treat 'Apple' and 'Apples' as two distinct words, unncessarily splitting the samples for apples. Similarly, 'is' and 'are' are both forms of the verb 'to be', so they should be considered as one word. What we need is *lemmatization*, which is the process of grouping together the inflected forms of a word for use in analysis.\n",
    "\n",
    "We will be using <a href=\"https://textblob.readthedocs.io/en/dev/index.html\">TextBlob</a>, a library for processing textual data. TextBlob in turn relies on <a href=\"http://www.nltk.org/\">NLTK</a> (short for *Natural Language ToolKit*) to do some of the heavy lifting. Since NLTK does not come with all packages installed, we will need to first download the ones we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/users/testuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/users/testuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/users/testuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/users/testuser/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process goes as follows:\n",
    "1. First convert each string to a `TextBlob` object. \n",
    "2. Split each string into sentences with the `.sentences` property if needed.\n",
    "3. Split each string (or sentence) into words with the `.words` property.\n",
    "4. Lemmatize each word with the `lemmatize()` method. \n",
    "\n",
    "Note that `lemmatize()` expects words to be in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['apple', 'is', 'good']),\n",
       " WordList(['apple', 'wa', 'bad']),\n",
       " WordList(['apple', 'are', 'good']),\n",
       " WordList(['apple', 'were', 'not', 'good']),\n",
       " WordList(['apple', 'is', 'not', 'bad']),\n",
       " WordList(['apple', 'were', 'are', 'bad'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use TextBlob to lemmatize the corpus\n",
    "from textblob import TextBlob\n",
    "\n",
    "tb = [TextBlob(c.lower()) for c in corpus2]\n",
    "sentences = [t.words for t in tb]\n",
    "data = [s.lemmatize() for s in sentences]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above successfully grouped 'apples' with 'apple', but it failed to group 'is' and 'are'. The second sample gives us some hint as to what went wrong---'was' was somehow converted to 'wa'. What happened was that `lemmatize()` by default treats all words as nouns. To ensure proper conversion, we will need to provide it with each word's part of speech (POS).\n",
    "\n",
    "First, we generate part-of-speech tags by using the `.tags` property of the `TextBlob` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('apple', 'NN'), ('is', 'VBZ'), ('good', 'JJ')],\n",
       " [('apple', 'NN'), ('was', 'VBD'), ('bad', 'JJ')],\n",
       " [('apples', 'NNS'), ('are', 'VBP'), ('good', 'JJ')],\n",
       " [('apples', 'NNS'), ('were', 'VBD'), ('not', 'RB'), ('good', 'JJ')],\n",
       " [('apple', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('bad', 'JJ')],\n",
       " [('apples', 'NNS'), ('were', 'VBD'), ('are', 'VBP'), ('bad', 'JJ')]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Penn Treebank POS\n",
    "tags = [t.tags for t in tb]\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then providing `lemmatize()` with part-of-speech tags. Unfortunately it is not as simple as passing the POS tags from above. The reason is that NLTK generates tags base on the <a href=\"https://catalog.ldc.upenn.edu/LDC99T42\">Penn Treebank</a> corpus, which uses different <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">POS</a> tags than the <a href=\"https://wordnet.princeton.edu/documentation/wndb5wn\">Wordnet</a> corpus that `lemmatize()` is based on. \n",
    "\n",
    "We therefore need to map the two POS systems before lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'be', 'good'],\n",
       " ['apple', 'be', 'bad'],\n",
       " ['apple', 'be', 'good'],\n",
       " ['apple', 'be', 'not', 'good'],\n",
       " ['apple', 'be', 'not', 'bad'],\n",
       " ['apple', 'be', 'be', 'bad']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to map Penn Treebank POS to Wordnet POS\n",
    "def pos_conv(pos):\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}    \n",
    "    return tag_dict.get(pos[0], 'n')\n",
    "\n",
    "# Convert Penn Treebank POS to Wordnet POS\n",
    "wordnet_tags = [[[w, pos_conv(pos)] for w, pos in t] for t in tags]\n",
    "\n",
    "# Lemmatize with POS\n",
    "data = [[w.lemmatize(t) for w,t in s] for s in wordnet_tags]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob and NLTK have many other useful features such as spelling correction and translation that you can explore on your own. One particularly useful feature is pre-trained sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentiment(polarity=0.7, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=0.7, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=-0.35, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=0.3499999999999999, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment analysis with TextBlob\n",
    "sentiment =  [t.sentiment for t in tb]\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Chinese Text\n",
    "\n",
    "One major issue with Chinese text is that there is no space between words. Unsurprisingly then, this is a major focus for Chinese natural language processing research.\n",
    "\n",
    "They are multiple libraries for Chinese NLP. Here we will try out `jieba` and `pkuseg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.600 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我愛吃', '北京', '餃子', '。']\n",
      "['我', '愛', '吃', '北京', '餃', '子', '。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "DEBUG 2023-10-18 13:03:55,233 _compat.py:47] Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '愛', '吃', '北京', '餃子', '。']\n",
      "['我', '愛', '吃', '北京', '餃子', '。']\n"
     ]
    }
   ],
   "source": [
    "text = '我愛吃北京餃子。'\n",
    "\n",
    "# jieba default\n",
    "import jieba\n",
    "seg_list = jieba.cut(text) \n",
    "print([w for w in seg_list])\n",
    "\n",
    "# jieba cut all mode\n",
    "import jieba\n",
    "seg_list = jieba.cut(text, cut_all=True) \n",
    "print([w for w in seg_list])\n",
    "\n",
    "# jieba + paddle\n",
    "import paddle\n",
    "paddle.enable_static()\n",
    "jieba.enable_paddle()\n",
    "seg_list = jieba.cut(text,use_paddle=True)\n",
    "print([w for w in seg_list])\n",
    "\n",
    "# pkuseg\n",
    "import spacy_pkuseg as pkuseg\n",
    "seg = pkuseg.pkuseg() \n",
    "seg_list = seg.cut(text)\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are much easier once we have the individual words. For example, we could immediately use ngram on the text.\n",
    "\n",
    "We can also fetch POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n",
      "DEBUG 2023-10-18 13:03:56,423 _compat.py:47] Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('我', 'r'), ('愛', 'v'), ('吃', 'v'), ('北京', 'ns'), ('餃子', 'n'), ('。', 'x')]\n",
      "[('我', 'r'), ('愛', 'v'), ('吃', 'v'), ('北京', 'LOC'), ('餃子', 'n'), ('。', 'v')]\n",
      "[('我', 'r'), ('愛', 'v'), ('吃', 'v'), ('北京', 'ns'), ('餃子', 'n'), ('。', 'w')]\n"
     ]
    }
   ],
   "source": [
    "# jieba\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(text)\n",
    "print([(w,f) for w,f in words])\n",
    "\n",
    "# jieba + paddle\n",
    "import paddle\n",
    "paddle.enable_static()\n",
    "jieba.enable_paddle()\n",
    "words = pseg.cut(text,use_paddle=True)\n",
    "print([(w,f) for w,f in words])\n",
    "\n",
    "# pkuseg\n",
    "seg = pkuseg.pkuseg(postag=True)\n",
    "seg_list = seg.cut(text)\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tags for `pkugseg`:\n",
    "https://github.com/lancopku/pkuseg-python/blob/master/tags.txt\n",
    "\n",
    "For `jieba`:\n",
    "https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Neural Network\n",
    "\n",
    "Below is a simple LSTM neural network model that runs sentiment analysis on the IMDB data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "1000 train sequences\n",
      "1000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (1000, 80)\n",
      "x_test shape: (1000, 80)\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 13:06:28.553715: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-10-18 13:06:28.553741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (scrp-login-2): /proc/driver/nvidia/version does not exist\n",
      "2023-10-18 13:06:28.554307: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/15\n",
      "32/32 [==============================] - 5s 97ms/step - loss: 0.6787 - accuracy: 0.5650 - val_loss: 0.6394 - val_accuracy: 0.6700\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.4354 - accuracy: 0.8650 - val_loss: 0.6220 - val_accuracy: 0.6690\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.1269 - accuracy: 0.9570 - val_loss: 0.7829 - val_accuracy: 0.7210\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0237 - accuracy: 0.9930 - val_loss: 0.8842 - val_accuracy: 0.7020\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.1089 - val_accuracy: 0.7280\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 2.7822e-04 - accuracy: 1.0000 - val_loss: 1.1553 - val_accuracy: 0.7310\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 8.2567e-05 - accuracy: 1.0000 - val_loss: 1.1876 - val_accuracy: 0.7290\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 7.2955e-05 - accuracy: 1.0000 - val_loss: 1.2107 - val_accuracy: 0.7310\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 5.2007e-05 - accuracy: 1.0000 - val_loss: 1.2404 - val_accuracy: 0.7340\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 2.9506e-05 - accuracy: 1.0000 - val_loss: 1.2603 - val_accuracy: 0.7370\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 2.5349e-05 - accuracy: 1.0000 - val_loss: 1.2791 - val_accuracy: 0.7340\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 2.5262e-05 - accuracy: 1.0000 - val_loss: 1.2954 - val_accuracy: 0.7360\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 1.6032e-05 - accuracy: 1.0000 - val_loss: 1.3160 - val_accuracy: 0.7360\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 3s 84ms/step - loss: 1.7886e-05 - accuracy: 1.0000 - val_loss: 1.3270 - val_accuracy: 0.7350\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 1.5272e-05 - accuracy: 1.0000 - val_loss: 1.3377 - val_accuracy: 0.7400\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.3377 - accuracy: 0.7400\n",
      "Test score: 1.3376963138580322\n",
      "Test accuracy: 0.7400000095367432\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from sklearn.utils import resample\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train,y_train,x_test,y_test = resample(x_train,y_train,x_test,y_test,\n",
    "                                         n_samples=1000)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that training a neural network is several orders of magnitude slower than a n-gram model. Furthermore, the neural network model above is not more accurate than our simple n-gram model. One reason is that with so many parameters, neural network models need more than a thousand sample to achieve good results if you are training one from scratch. You can try running the same script with more data on a computer with GPU and see whether you get better results.\n",
    "\n",
    "A much better way to incorporate a neural network models is to use a *pre-trained* language model, \n",
    "which has been trained to understand language based on an enormous amount of text data.\n",
    "The main reason why the models we have tried so far do not work well is that they have to learn\n",
    "English from scratch based on the relatively small number of samples we provide. \n",
    "The use of a pre-trained model circumvent this issue. We will take about the use of pre-trained \n",
    "models in a later lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Readings\n",
    "- <a href=\"https://github.com/dipanjanS/text-analytics-with-python\">Text Analytics with Python</a> (or the <a href=\"https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\">free tutorial</a> by the same author on Towards Data Science.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
