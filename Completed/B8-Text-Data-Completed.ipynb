{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data\n",
    "\n",
    "Version: 2024-10-14\n",
    "\n",
    "Trying to construct models that understand text falls under the field of *natural language processing*. This is a field of enormous practical importance: chatbot, automated translation and generated new articles area few notable applications. In this notebook we will look into some basic ways of processing text data.\n",
    "\n",
    "Below is what you might get in a typical dataset of review data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text data\n",
    "corpus = [\n",
    "    \"This is good.\",\n",
    "    \"This is bad.\",\n",
    "    \"This is very good.\",\n",
    "    \"This is not good.\",\n",
    "    \"This is not bad.\",\n",
    "    \"This is...is bad.\"\n",
    "]\n",
    "\n",
    "ratings = [\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing review data the typical goal is to predict a single value, the rating, from the written text. This is a form of *sentiment analysis*. In the case of chatbot and automated translation, where one single value is not sufficient to represent the meaning of text, a vector is outputed by the model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. N-gram\n",
    "\n",
    "Let us count the number of times each word appears in a sample. This is called *unigram* in natural language processing. To do so, we will use ```CountVectorizer``` of scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0]\n",
      " [1 0 1 0 1 0]\n",
      " [0 1 1 0 1 1]\n",
      " [0 1 1 1 1 0]\n",
      " [1 0 1 1 1 0]\n",
      " [1 0 2 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ```get_feature_names_out()``` to see which word each column represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bad', 'good', 'is', 'not', 'this', 'very'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word-count vector can now be used with a suitable model to conduct language processing. Here we will simply use a logit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "[1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = ratings\n",
    "\n",
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "print(model.score(X,y))\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which phrases do our model have a difficulty understanding? Why might that be the case?\n",
    "\n",
    "Now let us take a look at the estimated coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.37588867e-01  2.37535611e-01 -3.55988077e-01  4.95675399e-05\n",
      "  -5.32559462e-05  3.55506184e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the coefficients of each word. Can you see what is wrong with our model? One thing you might notice is that 'is' has a very negative coefficient while 'very' has very a positive coefficient, even though these words do not have such connotations themselves.  \n",
    "\n",
    "When we start counting combination of words instead of individual words, what we have is *n-gram*. ```CountVectorizer``` allows us to specify the range of words we wish to consider via the option ```ngram_range```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 1]\n",
      " [0 0 0 1 0 0 1 1 0]\n",
      " [0 0 0 1 0 1 0 1 0]\n",
      " [1 0 1 0 0 0 0 1 0]]\n",
      "['is bad' 'is good' 'is is' 'is not' 'is very' 'not bad' 'not good'\n",
      " 'this is' 'very good']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try running the logistic regression again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[-6.65676508e-01  3.78679647e-01 -2.99770295e-01 -3.25313983e-02\n",
      "   3.19546774e-01  3.84908615e-01 -4.17440013e-01  1.85142328e-05\n",
      "   3.19546774e-01]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "print(model.score(X,y))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. IMDB Movie Review\n",
    "\n",
    "Now let us try something real. We will analyse a sample of <a href=\"https://www.imdb.com/\">IMDB</a> movie reviews, trying to predict the rating a user gives based on his written review. For speed reasons we will be using a subsample, but the original text data can be found <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">here</a>.\n",
    "\n",
    "First let us import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "imdb_train = pd.read_csv(\"../Data/imdb_train.csv\",\n",
    "                         names=['label','text'])\n",
    "imdb_test = pd.read_csv(\"../Data/imdb_test.csv\",\n",
    "                         names=['label','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many samples do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(imdb_train.shape)\n",
    "print(imdb_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inside each sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This was an absolutely terrible movie. Don't b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I have been known to fall asleep during films,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mann photographs the Alberta Rocky Mountains i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the kind of film for a snowy Sunday af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>As others have mentioned, all the women that g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  This was an absolutely terrible movie. Don't b...\n",
       "1      0  I have been known to fall asleep during films,...\n",
       "2      0  Mann photographs the Alberta Rocky Mountains i...\n",
       "3      1  This is the kind of film for a snowy Sunday af...\n",
       "4      1  As others have mentioned, all the women that g..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Words are encoded by their frequency-of-apperance ranking in the data. This allows us to easily delete words that either\n",
    "- appear frequently but add little to the meaning of the text (e.g. articles, conjunctions and prepositions), or\n",
    "- appear too infrequently to be of use.-->\n",
    "\n",
    "We will now repeat what we have done previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = imdb_train['label']\n",
    "y_test = imdb_test['label']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x_train = vectorizer.fit_transform(imdb_train['text'])\n",
    "x_test = vectorizer.transform(imdb_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.786\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train,y_train)\n",
    "print(model.score(x_train,y_train))\n",
    "print(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Lemmatization\n",
    "\n",
    "Consider the following corpus of text, modified from the original one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text data\n",
    "corpus2 = [\n",
    "    \"Apple is good.\",\n",
    "    \"Apple was bad.\",\n",
    "    \"Apples are good.\",\n",
    "    \"Apples were not good.\",\n",
    "    \"Apple is not bad.\",\n",
    "    \"Apples were...are bad.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having plurals complicates our analysis: `CountVectorizer` will treat 'Apple' and 'Apples' as two distinct words, unncessarily splitting the samples for apples. Similarly, 'is' and 'are' are both forms of the verb 'to be', so they should be considered as one word. What we need is *lemmatization*, which is the process of grouping together the inflected forms of a word for use in analysis.\n",
    "\n",
    "We will be using <a href=\"https://textblob.readthedocs.io/en/dev/index.html\">TextBlob</a>, a library for processing textual data. TextBlob in turn relies on <a href=\"http://www.nltk.org/\">NLTK</a> (short for *Natural Language ToolKit*) to do some of the heavy lifting. Since NLTK does not come with all packages installed, we will need to first download the ones we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/users/testuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/users/testuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/users/testuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/users/testuser/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process goes as follows:\n",
    "1. First convert each string to a `TextBlob` object. \n",
    "2. Split each string into sentences with the `.sentences` property if needed.\n",
    "3. Split each string (or sentence) into words with the `.words` property.\n",
    "4. Lemmatize each word with the `lemmatize()` method. \n",
    "\n",
    "Note that `lemmatize()` expects words to be in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['apple', 'is', 'good']),\n",
       " WordList(['apple', 'wa', 'bad']),\n",
       " WordList(['apple', 'are', 'good']),\n",
       " WordList(['apple', 'were', 'not', 'good']),\n",
       " WordList(['apple', 'is', 'not', 'bad']),\n",
       " WordList(['apple', 'were', 'are', 'bad'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use TextBlob to lemmatize the corpus\n",
    "from textblob import TextBlob\n",
    "\n",
    "tb = [TextBlob(c.lower()) for c in corpus2]\n",
    "sentences = [t.words for t in tb]\n",
    "data = [s.lemmatize() for s in sentences]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above successfully grouped 'apples' with 'apple', but it failed to group 'is' and 'are'. The second sample gives us some hint as to what went wrong---'was' was somehow converted to 'wa'. What happened was that `lemmatize()` by default treats all words as nouns. To ensure proper conversion, we will need to provide it with each word's part of speech (POS).\n",
    "\n",
    "First, we generate part-of-speech tags by using the `.tags` property of the `TextBlob` object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('apple', 'NN'), ('is', 'VBZ'), ('good', 'JJ')],\n",
       " [('apple', 'NN'), ('was', 'VBD'), ('bad', 'JJ')],\n",
       " [('apples', 'NNS'), ('are', 'VBP'), ('good', 'JJ')],\n",
       " [('apples', 'NNS'), ('were', 'VBD'), ('not', 'RB'), ('good', 'JJ')],\n",
       " [('apple', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('bad', 'JJ')],\n",
       " [('apples', 'NNS'), ('were', 'VBD'), ('are', 'VBP'), ('bad', 'JJ')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Penn Treebank POS\n",
    "tags = [t.tags for t in tb]\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then providing `lemmatize()` with part-of-speech tags. Unfortunately it is not as simple as passing the POS tags from above. The reason is that NLTK generates tags base on the <a href=\"https://catalog.ldc.upenn.edu/LDC99T42\">Penn Treebank</a> corpus, which uses different <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">POS</a> tags than the <a href=\"https://wordnet.princeton.edu/documentation/wndb5wn\">Wordnet</a> corpus that `lemmatize()` is based on. \n",
    "\n",
    "We therefore need to map the two POS systems before lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'be', 'good'],\n",
       " ['apple', 'be', 'bad'],\n",
       " ['apple', 'be', 'good'],\n",
       " ['apple', 'be', 'not', 'good'],\n",
       " ['apple', 'be', 'not', 'bad'],\n",
       " ['apple', 'be', 'be', 'bad']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to map Penn Treebank POS to Wordnet POS\n",
    "def pos_conv(pos):\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}    \n",
    "    return tag_dict.get(pos[0], 'n')\n",
    "\n",
    "# Convert Penn Treebank POS to Wordnet POS\n",
    "wordnet_tags = [[[w, pos_conv(pos)] for w, pos in t] for t in tags]\n",
    "\n",
    "# Lemmatize with POS\n",
    "data = [[w.lemmatize(t) for w,t in s] for s in wordnet_tags]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob and NLTK have many other useful features such as spelling correction and translation that you can explore on your own. One particularly useful feature is pre-trained sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentiment(polarity=0.7, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=0.7, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=-0.35, subjectivity=0.6000000000000001),\n",
       " Sentiment(polarity=0.3499999999999999, subjectivity=0.6666666666666666),\n",
       " Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment analysis with TextBlob\n",
    "sentiment =  [t.sentiment for t in tb]\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Chinese Text\n",
    "\n",
    "One major issue with Chinese text is that there is no space between words. Unsurprisingly then, this is a major focus for Chinese natural language processing research.\n",
    "\n",
    "They are multiple libraries for Chinese NLP. Here we will try out `jieba` and `pkuseg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.657 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我愛吃', '北京', '餃子', '。']\n",
      "['我', '愛', '吃', '北京', '餃', '子', '。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Import error, cannot find paddle.fluid and jieba.lac_small.predict module. Now, back to jieba basic cut......\n",
      "[2024-10-14 12:43:28,023] [   DEBUG] _compat.py:50 - Import error, cannot find paddle.fluid and jieba.lac_small.predict module. Now, back to jieba basic cut......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我愛吃', '北京', '餃子', '。']\n",
      "['我', '愛', '吃', '北京', '餃子', '。']\n"
     ]
    }
   ],
   "source": [
    "text = '我愛吃北京餃子。'\n",
    "\n",
    "# jieba default\n",
    "import jieba\n",
    "seg_list = jieba.cut(text) \n",
    "print([w for w in seg_list])\n",
    "\n",
    "# jieba cut all mode\n",
    "import jieba\n",
    "seg_list = jieba.cut(text, cut_all=True) \n",
    "print([w for w in seg_list])\n",
    "\n",
    "# jieba + paddle\n",
    "import paddle\n",
    "paddle.enable_static()\n",
    "jieba.enable_paddle()\n",
    "seg_list = jieba.cut(text,use_paddle=True)\n",
    "print([w for w in seg_list])\n",
    "\n",
    "# pkuseg\n",
    "import spacy_pkuseg as pkuseg\n",
    "seg = pkuseg.pkuseg() \n",
    "seg_list = seg.cut(text)\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are much easier once we have the individual words. For example, we could immediately use ngram on the text.\n",
    "\n",
    "We can also fetch POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Import error, cannot find paddle.fluid and jieba.lac_small.predict module. Now, back to jieba basic cut......\n",
      "[2024-10-14 12:43:29,189] [   DEBUG] _compat.py:50 - Import error, cannot find paddle.fluid and jieba.lac_small.predict module. Now, back to jieba basic cut......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('我', 'r'), ('愛', 'v'), ('吃', 'v'), ('北京', 'ns'), ('餃子', 'n'), ('。', 'x')]\n",
      "[('我', 'r'), ('愛', 'v'), ('吃', 'v'), ('北京', 'ns'), ('餃子', 'n'), ('。', 'x')]\n",
      "[('我', 'r'), ('愛', 'v'), ('吃', 'v'), ('北京', 'ns'), ('餃子', 'n'), ('。', 'w')]\n"
     ]
    }
   ],
   "source": [
    "# jieba\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(text)\n",
    "print([(w,f) for w,f in words])\n",
    "\n",
    "# jieba + paddle\n",
    "import paddle\n",
    "paddle.enable_static()\n",
    "jieba.enable_paddle()\n",
    "words = pseg.cut(text,use_paddle=True)\n",
    "print([(w,f) for w,f in words])\n",
    "\n",
    "# pkuseg\n",
    "seg = pkuseg.pkuseg(postag=True)\n",
    "seg_list = seg.cut(text)\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tags for `pkugseg`:\n",
    "https://github.com/lancopku/pkuseg-python/blob/master/tags.txt\n",
    "\n",
    "For `jieba`:\n",
    "https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Neural Network\n",
    "\n",
    "Below is a simple LSTM neural network model that runs sentiment analysis on the IMDB data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 12:43:30.665821: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-14 12:43:30.665943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-14 12:43:30.699404: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-14 12:43:30.771625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 12:43:32.833444: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "1000 train sequences\n",
      "1000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (1000, 80)\n",
      "x_test shape: (1000, 80)\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 12:43:37.664373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22361 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2024-10-14 12:43:38.000351: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: Permission denied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 12:43:40.463256: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-10-14 12:43:40.927002: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f8e7a0e05b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-14 12:43:40.927037: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-10-14 12:43:40.938480: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728881021.039628 3529329 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 6s 102ms/step - loss: 0.6838 - accuracy: 0.5520 - val_loss: 0.6425 - val_accuracy: 0.6890\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 2s 73ms/step - loss: 0.4528 - accuracy: 0.8270 - val_loss: 0.5539 - val_accuracy: 0.7210\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 2s 53ms/step - loss: 0.1439 - accuracy: 0.9620 - val_loss: 0.7331 - val_accuracy: 0.7390\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.0307 - accuracy: 0.9930 - val_loss: 1.0014 - val_accuracy: 0.7350\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 2s 51ms/step - loss: 0.0107 - accuracy: 0.9990 - val_loss: 0.9336 - val_accuracy: 0.7290\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.2709 - val_accuracy: 0.7420\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 2.3477e-04 - accuracy: 1.0000 - val_loss: 1.1420 - val_accuracy: 0.7470\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 5.6955e-05 - accuracy: 1.0000 - val_loss: 1.1703 - val_accuracy: 0.7450\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 3.8300e-05 - accuracy: 1.0000 - val_loss: 1.2088 - val_accuracy: 0.7470\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.4399e-05 - accuracy: 1.0000 - val_loss: 1.2346 - val_accuracy: 0.7450\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 2.0224e-05 - accuracy: 1.0000 - val_loss: 1.2586 - val_accuracy: 0.7430\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 1.7535e-05 - accuracy: 1.0000 - val_loss: 1.2819 - val_accuracy: 0.7400\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.8123e-05 - accuracy: 1.0000 - val_loss: 1.3013 - val_accuracy: 0.7410\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.2677e-05 - accuracy: 1.0000 - val_loss: 1.3216 - val_accuracy: 0.7420\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 8.6147e-06 - accuracy: 1.0000 - val_loss: 1.3359 - val_accuracy: 0.7450\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.3359 - accuracy: 0.7450\n",
      "Test score: 1.3359293937683105\n",
      "Test accuracy: 0.7450000047683716\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.utils import resample\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train,y_train,x_test,y_test = resample(x_train,y_train,x_test,y_test,\n",
    "                                         n_samples=1000)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that training a neural network is several orders of magnitude slower than a n-gram model. Furthermore, the neural network model above is not more accurate than our simple n-gram model. One reason is that with so many parameters, neural network models need more than a thousand sample to achieve good results if you are training one from scratch. You can try running the same script with more data on a computer with GPU and see whether you get better results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Large Language Models\n",
    "\n",
    "A much better way to incorporate a neural network models is to use a *pre-trained* language model, \n",
    "which has been trained to understand language based on an enormous amount of text data.\n",
    "The main reason why the models we have tried so far do not work well is that they have to learn\n",
    "English from scratch based on the relatively small number of samples we provide. \n",
    "The use of a pre-trained model circumvent this issue. \n",
    "\n",
    "We will go into the details of such pre-trained models in a later lecture.\n",
    "Here we will simply have a demo. As running these models are very computationally intensive,\n",
    "you should run the following code with access to GPU, otherwise it is going to be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.882"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run with GPU! This is very slow on CPU.\n",
    "\n",
    "# Data\n",
    "x_test = imdb_test['text'].tolist()\n",
    "y_test = imdb_test['label'].tolist()\n",
    "\n",
    "# Use a pre-trained text classifier through Hugging Face transformer library\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', device=0) # device=0 means use (first) GPU\n",
    "tokenizer_kwargs = {'truncation':True,'max_length':512} # Truncate to 512 tokens\n",
    "results = classifier(x_test,**tokenizer_kwargs) # Returns a text label\n",
    "y_predicted = [ 1 if x['label']=='POSITIVE' else 0 for x in results] # Convert to 1 or 0\n",
    "\n",
    "# Measure accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G. Accessing Language Models through API\n",
    "\n",
    "Instead of running your own copy pf a pre-trained language model,\n",
    "you can defer the task to a remotely-hosted model. The most well-known\n",
    "remotely-hosted models are OpenAI's GPTs and Anthropic's Claude.\n",
    "Here we will use Meta's Llama 3.1, hosted on CUHK Department of Economics'\n",
    "servers. \n",
    "\n",
    "Note that while this is the most powerful and convenient method of conducting \n",
    "text classification, it is also the slowest&mdash;you pay a price for being able \n",
    "to give the model instuctions in plain language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification through API\n",
    "\n",
    "# Need to supply your own API key\n",
    "api_key = 'your-key-here'\n",
    "\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Data - using only 30 samples for speed reasons\n",
    "x_test = imdb_test['text'].head(30).tolist()\n",
    "y_test = imdb_test['label'].head(30).tolist()\n",
    "\n",
    "# OpenAI API\n",
    "client = OpenAI(\n",
    "    base_url = 'https://scrp-chat.econ.cuhk.edu.hk/api',\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "# Function for running the inference\n",
    "def f(x):\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"llama3.1:8b-instruct-q5_K_M\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"\"\"Please classify if the given text's sentiment is positive or negative.\n",
    "                       If it is positive, return 1. Otherwise return 0. \n",
    "                       Show only the finally answer, do not show your reasoning.\n",
    "                    \"\"\"},        \n",
    "        {\"role\": \"user\", \"content\": x}\n",
    "      ],\n",
    "      temperature=0  \n",
    "    )\n",
    "    return int(response.choices[0].message.content)\n",
    "\n",
    "# Loop through data\n",
    "y_predicted = [f(x) for x in x_test]\n",
    "\n",
    "# Measure accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\",accuracy_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Readings\n",
    "- <a href=\"https://github.com/dipanjanS/text-analytics-with-python\">Text Analytics with Python</a> (or the <a href=\"https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\">free tutorial</a> by the same author on Towards Data Science.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
