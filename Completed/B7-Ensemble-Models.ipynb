{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55404ed-b582-4c31-ad37-d83b21c9e89c",
   "metadata": {},
   "source": [
    "# Ensemble Methods (Work-in-Progress)\n",
    "\n",
    "Ensemble Methods combine multiple models to give better performance. There are two main ways to do this:\n",
    "- **Averaging and Voting**: take the average predictions of different models or hold a majority vote.\n",
    "- **Boosting**: iteratively train additional models, targeting samples that have not been predicted correctly in previous steps.\n",
    "\n",
    "<!--https://towardsdatascience.com/lightgbm-vs-xgboost-which-algorithm-win-the-race-1ff7dd4917d-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff48dcb6-b1ce-423c-b7b2-f6d7ccb7f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "imdb_train = pd.read_csv(\"../Data/imdb_train.csv\",\n",
    "                         names=['label','text'])\n",
    "imdb_test = pd.read_csv(\"../Data/imdb_test.csv\",\n",
    "                         names=['label','text'])\n",
    "\n",
    "# Target\n",
    "y_train = imdb_train['label']\n",
    "y_test = imdb_test['label']\n",
    "\n",
    "# Features\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(imdb_train['text'])\n",
    "X_test = vectorizer.transform(imdb_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e78011d4-b9cb-4d3e-9055-fd59f7000106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999\n",
      "0.795\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=0.11,max_iter=1000)\n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c1ad329b-d812-4e4e-aeac-56b3a0809051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999\n",
      "0.795\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=0.11,max_iter=1000)\n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "275f0f62-9aab-4ebe-b203-fe0cb0e52705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.658\n",
      "0.604\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=25)\n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "71be4649-b7f7-4cd8-b5cc-4a0fc673cf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999\n",
      "0.791\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(C=0.008)\n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6abde-1cb2-45b4-824a-9f0300b8c651",
   "metadata": {},
   "source": [
    "## A. Averaging Methods\n",
    "\n",
    "Averaging methods take the average predictions of different models or hold a majority vote.\n",
    "The attraction of these methods is they work with all models, including highly complex ones. The downside is, they do not take additional steps to address samples in which the overall model is not performing well.\n",
    "\n",
    "\n",
    "### Regression - Averaging\n",
    "\n",
    "For regression task, a simple way to construct an ensemble is to use the average prediction of multiple models. \n",
    "\n",
    "You can combine pretty much any number of models with `sklearn.ensemble.VotingRegressor`. Simply provide a list of estimators when you create the ensemble:\n",
    "```python\n",
    "VotingClassifier(estimators=[\n",
    "    (name_1, model_1),\n",
    "    (name_2, model_2),\n",
    "    ...\n",
    "    ],\n",
    "    weights)                 \n",
    "```\n",
    "`weights` is an optional list specifying the weight each model should carry. Default is equal weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994428f5-3a95-47e6-ac01-e21764a8a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabb165-306d-4e3b-8663-3a3c8db23cda",
   "metadata": {},
   "source": [
    "### Classification - Voting\n",
    "\n",
    "For classification, a simple way to construct an ensemble is to have multiple models vote for the prediction. There are two main ways to do this:\n",
    "- **Majority voting**: use the most common prediction among all included models as the final prediction. \n",
    "- **Soft voting**: sum the predicted *probabilities* of all included models and use that to make the final prediction.\n",
    "\n",
    "Similar to regression task, You can combine pretty much any number of models with `sklearn.ensemble.VotingClassifier`. Simply provide a list of estimators when you create the ensemble:\n",
    "```python\n",
    "VotingClassifier(estimators=[\n",
    "    (name_1, model_1),\n",
    "    (name_2, model_2),\n",
    "    ...\n",
    "    ],\n",
    "    voting, weights)                 \n",
    "```\n",
    "\n",
    "Besides setting `weights`, for classification you can also specify the voting method. Set `voting` to 'hard' for majority voting and 'soft' for soft voting. Default is 'hard'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a91b36fe-e9be-4fd1-b29c-44a054a23082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999\n",
      "0.795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model_1 = LogisticRegression(C=0.11,max_iter=1000)\n",
    "model_2 = KNeighborsClassifier(n_neighbors=25)\n",
    "model_3 = LinearSVC(C=0.008)\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "                            ('logit', model_1),    \n",
    "                            ('knn', model_2),\n",
    "                            ('svc',model_3)]\n",
    "                           )\n",
    "ensemble.fit(X_train,y_train)\n",
    "print(ensemble.score(X_train,y_train))\n",
    "print(ensemble.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abb19f-23e2-471d-9d4a-69df016d0318",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "Instead of having different model classes, we could alternatively train multiple copies of the same model on different samples of data. The typical way to do this is to *bootstrap* the training data&mdash;in other words, resampling the data with replacement.\n",
    "\n",
    "The scikit-learn classes for bagging are `sklearn.ensemble.BaggingRegressor` and `sklearn.ensemble.BaggingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4205249a-f128-430e-b45a-8e3d5a4bb5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992\n",
      "0.782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "ensemble = BaggingClassifier(base_estimator=model_1,n_jobs=4)\n",
    "ensemble.fit(X_train,y_train)\n",
    "print(ensemble.score(X_train,y_train))\n",
    "print(ensemble.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2274d-50f8-459c-849b-cc47289d4420",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "A particularly important averaging model is random forest, which is an ensemble of decision trees. As we have seen previously, decision trees are prone to overfitting due to their ability to partition data down to each individual sample. Random forest overcome this problem in two ways:\n",
    "1. Use soft voting from a large number of trees.\n",
    "2. Train each tree on a bootstrapped sample of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "58727d16-d8c2-4e98-9c87-8b19aa725336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_jobs=4)\n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd050ec-d125-405d-b247-b7f5198e47d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## B. Boosting Methods\n",
    "Boosting methods iteratively train additional models, targeting samples that have not been predicted correctly in previous steps. The models boosting methods use are typically simple&mdash;or *weak*&mdash;such as decision tree with limited branches. The power of these methods comes from combining multiple weak models with targeted training.\n",
    "\n",
    "Among the various boosting methods, gradient boosting is the most popular. So popular, in fact, that it is considered the go-to method before you consider using artificial neural networks. There are four main gradient boosting implementations:\n",
    "- Scikit-learn's `GradientBoostingClassifier`: simple baseline implementation, not really used in practice.\n",
    "- `xgboost`: most well known.\n",
    "- `lightgbm`: fastest.\n",
    "- `catboost`: good defaults.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a921977e-0ee2-4125-9264-a07a249c6232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.886\n",
      "0.775\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(max_depth=1,learning_rate=0.35,n_jobs=4)\n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "88dac36e-d105-4fff-b0b3-26c07152d84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.886\n",
      "0.775\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = XGBClassifier(max_depth=1,learning_rate=0.35,n_jobs=4)\n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c83a1847-4b38-4dab-b8b7-30fae910227d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.784\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(max_depth=1,learning_rate=0.35,thread_count=4)\n",
    "model.fit(X_train,y_train,verbose=False)\n",
    "print(model.score(X_train,y_train))\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb8e6c-c50f-4942-8c3a-1821895132a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
