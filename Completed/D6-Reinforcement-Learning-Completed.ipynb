{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5181d68b-2c95-4247-909f-4bea08c67be9",
   "metadata": {},
   "source": [
    "## A. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b8435c4-1fa1-47fd-8032-2fe63373931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class matching_pennies(gym.Env):\n",
    "    \"\"\"\n",
    "    A matching penny game\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.opponent_action_space = spaces.Discrete(2)\n",
    "        self.opponent_action = None\n",
    "        \n",
    "    def step(self,action):\n",
    "        # reward is 1 if matched opponent's previous action and 0 otherwise\n",
    "        reward = 1 if action == self.opponent_action else 0\n",
    "        # observation is what the opponent has used\n",
    "        observation = self._opponent_action()\n",
    "        # game never terminates\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = None\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info    \n",
    "        \n",
    "    def reset(self,seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        return self._opponent_action(), None\n",
    "\n",
    "    def _opponent_action(self):\n",
    "        self.opponent_action = self.opponent_action_space.sample()\n",
    "        return self.opponent_action\n",
    "\n",
    "    def to_onehot(self,action,dim):\n",
    "        a = np.zeros(dim,dtype=np.float32)\n",
    "        a[action] = 1.0\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860d40a1-5814-4950-8ac1-331fa9a5442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (agent opponent): 1 0 reward: 0 New obs: 1\n",
      "actions (agent opponent): 0 1 reward: 0 New obs: 1\n",
      "actions (agent opponent): 1 1 reward: 1 New obs: 0\n",
      "actions (agent opponent): 0 0 reward: 1 New obs: 0\n",
      "actions (agent opponent): 0 0 reward: 1 New obs: 1\n"
     ]
    }
   ],
   "source": [
    "env = matching_pennies()\n",
    "observation, info = env.reset(seed=42)\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions (agent opponent):\",action,obs_prev,\"reward:\",reward,\"New obs:\",observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377520d-3b34-439a-80ca-331385943416",
   "metadata": {},
   "source": [
    "## B. Policy Network\n",
    "\n",
    "### B1. The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60bf42d5-776b-4941-92a8-466d8ab7c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, cuda, optim\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"A simple policy based on a fully-connected network.\"\"\"\n",
    "    def __init__(self, action_dim, state_dim, hidden_dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Use GPU if available, otherwise use CPU\n",
    "        self.device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax() # Output needs to be probabilities of each action \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.logit(x)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        probs = self.forward(state)\n",
    "        action = probs.multinomial(num_samples=1,replacement=True)\n",
    "        return action, probs[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56bf70-43c1-4310-aecb-bd3d41c1f1ae",
   "metadata": {},
   "source": [
    "Let us try using this policy to interact with the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291dda31-6229-4bfb-8793-3e0f4b9a9cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "351e1681-73fe-4ea1-9a42-36a9db8d881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "actions: [0] 0 reward: 1 obs: 1 log_prob: [0.18378031]\n",
      "actions: [0] 1 reward: 0 obs: 0 log_prob: [0.18378031]\n",
      "actions: [1] 0 reward: 0 obs: 1 log_prob: [0.8162197]\n",
      "actions: [1] 1 reward: 1 obs: 0 log_prob: [0.8162197]\n",
      "actions: [1] 0 reward: 0 obs: 1 log_prob: [0.8162197]\n"
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "env = matching_pennies()\n",
    "\n",
    "# Create policy\n",
    "policy = PolicyNetwork(env.action_space.n,env.opponent_action_space.n) #.to(device)\n",
    "\n",
    "# Reset environment and obtain initial state\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Main Loop\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "\n",
    "    # The neural network requires the state to be in one-hot encoding\n",
    "    state_onehot = torch.from_numpy(env.to_onehot(obs_prev,env.opponent_action_space.n))\n",
    "\n",
    "    # Sample action\n",
    "    action, prob = policy.get_action(state_onehot)\n",
    "\n",
    "    # Convert action from PyTorch tensor to numpy\n",
    "    action = action.detach().numpy()\n",
    "\n",
    "    # Interact with environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions:\",action,obs_prev,\"reward:\",reward,\"obs:\",observation,\"log_prob:\",prob.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e7c30-34ee-4e90-99bf-406012dc3afe",
   "metadata": {},
   "source": [
    "We can see that the policy's performance is quite poor. This is expected, as we have yet to train the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa242f-e0e8-4593-9c6a-051f280d0e13",
   "metadata": {},
   "source": [
    "### B2. Training the Policy Network with the REINFORCE Algorithm\n",
    "\n",
    "https://link.springer.com/article/10.1007/BF00992696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f76e769d-3915-49ef-ac1e-189fbfd4850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE_loss(policy, rewards, log_probs, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Loss function based on the REINFORCE algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute discounted rewards\n",
    "    discounted_rewards = []\n",
    "    for t in range(len(rewards)):\n",
    "        r = torch.flip(rewards[t:],[0]) # rewards in descending order\n",
    "        gammas = torch.flip(torch.cumprod(torch.full([len(r)],gamma),dim=0),[0]) # discount factors in descending order\n",
    "        r_npv = torch.sum(r * gammas) # Take dot product and sum\n",
    "        discounted_rewards.append(r_npv)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "\n",
    "    # Standardize discounted rewards\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) \n",
    "\n",
    "    # Compute loss\n",
    "    loss = -1 * torch.dot(log_probs,discounted_rewards)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "571e0c59-6472-426f-95ac-4424e29c24fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Epoch:    0 Loss: 0.11626309 Mean reward: 0.6\n",
      "Epoch:  100 Loss: -0.8199182 Mean reward: 0.4\n",
      "Epoch:  200 Loss: -1.1424112 Mean reward: 0.6\n",
      "Epoch:  300 Loss: -0.25469792 Mean reward: 0.7\n",
      "Epoch:  400 Loss: -2.2170753 Mean reward: 0.9\n",
      "Epoch:  500 Loss: -2.707262 Mean reward: 0.8\n",
      "Epoch:  600 Loss: -3.4863317 Mean reward: 0.8\n",
      "Epoch:  700 Loss: -1.6021891 Mean reward: 0.9\n",
      "Epoch:  800 Loss: 0.030924983 Mean reward: 1.0\n",
      "Epoch:  900 Loss: -1.7986401 Mean reward: 1.0\n",
      "Epoch: 1000 Loss: -5.0884314 Mean reward: 0.9\n",
      "Epoch: 1100 Loss: 2.4223878 Mean reward: 1.0\n",
      "Epoch: 1200 Loss: -0.010129217 Mean reward: 1.0\n",
      "Epoch: 1300 Loss: -2.4018488 Mean reward: 1.0\n",
      "Epoch: 1400 Loss: -0.010212412 Mean reward: 1.0\n",
      "Epoch: 1500 Loss: 0.012451749 Mean reward: 1.0\n",
      "Epoch: 1600 Loss: 0.015387452 Mean reward: 1.0\n",
      "Epoch: 1700 Loss: 0.0010852888 Mean reward: 1.0\n",
      "Epoch: 1800 Loss: 4.445414 Mean reward: 0.9\n",
      "Epoch: 1900 Loss: 4.552398e-05 Mean reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Settings\n",
    "epochs = 2000\n",
    "steps = 20\n",
    "learning_rate=2e-3\n",
    "gamma = 0.9\n",
    "\n",
    "# Create environment, policy and optimizer\n",
    "env = matching_pennies()\n",
    "policy = PolicyNetwork(env.action_space.n,env.opponent_action_space.n,5) #.to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "policy.train()\n",
    "\n",
    "observation, info = env.reset()\n",
    "for i in range(epochs):\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    # REINFORCE needs multiple observations to update policy\n",
    "    for s in range(steps):   \n",
    "        obs_prev = observation\n",
    "        state_onehot = torch.from_numpy(env.to_onehot(obs_prev,env.opponent_action_space.n))\n",
    "        action, prob = policy.get_action(state_onehot)\n",
    "        action = action.detach().numpy()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        log_probs.append(torch.log(prob))\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "    \n",
    "    # Convert reward and log_prob lists to tensors and move to device\n",
    "    rewards_pt = torch.tensor(rewards) #.to(device)\n",
    "    log_probs = torch.stack(log_probs).flatten() \n",
    "    loss = REINFORCE_loss(policy, rewards_pt, log_probs, gamma=gamma)\n",
    "\n",
    "    # Back propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i %100 == 0:\n",
    "        # Print status every 100 epochs\n",
    "        print(\"Epoch:\",str(i).rjust(4),\"Loss:\",loss.detach().numpy(),\"Mean reward:\",round(np.mean(rewards),1))\n",
    "\n",
    "    if terminated:\n",
    "        # If the environment reached the termianate, reset it\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f26f5-9032-4371-b540-e9bcd1b8e89c",
   "metadata": {},
   "source": [
    "As we can see, with reinforcement learning performance tends to fluctuate quite a bit even after extensive training,\n",
    "so early stoppoing and model checkpointing is necessary to get good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b795f-4203-48b1-add3-854510789849",
   "metadata": {},
   "source": [
    "## C. Q Learning\n",
    "\n",
    "Q Learning does not use the model to approximate the policy. Instead, it uses the model to \n",
    "estimate the net present value of the expected future stream of rewards for each action.\n",
    "The action that maximizes the estimated NPV is then carried out.\n",
    "\n",
    "https://arxiv.org/abs/1312.5602\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "5cefc221-2ec4-44b5-95db-3c741dceb8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, cuda, optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Q value estimator based on a fully-connected network.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_dim, state_dim, hidden_dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "        # Use GPU if available, otherwise use CPU\n",
    "        self.device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        self.qvals = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim) # Output is the est. Q value of each action\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = self.state_to_onehot(state)\n",
    "        return self.qvals(state)\n",
    "\n",
    "    def get_action(self, state, epsilon=0):\n",
    "        # Input: a state\n",
    "        # Output: the value-maximizing action and the estimated value\n",
    "        values = self.forward([state]).detach().numpy().flatten()\n",
    "        action = np.argmax(values)\n",
    "\n",
    "        # With probability epsilon select a random action\n",
    "        if epsilon > 0 and np.random.binomial(1,epsilon) == 1:\n",
    "            action = np.random.randint(len(values))\n",
    "        \n",
    "        return action, values[action]\n",
    "\n",
    "    def state_to_onehot(self,state):\n",
    "        # The neural network requires the state to be in one-hot encoding\n",
    "        # This function converts a vector of integer states to one-hot encoding\n",
    "        state = np.asarray(state).astype(int)\n",
    "        onehot = np.zeros((len(state),self.state_dim),dtype=np.float32)\n",
    "        onehot[np.arange(len(state)),state] = 1\n",
    "        return torch.tensor(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb6daa-5fdc-498c-9388-1b0f08d106a2",
   "metadata": {},
   "source": [
    "Let us try using this policy to interact with the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "51dca4f6-0ac3-4f6a-996a-f963706b3c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "actions: 1 1 reward: 1 obs: 0 value: 0.5923747\n",
      "actions: 1 0 reward: 0 obs: 0 value: 0.5923747\n",
      "actions: 1 0 reward: 0 obs: 0 value: 0.5923747\n",
      "actions: 1 0 reward: 0 obs: 0 value: 0.5923747\n",
      "actions: 1 0 reward: 0 obs: 0 value: 0.5923747\n"
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "env = matching_pennies()\n",
    "\n",
    "# Create the Q network\n",
    "Q = DQN(env.action_space.n,env.opponent_action_space.n) #.to(device)\n",
    "\n",
    "# Reset environment and obtain initial state\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Main Loop\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "\n",
    "    # Sample action\n",
    "    action, value = Q.get_action(obs_prev,epsilon=0)\n",
    "\n",
    "    # Interact with environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions:\",action,obs_prev,\"reward:\",reward,\"obs:\",observation,\"value:\",value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "657536d7-caaf-4442-8d8c-51678f0adf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n",
      "Epoch:  300 Loss: 0.20024265 Mean reward: 0.4\n",
      "Epoch:  400 Loss: 0.21400134 Mean reward: 0.5\n",
      "Epoch:  500 Loss: 0.34511584 Mean reward: 0.5\n",
      "Epoch:  600 Loss: 0.316028 Mean reward: 0.4\n",
      "Epoch:  700 Loss: 0.18209685 Mean reward: 0.5\n",
      "Epoch:  800 Loss: 0.19084479 Mean reward: 0.5\n",
      "Epoch:  900 Loss: 0.2627156 Mean reward: 0.3\n",
      "Epoch: 1000 Loss: 0.22860463 Mean reward: 0.4\n",
      "Epoch: 1100 Loss: 0.40016484 Mean reward: 0.6\n",
      "Epoch: 1200 Loss: 0.20767276 Mean reward: 0.4\n",
      "Epoch: 1300 Loss: 0.13677607 Mean reward: 0.6\n",
      "Epoch: 1400 Loss: 0.0608093 Mean reward: 0.8\n",
      "Epoch: 1500 Loss: 0.00490437 Mean reward: 1.0\n",
      "Epoch: 1600 Loss: 0.03430771 Mean reward: 1.0\n",
      "Epoch: 1700 Loss: 0.020101698 Mean reward: 0.9\n",
      "Epoch: 1800 Loss: 0.0052849497 Mean reward: 1.0\n",
      "Epoch: 1900 Loss: 0.00017615841 Mean reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Settings\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "replay_size = 200\n",
    "learning_rate=2e-3\n",
    "gamma = 0.9\n",
    "\n",
    "# Create environment, Q network and optimizer\n",
    "env = matching_pennies()\n",
    "Q = DQN(env.action_space.n,env.opponent_action_space.n,5) #.to(device)\n",
    "Q_target = DQN(env.action_space.n,env.opponent_action_space.n,5) #.to(device)\n",
    "optimizer = optim.Adam(Q.parameters(), lr=learning_rate)\n",
    "\n",
    "Q.train() # Set the PyTorch model to training mode\n",
    "\n",
    "# Initialize memory for experience reply\n",
    "replay_memory = np.empty((replay_size,4),dtype=float)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "for i in range(epochs):\n",
    "    \n",
    "    obs_prev = observation\n",
    "    action, value = Q.get_action(obs_prev,epsilon=min(0.1,(epochs - i)/epochs))\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Add new experience to replay memory. Note the index of each item in memory\n",
    "    j = i % replay_size \n",
    "    replay_memory[j] = [obs_prev,action,reward,observation]\n",
    "\n",
    "    if i % 32 == 0:\n",
    "        # Periodically copy parameters to target network\n",
    "        for param, target_param in zip(Q.parameters(), Q_target.parameters()):\n",
    "            target_param.data.copy_(param)\n",
    "\n",
    "    if i > replay_size:\n",
    "   \n",
    "        # Sample replay memory for gradient descent\n",
    "        replay_idx = np.random.choice(replay_size,batch_size)\n",
    "        replay_samples = torch.tensor(replay_memory[replay_idx],dtype=torch.float) #.to(device)\n",
    "\n",
    "        s_t = replay_samples[:,0]\n",
    "        actions = replay_samples[:,1].int()\n",
    "        rewards = replay_samples[:,2]\n",
    "        s_t1 = replay_samples[:,3]\n",
    "            \n",
    "        # Compute temporal difference loss\n",
    "        q = Q(s_t)[np.arange(batch_size),actions]\n",
    "        q_target = torch.max(Q_target(s_t1),dim=1)[0]\n",
    "        y = rewards + gamma * q_target\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(q, y)\n",
    "            \n",
    "        # Back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # Print status every 100 epochs\n",
    "            print(\"Epoch:\",str(i).rjust(4),\n",
    "                  \"Loss:\",loss.detach().numpy(),\n",
    "                  \"Mean reward:\",round(np.mean(rewards.detach().numpy()),1))\n",
    "\n",
    "    if terminated:\n",
    "        # If the environment reached the termianate, reset it\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "e3f5dca4-3860-4b7e-9060-f8a349ef3b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: 0 0 reward: 1 obs: 0 value: 9.970876\n",
      "actions: 0 0 reward: 1 obs: 1 value: 9.970876\n",
      "actions: 1 1 reward: 1 obs: 0 value: 9.970706\n",
      "actions: 0 0 reward: 1 obs: 1 value: 9.970876\n",
      "actions: 1 1 reward: 1 obs: 0 value: 9.970706\n"
     ]
    }
   ],
   "source": [
    "# Main Loop\n",
    "observation, info = env.reset()\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "\n",
    "    # Sample action\n",
    "    action, value = Q.get_action(obs_prev,epsilon=0)\n",
    "\n",
    "    # Interact with environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions:\",action,obs_prev,\"reward:\",reward,\"obs:\",observation,\"value:\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce0add42-fee3-4c93-814d-c7d1d0a60fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class market:\n",
    "    def __init__(self):\n",
    "        self.action_space = [0,1]\n",
    "    def step(self):\n",
    "        # observation is price and quantity transacted\n",
    "        observation = [p, q]\n",
    "        # reward is profit\n",
    "        reward = [pi]\n",
    "        # terminated\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16bd28-6048-468b-8e78-d85bc7c1f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class intertemporal_tradeoff:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
