{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5181d68b-2c95-4247-909f-4bea08c67be9",
   "metadata": {},
   "source": [
    "## A. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "2b8435c4-1fa1-47fd-8032-2fe63373931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class matching_pennies(gym.Env):\n",
    "    \"\"\"\n",
    "    A matching penny game\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.opponent_action_space = spaces.Discrete(2)\n",
    "        self.opponent_action = None\n",
    "        \n",
    "    def step(self,action):\n",
    "        # reward is 1 if matched opponent's previous action and 0 otherwise\n",
    "        reward = 1 if action == self.opponent_action else 0\n",
    "        # observation is what the opponent is using next\n",
    "        observation = self.opponent_step()\n",
    "        # game never terminates\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = None\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info    \n",
    "        \n",
    "    def reset(self,seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        return self.opponent_step(), None\n",
    "\n",
    "    def opponent_step(self):\n",
    "        self.opponent_action = self.opponent_action_space.sample()\n",
    "        return self.opponent_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "860d40a1-5814-4950-8ac1-331fa9a5442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (agent opponent): 0 1 reward: 0 New obs: 0\n",
      "actions (agent opponent): 0 0 reward: 1 New obs: 1\n",
      "actions (agent opponent): 1 1 reward: 1 New obs: 0\n",
      "actions (agent opponent): 0 0 reward: 1 New obs: 0\n",
      "actions (agent opponent): 0 0 reward: 1 New obs: 0\n"
     ]
    }
   ],
   "source": [
    "env = matching_pennies()\n",
    "observation, info = env.reset(seed=42)\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions (agent opponent):\",action,obs_prev,\"reward:\",reward,\"New obs:\",observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377520d-3b34-439a-80ca-331385943416",
   "metadata": {},
   "source": [
    "## B. Policy Network\n",
    "\n",
    "### B1. The Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "60bf42d5-776b-4941-92a8-466d8ab7c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, cuda, optim\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"A simple policy based on a fully-connected network.\"\"\"\n",
    "    def __init__(self, action_dim, state_dim, hidden_dim=1, state_onehot=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.state_onehot = state_onehot\n",
    "\n",
    "        # Use GPU if available, otherwise use CPU\n",
    "        self.device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        self.logit = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax() # Output needs to be probabilities of each action \n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if self.state_onehot:\n",
    "            # Convert input to onehot encoding\n",
    "            state = self.state_to_onehot(state)\n",
    "        elif not isinstance(state, torch.Tensor):\n",
    "            # Convert input to tensor\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "        return self.logit(state)    \n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Input: a state\n",
    "        # Output: a random action based on the policy's probability distribution\n",
    "        probs = torch.flatten(self.forward([state]))\n",
    "        action = probs.multinomial(num_samples=1,replacement=True)\n",
    "        return action.detach().numpy()[0], probs[action]\n",
    "\n",
    "    def state_to_onehot(self,state):\n",
    "        # The neural network requires the state to be in one-hot encoding\n",
    "        # This function converts a vector of integer states to one-hot encoding\n",
    "        state = np.asarray(state).astype(int)\n",
    "        onehot = np.zeros((len(state),self.state_dim),dtype=np.float32)\n",
    "        onehot[np.arange(len(state)),state] = 1\n",
    "        return torch.tensor(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56bf70-43c1-4310-aecb-bd3d41c1f1ae",
   "metadata": {},
   "source": [
    "Let us try using this policy to interact with the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "351e1681-73fe-4ea1-9a42-36a9db8d881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "actions: 1 1 reward: 1 obs: 0 log_prob: [0.654229]\n",
      "actions: 1 0 reward: 0 obs: 1 log_prob: [0.654229]\n",
      "actions: 1 1 reward: 1 obs: 1 log_prob: [0.654229]\n",
      "actions: 0 1 reward: 0 obs: 1 log_prob: [0.345771]\n",
      "actions: 1 1 reward: 1 obs: 0 log_prob: [0.654229]\n"
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "env = matching_pennies()\n",
    "\n",
    "# Create policy\n",
    "policy = PolicyNetwork(env.action_space.n,\n",
    "                       env.opponent_action_space.n,\n",
    "                       state_onehot=True) #.to(device)\n",
    "\n",
    "# Reset environment and obtain initial state\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Main Loop\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "\n",
    "    # Sample action\n",
    "    action, prob = policy.get_action(obs_prev)\n",
    "\n",
    "    # Interact with environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions:\",action,obs_prev,\"reward:\",reward,\"obs:\",observation,\"log_prob:\",prob.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e7c30-34ee-4e90-99bf-406012dc3afe",
   "metadata": {},
   "source": [
    "We can see that the policy's performance is quite poor. This is expected, as we have yet to train the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa242f-e0e8-4593-9c6a-051f280d0e13",
   "metadata": {},
   "source": [
    "### B2. Training the Policy Network with the REINFORCE Algorithm\n",
    "\n",
    "https://link.springer.com/article/10.1007/BF00992696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "f76e769d-3915-49ef-ac1e-189fbfd4850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE_loss(rewards, log_probs, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Loss function based on the REINFORCE algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute discounted rewards\n",
    "    discounted_rewards = []\n",
    "    for t in range(len(rewards)):\n",
    "        r = torch.flip(rewards[t:],[0]) # rewards in descending order\n",
    "        gammas = torch.flip(torch.cumprod(torch.full([len(r)],gamma),dim=0),[0]) # discount factors in descending order\n",
    "        r_npv = torch.sum(r * gammas) # Take dot product and sum\n",
    "        discounted_rewards.append(r_npv)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "\n",
    "    # Standardize discounted rewards\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) \n",
    "\n",
    "    # Compute loss\n",
    "    loss = -1 * torch.dot(log_probs,discounted_rewards)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "571e0c59-6472-426f-95ac-4424e29c24fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Epoch:    0 Loss: 0.28368735 Mean reward: 0.3\n",
      "Epoch:  100 Loss: -0.54938984 Mean reward: 0.5\n",
      "Epoch:  200 Loss: 1.3230834 Mean reward: 0.5\n",
      "Epoch:  300 Loss: 1.3510773 Mean reward: 0.8\n",
      "Epoch:  400 Loss: -4.4143143 Mean reward: 0.7\n",
      "Epoch:  500 Loss: -2.1841493 Mean reward: 0.9\n",
      "Epoch:  600 Loss: -2.6659837 Mean reward: 1.0\n",
      "Epoch:  700 Loss: 1.6938107 Mean reward: 1.0\n",
      "Epoch:  800 Loss: -5.585439 Mean reward: 0.9\n",
      "Epoch:  900 Loss: -0.016009888 Mean reward: 1.0\n",
      "Epoch: 1000 Loss: 0.06854717 Mean reward: 1.0\n",
      "Epoch: 1100 Loss: -0.2960411 Mean reward: 1.0\n",
      "Epoch: 1200 Loss: -2.1644385 Mean reward: 1.0\n",
      "Epoch: 1300 Loss: 0.0043674503 Mean reward: 1.0\n",
      "Epoch: 1400 Loss: -1.9839033e-05 Mean reward: 1.0\n",
      "Epoch: 1500 Loss: -4.554878 Mean reward: 1.0\n",
      "Epoch: 1600 Loss: -0.028746527 Mean reward: 1.0\n",
      "Epoch: 1700 Loss: 0.0040667234 Mean reward: 1.0\n",
      "Epoch: 1800 Loss: -0.002804216 Mean reward: 1.0\n",
      "Epoch: 1900 Loss: -0.009262005 Mean reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Settings\n",
    "epochs = 2000\n",
    "steps = 20\n",
    "learning_rate=2e-3\n",
    "gamma = 0.9\n",
    "\n",
    "# Create environment, policy and optimizer\n",
    "env = matching_pennies()\n",
    "policy = PolicyNetwork(env.action_space.n,\n",
    "                       env.opponent_action_space.n,\n",
    "                       5,\n",
    "                       state_onehot=True) #.to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "policy.train()\n",
    "\n",
    "observation, info = env.reset()\n",
    "for i in range(epochs):\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    # REINFORCE needs multiple observations to update policy\n",
    "    for s in range(steps):   \n",
    "        obs_prev = observation\n",
    "        action, prob = policy.get_action(obs_prev)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        log_probs.append(torch.log(prob))\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "    \n",
    "    # Convert reward and log_prob lists to tensors and move to device\n",
    "    rewards_pt = torch.tensor(rewards) #.to(device)\n",
    "    log_probs = torch.stack(log_probs).flatten() \n",
    "    loss = REINFORCE_loss(rewards_pt, log_probs, gamma=gamma)\n",
    "\n",
    "    # Back propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i %100 == 0:\n",
    "        # Print status every 100 epochs\n",
    "        print(\"Epoch:\",str(i).rjust(4),\"Loss:\",loss.detach().numpy(),\"Mean reward:\",round(np.mean(rewards),1))\n",
    "\n",
    "    if terminated:\n",
    "        # If the environment reached the termianate, reset it\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f26f5-9032-4371-b540-e9bcd1b8e89c",
   "metadata": {},
   "source": [
    "As we can see, with reinforcement learning performance tends to fluctuate quite a bit even after extensive training,\n",
    "so early stoppoing and model checkpointing is necessary to get good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b795f-4203-48b1-add3-854510789849",
   "metadata": {},
   "source": [
    "## C. Q Learning\n",
    "\n",
    "Q Learning does not use the model to approximate the policy. Instead, it uses the model to \n",
    "estimate the net present value of the expected future stream of rewards for each action.\n",
    "The action that maximizes the estimated NPV is then carried out.\n",
    "\n",
    "- Deep Q Learning: https://arxiv.org/abs/1312.5602\n",
    "- Double DQN: https://arxiv.org/abs/1509.06461\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "5cefc221-2ec4-44b5-95db-3c741dceb8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, cuda, optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Q value estimator based on a fully-connected network.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_dim, state_dim, hidden_dim=1, state_onehot=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.state_onehot = state_onehot\n",
    "\n",
    "        # Use GPU if available, otherwise use CPU\n",
    "        self.device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        self.qvals = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim) # Output is the est. Q value of each action\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if self.state_onehot:\n",
    "            # Convert input to onehot encoding\n",
    "            state = self.state_to_onehot(state)\n",
    "        elif not isinstance(state, torch.Tensor):\n",
    "            # Convert input to tensor\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "        return self.qvals(state)\n",
    "\n",
    "    def get_action(self, state, epsilon=0,p=None):\n",
    "        # Input: a state\n",
    "        # Output: the value-maximizing action and the estimated value\n",
    "        values = self.forward([state]).detach().numpy().flatten()\n",
    "        action = np.argmax(values)\n",
    "\n",
    "        # With probability epsilon select a random action\n",
    "        if epsilon > 0 and np.random.binomial(1,epsilon) == 1:\n",
    "            action = np.random.choice(np.arange(len(values)), p=p)\n",
    "        \n",
    "        return action, values[action]\n",
    "\n",
    "    def state_to_onehot(self,state):\n",
    "        # If the neural network requires the state to be in one-hot encoding,\n",
    "        # this function converts a vector of integer states to one-hot encoding\n",
    "        state = np.asarray(state).astype(int)\n",
    "        onehot = np.zeros((len(state),self.state_dim),dtype=np.float32)\n",
    "        onehot[np.arange(len(state)),state] = 1\n",
    "        return torch.tensor(onehot)\n",
    "\n",
    "    def copy_(self,source):\n",
    "        # Copy parameters from source model\n",
    "        for source_param, self_param in zip(source.parameters(), self.parameters()):\n",
    "            self_param.data.copy_(source_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "0073b236-ccc5-4e7c-bf3e-2e63e10d5895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(np.arange(len([0.3,0.7])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb6daa-5fdc-498c-9388-1b0f08d106a2",
   "metadata": {},
   "source": [
    "Let us try using this policy to interact with the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "51dca4f6-0ac3-4f6a-996a-f963706b3c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "actions: 0 0 reward: 1 obs: 1 value: -0.26208562\n",
      "actions: 0 1 reward: 0 obs: 0 value: -0.4125787\n",
      "actions: 0 0 reward: 1 obs: 1 value: -0.26208562\n",
      "actions: 0 1 reward: 0 obs: 1 value: -0.4125787\n",
      "actions: 0 1 reward: 0 obs: 0 value: -0.4125787\n"
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "env = matching_pennies()\n",
    "\n",
    "# Create the Q network\n",
    "Q = DQN(env.action_space.n,\n",
    "        env.opponent_action_space.n,\n",
    "        state_onehot=True) #.to(device)\n",
    "\n",
    "# Reset environment and obtain initial state\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Main Loop\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "\n",
    "    # Sample action\n",
    "    action, value = Q.get_action(obs_prev,epsilon=0)\n",
    "\n",
    "    # Interact with environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions:\",action,obs_prev,\"reward:\",reward,\"obs:\",observation,\"value:\",value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "657536d7-caaf-4442-8d8c-51678f0adf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n",
      "Epoch:  300 Loss: 0.2954119 Mean reward: 0.4\n",
      "Epoch:  400 Loss: 0.37430918 Mean reward: 0.6\n",
      "Epoch:  500 Loss: 0.2813556 Mean reward: 0.6\n",
      "Epoch:  600 Loss: 0.22838762 Mean reward: 0.5\n",
      "Epoch:  700 Loss: 0.3197466 Mean reward: 0.6\n",
      "Epoch:  800 Loss: 0.17621846 Mean reward: 0.6\n",
      "Epoch:  900 Loss: 0.16213207 Mean reward: 0.5\n",
      "Epoch: 1000 Loss: 0.108626485 Mean reward: 0.7\n",
      "Epoch: 1100 Loss: 0.03381476 Mean reward: 0.8\n",
      "Epoch: 1200 Loss: 0.006733194 Mean reward: 0.9\n",
      "Epoch: 1300 Loss: 0.0070919204 Mean reward: 1.0\n",
      "Epoch: 1400 Loss: 0.003033378 Mean reward: 1.0\n",
      "Epoch: 1500 Loss: 0.0008553036 Mean reward: 1.0\n",
      "Epoch: 1600 Loss: 0.0043909918 Mean reward: 0.9\n",
      "Epoch: 1700 Loss: 0.0047095013 Mean reward: 0.9\n",
      "Epoch: 1800 Loss: 0.00028236024 Mean reward: 1.0\n",
      "Epoch: 1900 Loss: 0.0013501876 Mean reward: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Settings\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "replay_size = 200\n",
    "learning_rate=2e-3\n",
    "gamma = 0.9\n",
    "doubleDQN = True\n",
    "\n",
    "# Create environment, Q network and optimizer\n",
    "env = matching_pennies()\n",
    "Q = DQN(env.action_space.n,\n",
    "        env.opponent_action_space.n,\n",
    "        5,\n",
    "        state_onehot=True) #.to(device)\n",
    "Q_target = DQN(env.action_space.n,\n",
    "               env.opponent_action_space.n,\n",
    "               5,\n",
    "               state_onehot=True) #.to(device)\n",
    "optimizer = optim.Adam(Q.parameters(), lr=learning_rate)\n",
    "\n",
    "Q.train() # Set the PyTorch model to training mode\n",
    "\n",
    "# Initialize memory for experience reply\n",
    "replay_memory = np.empty((replay_size,4),dtype=float)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Interact with the environment\n",
    "    obs_prev = observation\n",
    "    action, value = Q.get_action(obs_prev,epsilon=max(0.1,(epochs - i)/epochs))\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Add new experience to replay memory. Note the index of each item in memory\n",
    "    j = i % replay_size \n",
    "    terminated_dummy = 1 if terminated else 0\n",
    "    replay_memory[j] = [obs_prev,action,reward,observation,terminated_dummy]\n",
    "\n",
    "    # Start training after replay memory is filled\n",
    "    if i > replay_size:\n",
    "       \n",
    "        # Sample replay memory for gradient descent\n",
    "        replay_idx = np.random.choice(replay_size,batch_size)\n",
    "        replay_samples = torch.tensor(replay_memory[replay_idx],dtype=torch.float) #.to(device)\n",
    "\n",
    "        s_t = replay_samples[:,0]\n",
    "        actions = replay_samples[:,1].int()\n",
    "        rewards = replay_samples[:,2]\n",
    "        s_t1 = replay_samples[:,3]\n",
    "            \n",
    "        # Compute temporal difference loss\n",
    "        q = Q(s_t)[np.arange(batch_size),actions]\n",
    "        if doubleDQN:\n",
    "            qt_actions = torch.max(Q(s_t1),dim=1)[1]\n",
    "            q_target = Q_target(s_t1)[np.arange(batch_size),qt_actions]\n",
    "        else:\n",
    "            q_target = torch.max(Q_target(s_t1),dim=1)[0]\n",
    "        y = rewards + (1 - terminated_dummy) * gamma * q_target\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(q, y)\n",
    "            \n",
    "        # Back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 32 == 0:\n",
    "            # Periodically copy parameters to target network\n",
    "            Q_target.copy_(Q)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # Print status every 100 epochs\n",
    "            print(\"Epoch:\",str(i).rjust(4),\n",
    "                  \"Loss:\",loss.detach().numpy(),\n",
    "                  \"Mean reward:\",round(np.mean(rewards.detach().numpy()),1))\n",
    "\n",
    "    if terminated:\n",
    "        # If the environment reached the termianate, reset it\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "e3f5dca4-3860-4b7e-9060-f8a349ef3b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: 0 0 reward: 1 obs: 0 value: 9.752915\n",
      "actions: 0 0 reward: 1 obs: 1 value: 9.752915\n",
      "actions: 1 1 reward: 1 obs: 1 value: 9.75479\n",
      "actions: 1 1 reward: 1 obs: 1 value: 9.75479\n",
      "actions: 1 1 reward: 1 obs: 1 value: 9.75479\n"
     ]
    }
   ],
   "source": [
    "# Main Loop\n",
    "observation, info = env.reset()\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "\n",
    "    # Sample action\n",
    "    action, value = Q.get_action(obs_prev,epsilon=0)\n",
    "\n",
    "    # Interact with environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions:\",action,obs_prev,\"reward:\",reward,\"obs:\",observation,\"value:\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce0add42-fee3-4c93-814d-c7d1d0a60fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class market:\n",
    "    def __init__(self):\n",
    "        self.action_space = [0,1]\n",
    "    def step(self):\n",
    "        # observation is price and quantity transacted\n",
    "        observation = [p, q]\n",
    "        # reward is profit\n",
    "        reward = [pi]\n",
    "        # terminated\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02359214-da35-4a5b-9c21-0cd044663bbb",
   "metadata": {},
   "source": [
    "## Quasi-Hyperbolic Discounting Pleasant Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "fa16bd28-6048-468b-8e78-d85bc7c1f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class movies(gym.Env):\n",
    "    \"\"\"\n",
    "    Quasi-hyperbolic discounting pleasant task example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(2) # Possible actions are watch now or not\n",
    "        self.movie_quality = [1,1.5,2.25,3.375]\n",
    "        self.t = 0\n",
    "        self.terminated = False\n",
    "                \n",
    "    def step(self,action):\n",
    "\n",
    "        if self.terminated or action == 0:\n",
    "            reward = 0\n",
    "        elif action == 1:\n",
    "            reward = self.movie_quality[self.t]\n",
    "            self.terminated = True\n",
    "        \n",
    "        if self.t == 3:\n",
    "            self.terminated = True\n",
    "\n",
    "        self.t += 1       \n",
    "        observation = self.t\n",
    "        truncated = False\n",
    "        info = None\n",
    "        \n",
    "        return observation, reward, self.terminated, truncated, info    \n",
    "        \n",
    "    def reset(self,seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = 0\n",
    "        self.terminated = False\n",
    "        return self.t, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "a2b60803-bdf0-4f48-9676-ef97038150e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "actions: 0 obs_prev: 0 reward: 0 obs_next: 1 value: 0.4543814\n",
      "actions: 1 obs_prev: 1 reward: 1.5 obs_next: 2 value: -0.038886547\n"
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "env = movies()\n",
    "\n",
    "# Create the Q network\n",
    "Q = DQN(env.action_space.n,1) #.to(device)\n",
    "\n",
    "# Reset environment and obtain initial state\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Main Loop\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "\n",
    "    # Sample action\n",
    "    action, value = Q.get_action(obs_prev,epsilon=1)\n",
    "\n",
    "    # Interact with environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions:\",action,\"obs_prev:\",obs_prev,\"reward:\",reward,\"obs_next:\",observation,\"value:\",value)\n",
    "\n",
    "    if terminated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "1b8af763-5c3a-40fc-9918-fd4597e99eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n",
      "Epoch:  300 Loss: 0.32236484 Mean state: 0.6 Mean reward: 0.9 Mean y: 1.2\n",
      "Epoch:  400 Loss: 0.37459922 Mean state: 1.1 Mean reward: 1.0 Mean y: 1.7\n",
      "Epoch:  500 Loss: 0.042553537 Mean state: 0.8 Mean reward: 0.6 Mean y: 1.8\n",
      "Epoch:  600 Loss: 0.0049793064 Mean state: 0.9 Mean reward: 1.0 Mean y: 2.2\n",
      "Epoch:  700 Loss: 0.00027488632 Mean state: 1.0 Mean reward: 0.8 Mean y: 2.2\n",
      "Epoch:  800 Loss: 0.00013354213 Mean state: 1.1 Mean reward: 0.7 Mean y: 2.2\n",
      "Epoch:  900 Loss: 1.7203974e-05 Mean state: 1.2 Mean reward: 0.8 Mean y: 2.6\n",
      "Epoch: 1000 Loss: 8.674604e-06 Mean state: 1.1 Mean reward: 0.6 Mean y: 2.4\n",
      "Epoch: 1100 Loss: 1.5109847e-06 Mean state: 1.3 Mean reward: 0.5 Mean y: 2.4\n",
      "Epoch: 1200 Loss: 2.5104993e-07 Mean state: 1.1 Mean reward: 0.6 Mean y: 2.7\n",
      "Epoch: 1300 Loss: 9.723606e-08 Mean state: 0.9 Mean reward: 0.7 Mean y: 2.4\n",
      "Epoch: 1400 Loss: 1.9826613e-08 Mean state: 1.2 Mean reward: 0.5 Mean y: 2.5\n",
      "Epoch: 1500 Loss: 4.9950737e-09 Mean state: 1.3 Mean reward: 0.8 Mean y: 2.5\n",
      "Epoch: 1600 Loss: 6.7344896e-10 Mean state: 1.4 Mean reward: 0.9 Mean y: 2.8\n",
      "Epoch: 1700 Loss: 1.1745982e-09 Mean state: 1.4 Mean reward: 1.1 Mean y: 2.7\n",
      "Epoch: 1800 Loss: 1.0258139e-10 Mean state: 1.2 Mean reward: 0.5 Mean y: 2.5\n",
      "Epoch: 1900 Loss: 2.9659386e-11 Mean state: 1.5 Mean reward: 0.6 Mean y: 2.7\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Settings\n",
    "epochs = 2000\n",
    "batch_size = 32\n",
    "replay_size = 200\n",
    "learning_rate=2e-2\n",
    "gamma = 0.9\n",
    "doubleDQN = True\n",
    "\n",
    "# Create environment, Q network and optimizer\n",
    "env = movies()\n",
    "Q = DQN(env.action_space.n,1,10) #.to(device)\n",
    "Q_target = DQN(env.action_space.n,1,10) #.to(device)\n",
    "optimizer = optim.Adam(Q.parameters(),lr=learning_rate)\n",
    "\n",
    "Q.train() # Set the PyTorch model to training mode\n",
    "\n",
    "# Initialize memory for experience reply\n",
    "replay_memory = np.empty((replay_size,5),dtype=float)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Interact with the environment\n",
    "    obs_prev = observation\n",
    "    action, value = Q.get_action(obs_prev,\n",
    "                                 epsilon=max(0.1,(epochs - i)/epochs))\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Add new experience to replay memory. Note the index of each item in memory\n",
    "    j = i % replay_size \n",
    "    terminated_dummy = 1 if terminated else 0\n",
    "    replay_memory[j] = [obs_prev,action,reward,observation,terminated_dummy]\n",
    "\n",
    "    #if i == 200:\n",
    "    #    print(replay_memory)\n",
    "\n",
    "    # Start training after replay memory is filled\n",
    "    if i > replay_size:\n",
    "\n",
    "        # Sample replay memory for gradient descent\n",
    "        replay_idx = np.random.choice(replay_size,batch_size)\n",
    "        replay_samples = torch.tensor(replay_memory[replay_idx],dtype=torch.float) #.to(device)\n",
    "\n",
    "        # State vector's shape is (batch_size,state_dim)\n",
    "        # Action and reward vectors' shape is (batch_size)\n",
    "        s_t = replay_samples[:,0].view(-1,1) \n",
    "        actions = replay_samples[:,1].int()  \n",
    "        rewards = replay_samples[:,2]\n",
    "        s_t1 = replay_samples[:,3].view(-1,1)\n",
    "        terminated_dummy = replay_samples[:,4]\n",
    "           \n",
    "        # Compute temporal difference loss\n",
    "        q = Q(s_t)[np.arange(batch_size),actions]\n",
    "        if doubleDQN:\n",
    "            qt_actions = torch.max(Q(s_t1),dim=1)[1]\n",
    "            q_target = Q_target(s_t1)[np.arange(batch_size),qt_actions]\n",
    "        else:\n",
    "            q_target = torch.max(Q_target(s_t1),dim=1)[0]\n",
    "        y = rewards + (1 - terminated_dummy) * gamma * q_target\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(q,y)\n",
    "            \n",
    "        # Back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if i == 300:\n",
    "        #    print(actions.size())\n",
    "        #    print(q.size())\n",
    "        #    print(qt_actions.size())\n",
    "        #    print(q_target.size())\n",
    "        #    print(rewards.size())\n",
    "        #    print(y.size())\n",
    "        \n",
    "        if i % 64 == 0:\n",
    "            # Periodically copy parameters to target network\n",
    "            Q_target.copy_(Q)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # Print status every 100 epochs\n",
    "            print(\"Epoch:\",str(i).rjust(4),\n",
    "                  \"Loss:\",loss.detach().numpy(),\n",
    "                  \"Mean state:\",round(np.mean(s_t.detach().numpy()),1),\n",
    "                  \"Mean reward:\",round(np.mean(rewards.detach().numpy()),1),\n",
    "                  \"Mean y:\",round(np.mean(y.detach().numpy()),1))\n",
    "\n",
    "    if terminated:\n",
    "        # If the environment reached the termianate, reset it\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "c350b0ee-d578-49e4-9db0-689cef5d6e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: 0 obs_prev: 0 reward: 0 obs_next: 1 value: 2.4603784\n",
      "actions: 0 obs_prev: 1 reward: 0 obs_next: 2 value: 2.7337518\n",
      "actions: 0 obs_prev: 2 reward: 0 obs_next: 3 value: 3.0374982\n",
      "actions: 1 obs_prev: 3 reward: 3.375 obs_next: 4 value: 3.3749976\n"
     ]
    }
   ],
   "source": [
    "# Reset environment and obtain initial state\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Main Loop\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "\n",
    "    # Sample actionv \n",
    "    action, value = Q.get_action(obs_prev,epsilon=0)\n",
    "\n",
    "    # Interact with environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions:\",action,\"obs_prev:\",obs_prev,\"reward:\",reward,\"obs_next:\",observation,\"value:\",value)\n",
    "\n",
    "    if terminated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "f00c20ff-e52a-4b5e-b523-4a7e31428380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.46 1.  ]\n",
      "[2.73 1.5 ]\n",
      "[3.04 2.25]\n",
      "[0.   3.37]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(np.round(Q([i]).detach().numpy(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348c685-9fa3-49ff-9453-fdadfe6d8ff7",
   "metadata": {},
   "source": [
    "## Quasi-Hyperbolic Discounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "f34291b3-7b2c-48a7-968c-526704988989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n",
      "Using cpu device\n",
      "Epoch:  300 Loss: 0.034831237 Mean state: 0.8 Mean reward: 0.5 Mean y: 1.2\n",
      "Epoch:  400 Loss: 0.005780112 Mean state: 1.0 Mean reward: 0.9 Mean y: 1.3\n",
      "Epoch:  500 Loss: 0.0019741203 Mean state: 0.7 Mean reward: 0.6 Mean y: 1.1\n",
      "Epoch:  600 Loss: 0.0012389993 Mean state: 0.9 Mean reward: 0.7 Mean y: 1.3\n",
      "Epoch:  700 Loss: 5.5109354e-05 Mean state: 0.8 Mean reward: 0.7 Mean y: 1.2\n",
      "Epoch:  800 Loss: 4.3531458e-05 Mean state: 0.7 Mean reward: 0.7 Mean y: 1.2\n",
      "Epoch:  900 Loss: 6.5375214e-05 Mean state: 0.9 Mean reward: 0.6 Mean y: 1.3\n",
      "Epoch: 1000 Loss: 1.0686587e-05 Mean state: 1.0 Mean reward: 0.5 Mean y: 1.2\n",
      "Epoch: 1100 Loss: 1.6823014e-05 Mean state: 0.9 Mean reward: 0.6 Mean y: 1.2\n",
      "Epoch: 1200 Loss: 2.4930569e-07 Mean state: 1.1 Mean reward: 0.9 Mean y: 1.5\n",
      "Epoch: 1300 Loss: 2.9762127e-06 Mean state: 0.9 Mean reward: 0.6 Mean y: 1.3\n",
      "Epoch: 1400 Loss: 1.632975e-06 Mean state: 1.1 Mean reward: 0.9 Mean y: 1.3\n",
      "Epoch: 1500 Loss: 4.0544082e-06 Mean state: 1.0 Mean reward: 0.7 Mean y: 1.2\n",
      "Epoch: 1600 Loss: 2.9298382e-05 Mean state: 0.6 Mean reward: 0.2 Mean y: 1.0\n",
      "Epoch: 1700 Loss: 3.4196006e-07 Mean state: 1.0 Mean reward: 0.8 Mean y: 1.5\n",
      "Epoch: 1800 Loss: 3.0343355e-05 Mean state: 0.9 Mean reward: 0.7 Mean y: 1.3\n",
      "Epoch: 1900 Loss: 1.0909201e-05 Mean state: 1.1 Mean reward: 0.7 Mean y: 1.3\n",
      "Epoch: 2000 Loss: 8.3150053e-07 Mean state: 0.9 Mean reward: 0.8 Mean y: 1.3\n",
      "Epoch: 2100 Loss: 8.568313e-07 Mean state: 1.2 Mean reward: 0.6 Mean y: 1.4\n",
      "Epoch: 2200 Loss: 8.805734e-06 Mean state: 1.2 Mean reward: 0.5 Mean y: 1.4\n",
      "Epoch: 2300 Loss: 2.8195425e-05 Mean state: 1.1 Mean reward: 0.8 Mean y: 1.5\n",
      "Epoch: 2400 Loss: 3.443238e-07 Mean state: 1.4 Mean reward: 0.9 Mean y: 1.7\n",
      "Epoch: 2500 Loss: 7.946952e-08 Mean state: 1.4 Mean reward: 0.6 Mean y: 1.4\n",
      "Epoch: 2600 Loss: 7.3166866e-08 Mean state: 1.2 Mean reward: 0.7 Mean y: 1.3\n",
      "Epoch: 2700 Loss: 3.2809062e-08 Mean state: 1.2 Mean reward: 0.4 Mean y: 1.2\n",
      "Epoch: 2800 Loss: 1.1703634e-05 Mean state: 1.3 Mean reward: 0.8 Mean y: 1.6\n",
      "Epoch: 2900 Loss: 0.0005494186 Mean state: 1.4 Mean reward: 0.8 Mean y: 1.6\n",
      "Epoch: 3000 Loss: 4.818629e-05 Mean state: 1.5 Mean reward: 1.1 Mean y: 1.8\n",
      "Epoch: 3100 Loss: 9.696965e-07 Mean state: 1.0 Mean reward: 0.7 Mean y: 1.4\n",
      "Epoch: 3200 Loss: 3.781464e-07 Mean state: 1.3 Mean reward: 0.6 Mean y: 1.6\n",
      "Epoch: 3300 Loss: 1.363521e-07 Mean state: 1.2 Mean reward: 0.7 Mean y: 1.5\n",
      "Epoch: 3400 Loss: 4.7701498e-08 Mean state: 1.1 Mean reward: 0.4 Mean y: 1.3\n",
      "Epoch: 3500 Loss: 2.6000308e-07 Mean state: 1.5 Mean reward: 0.8 Mean y: 1.7\n",
      "Epoch: 3600 Loss: 2.406833e-09 Mean state: 1.7 Mean reward: 1.1 Mean y: 1.9\n",
      "Epoch: 3700 Loss: 1.9802891e-07 Mean state: 1.7 Mean reward: 1.2 Mean y: 1.9\n",
      "Epoch: 3800 Loss: 1.3520604e-07 Mean state: 1.2 Mean reward: 0.5 Mean y: 1.4\n",
      "Epoch: 3900 Loss: 4.570145e-06 Mean state: 1.6 Mean reward: 0.9 Mean y: 1.8\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Settings\n",
    "epochs = 4000\n",
    "batch_size = 32\n",
    "replay_size = 200\n",
    "learning_rate=2e-2\n",
    "gamma = 0.99\n",
    "beta = 0.5\n",
    "sophisticated = True\n",
    "\n",
    "# Create environment, Q network and optimizer\n",
    "env = movies()\n",
    "Q = DQN(env.action_space.n,1,10) #.to(device)\n",
    "Qd = DQN(env.action_space.n,1,10) #.to(device)\n",
    "Qd_target = DQN(env.action_space.n,1,10) #.to(device)\n",
    "optimizer = optim.Adam(Q.parameters(),lr=learning_rate)\n",
    "optimizer_d = optim.Adam(Qd.parameters(),lr=learning_rate)\n",
    "\n",
    "Q.train() # Set the PyTorch model to training mode\n",
    "Qd.train()\n",
    "\n",
    "# Initialize memory for experience reply\n",
    "replay_memory = np.empty((replay_size,5),dtype=float)\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "for i in range(epochs):\n",
    "\n",
    "    # Interact with the environment\n",
    "    obs_prev = observation\n",
    "    action, value = Qd.get_action(obs_prev,\n",
    "                                 epsilon=max(0.1,(epochs - i)/epochs))\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Add new experience to replay memory. Note the index of each item in memory\n",
    "    j = i % replay_size \n",
    "    terminated_dummy = 1 if terminated else 0\n",
    "    replay_memory[j] = [obs_prev,action,reward,observation,terminated_dummy]\n",
    "\n",
    "    #if i == 200:\n",
    "    #    print(replay_memory)\n",
    "\n",
    "    # Start training after replay memory is filled\n",
    "    if i > replay_size:\n",
    "\n",
    "        # Sample replay memory for gradient descent\n",
    "        replay_idx = np.random.choice(replay_size,batch_size)\n",
    "        replay_samples = torch.tensor(replay_memory[replay_idx],dtype=torch.float) #.to(device)\n",
    "\n",
    "        # State vector's shape is (batch_size,state_dim)\n",
    "        # Action and reward vectors' shape is (batch_size)\n",
    "        s_t = replay_samples[:,0].view(-1,1) \n",
    "        actions = replay_samples[:,1].int()  \n",
    "        rewards = replay_samples[:,2]\n",
    "        s_t1 = replay_samples[:,3].view(-1,1)\n",
    "        terminated_dummy = replay_samples[:,4]\n",
    "\n",
    "        # Compute temporal difference loss\n",
    "        qd = Qd(s_t)[np.arange(batch_size),actions]\n",
    "        \n",
    "        # DoubleDQN\n",
    "        qdt_actions = torch.max(Qd(s_t1),dim=1)[1]\n",
    "        qd_target = Qd_target(s_t1)[np.arange(batch_size),qdt_actions]\n",
    "        \n",
    "        yd = rewards + (1 - terminated_dummy) * gamma * qd_target\n",
    "        loss_fn_d = nn.MSELoss()\n",
    "        loss_d = loss_fn_d(qd,yd)\n",
    "            \n",
    "        # Back propagation\n",
    "        optimizer_d.zero_grad()\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()        \n",
    "\n",
    "        if i % 64 == 0:\n",
    "            # Periodically copy parameters to target network\n",
    "            Qd_target.copy_(Qd)        \n",
    "           \n",
    "        # Compute temporal difference loss\n",
    "        q = Q(s_t)[np.arange(batch_size),actions]\n",
    "        with torch.no_grad():\n",
    "            if sophisticated:\n",
    "                qt_actions = torch.max(Q(s_t1),dim=1)[1]\n",
    "                q_target = Qd_target(s_t1)[np.arange(batch_size),qt_actions]\n",
    "            else:\n",
    "                q_target = qd_target.detach()\n",
    "        y = rewards + (1 - terminated_dummy) * beta * gamma * q_target.detach()\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(q,y)\n",
    "            \n",
    "        # Back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if i == 300:\n",
    "        #    print(actions.size())\n",
    "        #    print(q.size())\n",
    "        #    print(qt_actions.size())\n",
    "        #    print(q_target.size())\n",
    "        #    print(rewards.size())\n",
    "        #    print(y.size())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # Print status every 100 epochs\n",
    "            print(\"Epoch:\",str(i).rjust(4),\n",
    "                  \"Loss:\",loss.detach().numpy(),\n",
    "                  \"Mean state:\",round(np.mean(s_t.detach().numpy()),1),\n",
    "                  \"Mean reward:\",round(np.mean(rewards.detach().numpy()),1),\n",
    "                  \"Mean y:\",round(np.mean(y.detach().numpy()),1))\n",
    "\n",
    "    if terminated:\n",
    "        # If the environment reached the termianate, reset it\n",
    "        observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "79a838a0-3fda-493a-bfc6-8ea9f102fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: 1 obs_prev: 0 reward: 1 obs_next: 1 value: 0.99475443\n"
     ]
    }
   ],
   "source": [
    "# Reset environment and obtain initial state\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Main Loop\n",
    "for i in range(5):\n",
    "    obs_prev = observation\n",
    "\n",
    "    # Sample actionv \n",
    "    action, value = Q.get_action(obs_prev,epsilon=0)\n",
    "\n",
    "    # Interact with environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)    \n",
    "    print(\"actions:\",action,\"obs_prev:\",obs_prev,\"reward:\",reward,\"obs_next:\",observation,\"value:\",value)\n",
    "\n",
    "    if terminated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "a0821ede-922d-4e13-839b-99d36649e431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74 0.99]\n",
      "[1.11 1.5 ]\n",
      "[1.66 2.25]\n",
      "[-0.01  3.38]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(np.round(Q([i]).detach().numpy(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "b50f5800-7c46-42a3-9364-4b18d88c18fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.27 1.  ]\n",
      "[3.31 1.5 ]\n",
      "[3.34 2.25]\n",
      "[-0.    3.37]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(np.round(Qd([i]).detach().numpy(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc057f4-2654-4fee-a70e-191e80cd1b76",
   "metadata": {},
   "source": [
    "# PPO\n",
    "\n",
    "- GAE: https://arxiv.org/abs/1506.02438\n",
    "- PPO: https://arxiv.org/abs/1707.06347\n",
    "\n",
    "We add two things in comparison with REINFORCE:\n",
    "1. Multiply the Generalized Advantage Estimate (GAE) instead of discounted sum of realized returns to the outputed probabilities from the policy network. This is a better choice because realized returns as a estimator for value has high variance.\n",
    "2. Add the predicted future value from a value network whenever discounted sum of returns is used.\n",
    "\n",
    "The objective function has an additional term that trains the value network by minimizing the MSE of the predicted future value with the NPV of actual returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "39390367-0579-4c11-9644-195302d67529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPO_loss(probs, td_residuals, rewards, values, values_next,\n",
    "             gamma=0.9, Lambda=0.9, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    PPO loss function\n",
    "    \"\"\"\n",
    "\n",
    "    ### Part I. Policy Loss (L_CLIP in PPO paper) ###\n",
    "    \n",
    "    # Clipped probability ratio\n",
    "    probs_old = probs.detach()\n",
    "    probs_ratio = probs/probs_old\n",
    "    clipped_ratio = torch.clamp(probs_ratio,1-epsilon,1+epsilon)\n",
    "        \n",
    "    # Generalized advantage estimate\n",
    "    GAE_list = []\n",
    "    for t in range(len(td_residuals)):\n",
    "        a = torch.flip(td_residuals[t:],[0]) # td_residuals in descending order\n",
    "        gl = torch.flip(torch.cumprod(torch.full([len(a)],gamma*Lambda),dim=0),[0]) # discount factors in descending order\n",
    "        GAE = torch.sum(a * gl) # Take dot product and sum\n",
    "        GAE_list.append(GAE)\n",
    "    GAE_list = torch.tensor(GAE_list)\n",
    "    \n",
    "    # Policy loss\n",
    "    policy_loss = -1 * torch.min(torch.dot(probs_ratio,GAE_list),\n",
    "                                 torch.dot(clipped_ratio,GAE_list))\n",
    "\n",
    "    ### Part II. Value Loss ###\n",
    "    \n",
    "    # Discounted rewards\n",
    "    vals_next = values_next.detach().flatten()\n",
    "    discounted_rewards = []\n",
    "    for t in range(len(rewards)):\n",
    "        r = torch.flip(rewards[t:],[0]) # rewards in descending order\n",
    "        gammas = torch.flip(torch.cumprod(torch.full([len(r)],gamma),dim=0),[0]) # discount factors in descending order\n",
    "        r_npv = torch.sum(r * gammas)  + gamma**len(rewards) * vals_next[-1] # Take dot product and sum\n",
    "        discounted_rewards.append(r_npv)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)    \n",
    "\n",
    "    # Value loss\n",
    "    value_mse = torch.nn.MSELoss()\n",
    "    value_loss = value_mse(values,discounted_rewards)\n",
    "\n",
    "    ### Part I + II: Total Loss (L_CLIP+VF in PPO paper) ###\n",
    "    \n",
    "    # If policy and value networks do not share parameters,\n",
    "    # they are optimized separately so equal weight is fine.\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "02280d96-5a49-4f77-baa2-70a2295d1640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n",
      "Epoch:    0 Loss: -6.9472485 Mean reward: 0.5\n",
      "Epoch:  100 Loss: -25.05658 Mean reward: 0.7\n",
      "Epoch:  200 Loss: 2.1252441 Mean reward: 0.4\n",
      "Epoch:  300 Loss: -31.24781 Mean reward: 0.9\n",
      "Epoch:  400 Loss: -29.000067 Mean reward: 0.8\n",
      "Epoch:  500 Loss: -39.239563 Mean reward: 1.0\n",
      "Epoch:  600 Loss: -38.19125 Mean reward: 1.0\n",
      "Epoch:  700 Loss: -36.576694 Mean reward: 1.0\n",
      "Epoch:  800 Loss: -36.789467 Mean reward: 1.0\n",
      "Epoch:  900 Loss: -39.429123 Mean reward: 1.0\n",
      "Epoch: 1000 Loss: -39.313236 Mean reward: 1.0\n",
      "Epoch: 1100 Loss: -38.557446 Mean reward: 1.0\n",
      "Epoch: 1200 Loss: -39.268036 Mean reward: 1.0\n",
      "Epoch: 1300 Loss: -38.251858 Mean reward: 1.0\n",
      "Epoch: 1400 Loss: -38.35975 Mean reward: 1.0\n",
      "Epoch: 1500 Loss: -37.360153 Mean reward: 1.0\n",
      "Epoch: 1600 Loss: -39.329643 Mean reward: 1.0\n",
      "Epoch: 1700 Loss: -39.281906 Mean reward: 1.0\n",
      "Epoch: 1800 Loss: -39.241333 Mean reward: 1.0\n",
      "Epoch: 1900 Loss: -39.036697 Mean reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Settings\n",
    "epochs = 2000\n",
    "steps = 20\n",
    "updates = 3\n",
    "learning_rate=2e-3\n",
    "gamma = 0.9\n",
    "Lambda = 0.9\n",
    "\n",
    "# Create environment, policy network, value network and optimizer\n",
    "env = matching_pennies()\n",
    "policy = PolicyNetwork(env.action_space.n,env.opponent_action_space.n,5) #.to(device)\n",
    "value = DQN(1,env.opponent_action_space.n,5,state_onehot=True) \n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "policy.train()\n",
    "value.train()\n",
    "\n",
    "observation, info = env.reset()\n",
    "for i in range(epochs):\n",
    "    probs_list = []\n",
    "    rewards_list = []\n",
    "    obs_list = [] \n",
    "    obs_prev_list = []\n",
    "\n",
    "    # Needs multiple observations to update policy\n",
    "    for s in range(steps):   \n",
    "        obs_prev = observation\n",
    "        action, prob = policy.get_action(obs_prev)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        probs_list.append(prob)\n",
    "        rewards_list.append(reward)\n",
    "        obs_list.append(observation)\n",
    "        obs_prev_list.append(obs_prev)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    # Compute value for observed states\n",
    "    values = value(torch.tensor(obs_list)).flatten()\n",
    "    values_prev = value(torch.tensor(obs_prev_list)).flatten()\n",
    "\n",
    "    # Compute TD residual\n",
    "    rewards = torch.tensor(rewards_list)\n",
    "    td_residuals = rewards + gamma * values.detach() - values_prev.detach()\n",
    "    \n",
    "    # Compute PPO loss\n",
    "    probs = torch.stack(probs_list).flatten() \n",
    "    loss = PPO_loss(  probs, \n",
    "                      td_residuals,\n",
    "                      rewards_pt, \n",
    "                      values_prev,\n",
    "                      values,\n",
    "                      gamma=gamma,\n",
    "                      Lambda=Lambda)\n",
    "\n",
    "    # Back propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i %100 == 0:\n",
    "        # Print status every 100 epochs\n",
    "        print(\"Epoch:\",str(i).rjust(4),\"Loss:\",loss.detach().numpy(),\"Mean reward:\",round(np.mean(rewards_list),1))\n",
    "\n",
    "    if terminated:\n",
    "        # If the environment reached the termianate, reset it\n",
    "        observation, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "3b2a11ec-c221-4c9f-ba76-caf94ef20065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "a4971968-934f-4de0-9b78-d8862d8d0470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0046, 1.0046, 1.0046, 1.0046, 1.0046, 0.6212, 1.0046, 0.6212, 0.6212,\n",
       "        1.0046, 1.0046, 0.6212, 1.0046, 0.6212, 0.6212, 1.0046, 1.0046, 0.6212,\n",
       "        1.0046, 0.6212], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value(torch.tensor(obs_list)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "08b2ab23-75e4-48fb-9009-c5d97a71b492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n",
      "Epoch:    0 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch:  100 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch:  200 Loss: -0.60822475 Mean reward: 0.8\n",
      "Epoch:  300 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch:  400 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch:  500 Loss: -0.60822475 Mean reward: 0.8\n",
      "Epoch:  600 Loss: -0.60822475 Mean reward: 0.8\n",
      "Epoch:  700 Loss: -1.1804235 Mean reward: 0.8\n",
      "Epoch:  800 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch:  900 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch: 1000 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch: 1100 Loss: -1.1804235 Mean reward: 0.8\n",
      "Epoch: 1200 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch: 1300 Loss: -0.60822475 Mean reward: 0.8\n",
      "Epoch: 1400 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch: 1500 Loss: -1.1804235 Mean reward: 0.8\n",
      "Epoch: 1600 Loss: -1.1804235 Mean reward: 0.8\n",
      "Epoch: 1700 Loss: -0.010719001 Mean reward: 1.0\n",
      "Epoch: 1800 Loss: -1.1804235 Mean reward: 0.8\n",
      "Epoch: 1900 Loss: -0.60822475 Mean reward: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Settings\n",
    "epochs = 2000\n",
    "steps = 20\n",
    "updates = 3\n",
    "learning_rate=2e-3\n",
    "gamma = 0.9\n",
    "Lambda = 0.9\n",
    "\n",
    "# Create environment, policy network, value network and optimizer\n",
    "env = movies()\n",
    "policy = PolicyNetwork(env.action_space.n,1,10) #.to(device)\n",
    "value = DQN(1,1,10) # Single output because we just need V(s_t), not V(a_t|s_t)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "policy.train()\n",
    "value.train()\n",
    "\n",
    "observation, info = env.reset()\n",
    "for i in range(epochs):\n",
    "    probs_list = []\n",
    "    rewards_list = []\n",
    "    obs_list = [] \n",
    "    obs_prev_list = []\n",
    "\n",
    "    # Needs multiple observations to update policy\n",
    "    for s in range(steps):   \n",
    "        obs_prev = observation\n",
    "        action, prob = policy.get_action(obs_prev)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        probs_list.append(prob)\n",
    "        rewards_list.append(reward)\n",
    "        obs_list.append(observation)\n",
    "        obs_prev_list.append(obs_prev)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    # Compute value for observed states\n",
    "    values = value(torch.tensor(obs_list,dtype=torch.float).view(-1,1)).flatten()\n",
    "    values_prev = value(torch.tensor(obs_prev_list,dtype=torch.float).view(-1,1)).flatten()\n",
    "\n",
    "    # Compute TD residual\n",
    "    rewards = torch.tensor(rewards_list,dtype=torch.float)\n",
    "    td_residuals = rewards + gamma * values.detach() - values_prev.detach()\n",
    "    \n",
    "    # Compute PPO loss\n",
    "    probs = torch.stack(probs_list).flatten() \n",
    "    loss = PPO_loss(  probs, \n",
    "                      td_residuals,\n",
    "                      rewards, \n",
    "                      values_prev,\n",
    "                      values,\n",
    "                      gamma=gamma,\n",
    "                      Lambda=Lambda)\n",
    "\n",
    "    # Back propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i %100 == 0:\n",
    "        # Print status every 100 epochs\n",
    "        print(\"Epoch:\",str(i).rjust(4),\"Loss:\",loss.detach().numpy(),\"Mean reward:\",round(np.mean(rewards_list),1))\n",
    "\n",
    "    if terminated:\n",
    "        # If the environment reached the termianate, reset it\n",
    "        observation, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "35fe2e8f-2245-40c9-86da-a3a50abb0c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12]\n",
      "[-0.15]\n",
      "[-0.28]\n",
      "[-0.43]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(np.round(value([i]).detach().numpy(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "203a5a58-3f9a-48f7-bb0c-b219e60c2f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78 0.22]\n",
      "[0.64 0.36]\n",
      "[0.24 0.76]\n",
      "[0.05 0.95]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(np.round(policy([i]).detach().numpy(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538bf12f-8e7a-4e90-9bb1-db4d666252a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
