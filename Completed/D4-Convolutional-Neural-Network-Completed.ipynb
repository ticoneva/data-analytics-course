{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f67061e2",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "In convolutional neural networks (CNN), each neuron is only connected to a neighborhood of outputs below them and groups of neurons are constrainted to have the same weights. CNN excel in detecting patterns, making them most suitable suitable for audio and visual processing, but they can also be used in natural language processing and time series modelling.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e5347",
   "metadata": {},
   "source": [
    "## A. Data\n",
    "\n",
    "We will load the MNIST data and process it. There are three things we generally need to do when working with image data and CNN models:\n",
    "1. Normalize the features.\n",
    "2. Shape the features in the a specify format.\n",
    "3. Convert the target to one-hot format, i.e. a vector of dummy variables.\n",
    "\n",
    "First let us load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55edaa34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d55b5",
   "metadata": {},
   "source": [
    "We will take a look at the first sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64bbe97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc31eb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "       205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tneth row\n",
    "x_train[0][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a551619",
   "metadata": {},
   "source": [
    "You can see that it is a 28 x 28 matrix with values ranging from 0 to 255. What it represents is a 28 x 28 pixel monochrome image of a handwritten digit, which we can show as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a232a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a2ad9",
   "metadata": {},
   "source": [
    "Normalizing the features to `[0,1]` speeds up learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b60c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "xf = x_test.flatten()\n",
    "print(np.max(xf))\n",
    "print(np.min(xf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dde4d9",
   "metadata": {},
   "source": [
    "Keras' CNN layers expect the features to be a 4D array, in the following format:\n",
    "```\n",
    "(samples,image-rows,image-columns,color-channels)\n",
    "```\n",
    "We will reshape the data to ensure this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a8e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features has to be in the following shape: (obs, rows, cols, color channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf1037",
   "metadata": {},
   "source": [
    "Target is digit's value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3325c1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851df6a",
   "metadata": {},
   "source": [
    "For classification task, the common practice is to have one output neuron per class. We can use `keras.utils.to_categorical()` to convert the target value to one-hot format, which simply means a vector of a dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "259fe710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba9ea1",
   "metadata": {},
   "source": [
    "## B. Model\n",
    "\n",
    "Now we build our model. First up is a standard convolutional neural network.\n",
    "\n",
    "\n",
    "### B1. Standard Convolutional Neural Network\n",
    "\n",
    "The model has the following structure:\n",
    "\n",
    "1. Input\n",
    "2. Convolution layer\n",
    "3. Pooling layer\n",
    "4. Repeat 2. and 3. if necessary\n",
    "5. Flatten 2D output to 1D\n",
    "6. Fully-connected layers\n",
    "7. Final output\n",
    "\n",
    "Dropout can be added at each stage as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29066d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.5057 - accuracy: 0.8400 - val_loss: 0.1043 - val_accuracy: 0.9668\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 0.1670 - accuracy: 0.9516 - val_loss: 0.0679 - val_accuracy: 0.9788\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.1249 - accuracy: 0.9633 - val_loss: 0.0515 - val_accuracy: 0.9839\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 11s 22ms/step - loss: 0.1051 - accuracy: 0.9696 - val_loss: 0.0493 - val_accuracy: 0.9842\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0917 - accuracy: 0.9727 - val_loss: 0.0451 - val_accuracy: 0.9859\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 11s 22ms/step - loss: 0.0820 - accuracy: 0.9767 - val_loss: 0.0393 - val_accuracy: 0.9883\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 11s 22ms/step - loss: 0.0747 - accuracy: 0.9782 - val_loss: 0.0399 - val_accuracy: 0.9874\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 11s 22ms/step - loss: 0.0674 - accuracy: 0.9805 - val_loss: 0.0363 - val_accuracy: 0.9881\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 11s 22ms/step - loss: 0.0633 - accuracy: 0.9814 - val_loss: 0.0319 - val_accuracy: 0.9898\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 0.0584 - accuracy: 0.9827 - val_loss: 0.0318 - val_accuracy: 0.9891\n",
      "Test loss: 0.03176392242312431\n",
      "Test accuracy: 0.9890999794006348\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Dropout,Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "#Settings\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# The data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Features has to be in the following shape: (obs, rows, cols, color channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Normalize features\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Model\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(6, kernel_size=(5, 5),\n",
    "                 activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(16, (5, 5), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(120, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888792f",
   "metadata": {},
   "source": [
    "### B2. Residual Network (ResNet)\n",
    "\n",
    "Residual networks adds the output from a lower layer to that of several layers higher. This helps training by shortening the path back-propagation has to take to reach the lower layer.\n",
    "\n",
    "A ResNet model has the following structure:\n",
    "\n",
    "1. Input\n",
    "2. Convolution \n",
    "3. Pooling layer\n",
    "4. Convolution layer A\n",
    "5. Convolution layer B\n",
    "6. Add output of 3. and 5.\n",
    "4. Repeat 3. to 6. if necessary\n",
    "5. Flatten 2D output to 1D\n",
    "6. Fully-connected layers\n",
    "7. Final output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54191464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-03 22:48:56.449159: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-12-03 22:48:56.449199: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (scrp-login-2): /proc/driver/nvidia/version does not exist\n",
      "2021-12-03 22:48:56.449588: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-03 22:48:57.365180: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 20s 41ms/step - loss: 0.5579 - accuracy: 0.8169 - val_loss: 0.1086 - val_accuracy: 0.9657\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.1895 - accuracy: 0.9443 - val_loss: 0.0667 - val_accuracy: 0.9787\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.1426 - accuracy: 0.9585 - val_loss: 0.0532 - val_accuracy: 0.9826\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.1204 - accuracy: 0.9651 - val_loss: 0.0468 - val_accuracy: 0.9847\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.1031 - accuracy: 0.9694 - val_loss: 0.0431 - val_accuracy: 0.9864\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0897 - accuracy: 0.9744 - val_loss: 0.0352 - val_accuracy: 0.9880\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0839 - accuracy: 0.9753 - val_loss: 0.0364 - val_accuracy: 0.9876\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0770 - accuracy: 0.9778 - val_loss: 0.0332 - val_accuracy: 0.9885\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0704 - accuracy: 0.9797 - val_loss: 0.0326 - val_accuracy: 0.9893\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0677 - accuracy: 0.9799 - val_loss: 0.0318 - val_accuracy: 0.9902\n",
      "Test loss: 0.031793199479579926\n",
      "Test accuracy: 0.9901999831199646\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Dropout,Flatten,Add\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "#Settings\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# The data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Features has to be in the following shape: (obs, rows, cols, color channels)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Normalize features\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Model\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(6, kernel_size=(5, 5),\n",
    "              activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# A residual block\n",
    "x1 = Conv2D(6, kernel_size=(5, 5),\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "x2 = Conv2D(6, kernel_size=(5, 5),\n",
    "               activation='relu',\n",
    "               padding='same')(x1)\n",
    "x = Add()([x,x2])\n",
    "\n",
    "# The rest is the same as before\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(120, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee948f76",
   "metadata": {},
   "source": [
    "## C. Running Cluster\n",
    "\n",
    "Training will be faster if we use more CPU cores, but it will be *a lot* faster if we use a GPU. The simple ResNet we use in this notebook are available as `resnet-mnist.py` under the 'Examples' folder.\n",
    "\n",
    "If you using the Department of Economics' SCRP HPC Cluster, you can run the IMDB example on a GPU by typing the following commands in a terminal:\n",
    "\n",
    "```\n",
    "conda activate tensorflow\n",
    "gpu python [path]/resnet-mnist.py\n",
    "```\n",
    "\n",
    "This runs the script on the slowest available GPU on the cluster. This usually means a GTX 1050 Ti.\n",
    "The speed up is going to be much more impressive if we use the fastest GPU available:\n",
    "```\n",
    "gpu ---gpus=rtx3090:1 python [path]/resnet-mnist.py\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
