{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Version: 2022-10-12\n",
    "\n",
    "In the previous notebook, we see that regularization is crucial to training a good model. The strength of regularization is controlled by a **hyperparameter** ```alpha```. How should we pick such hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # organize data\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso # regressions\n",
    "from sklearn.preprocessing import StandardScaler # Standardize data\n",
    "from sklearn.pipeline import Pipeline # Pipeline\n",
    "import numpy as np # calculate mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>price</th>\n",
       "      <th>mpg</th>\n",
       "      <th>rep78</th>\n",
       "      <th>headroom</th>\n",
       "      <th>trunk</th>\n",
       "      <th>weight</th>\n",
       "      <th>length</th>\n",
       "      <th>turn</th>\n",
       "      <th>displacement</th>\n",
       "      <th>gear_ratio</th>\n",
       "      <th>foreign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMC Concord</td>\n",
       "      <td>4099</td>\n",
       "      <td>22</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>11</td>\n",
       "      <td>2930</td>\n",
       "      <td>186</td>\n",
       "      <td>40</td>\n",
       "      <td>121</td>\n",
       "      <td>3.58</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMC Pacer</td>\n",
       "      <td>4749</td>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3350</td>\n",
       "      <td>173</td>\n",
       "      <td>40</td>\n",
       "      <td>258</td>\n",
       "      <td>2.53</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMC Spirit</td>\n",
       "      <td>3799</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2640</td>\n",
       "      <td>168</td>\n",
       "      <td>35</td>\n",
       "      <td>121</td>\n",
       "      <td>3.08</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buick Century</td>\n",
       "      <td>4816</td>\n",
       "      <td>20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>16</td>\n",
       "      <td>3250</td>\n",
       "      <td>196</td>\n",
       "      <td>40</td>\n",
       "      <td>196</td>\n",
       "      <td>2.93</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buick Electra</td>\n",
       "      <td>7827</td>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20</td>\n",
       "      <td>4080</td>\n",
       "      <td>222</td>\n",
       "      <td>43</td>\n",
       "      <td>350</td>\n",
       "      <td>2.41</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            make  price  mpg  rep78  headroom  trunk  weight  length  turn  \\\n",
       "0    AMC Concord   4099   22    3.0       2.5     11    2930     186    40   \n",
       "1      AMC Pacer   4749   17    3.0       3.0     11    3350     173    40   \n",
       "2     AMC Spirit   3799   22    NaN       3.0     12    2640     168    35   \n",
       "3  Buick Century   4816   20    3.0       4.5     16    3250     196    40   \n",
       "4  Buick Electra   7827   15    4.0       4.0     20    4080     222    43   \n",
       "\n",
       "   displacement  gear_ratio   foreign  \n",
       "0           121        3.58  Domestic  \n",
       "1           258        2.53  Domestic  \n",
       "2           121        3.08  Domestic  \n",
       "3           196        2.93  Domestic  \n",
       "4           350        2.41  Domestic  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data. \n",
    "auto = pd.read_csv(\"../Data/auto.csv\")\n",
    "\n",
    "# Check data\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. A simple out-of-sample test\n",
    "Let's start with a simple out-of-sample test: we will divide our data into two parts, one for training the model and the other for testing the model's out-of-sample performance. The former is commonly called **training set** while the latter **test set** or **holdout set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample R-squared: 0.39042888813645216\n",
      "Out-of-sample R-squared: -0.24575377887631067\n"
     ]
    }
   ],
   "source": [
    "# Pick variables\n",
    "y = auto[\"price\"]\n",
    "x = auto[[\"mpg\",\"weight\",\"headroom\",\"displacement\"]]\n",
    "\n",
    "# Use about 3/4 of data for training: 60 samples\n",
    "train_num = 60\n",
    "\n",
    "# In-sample data for training model\n",
    "y_in = y[:train_num]\n",
    "x_in = x[:train_num]\n",
    "\n",
    "# Out-of-sample data for testing model\n",
    "y_out = y[train_num:]\n",
    "x_out = x[train_num:]\n",
    "\n",
    "# Train Ridge model and show R-Squared values\n",
    "scaler = StandardScaler()\n",
    "lasso = Lasso(alpha=50)\n",
    "model = Pipeline(steps=[(\"scaler\", scaler),\n",
    "                       (\"lasso\", lasso)])\n",
    "model.fit(x_in,y_in)\n",
    "print(\"in-sample R-squared:\",model.score(x_in,y_in))\n",
    "print(\"Out-of-sample R-squared:\",model.score(x_out,y_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we enclose the model creation and training process in a loop, \n",
    "we can easily try different alpha values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "in-sample R-squared: 0.3203323500131189\n",
      "Out-of-sample R-squared: 0.26184014397537436\n",
      "5\n",
      "in-sample R-squared: 0.3203204009322512\n",
      "Out-of-sample R-squared: 0.26134629617267846\n",
      "10\n",
      "in-sample R-squared: 0.32028314641925504\n",
      "Out-of-sample R-squared: 0.2606944031523071\n",
      "50\n",
      "in-sample R-squared: 0.3190920642530575\n",
      "Out-of-sample R-squared: 0.25502820863505204\n",
      "100\n",
      "in-sample R-squared: 0.31537073839155483\n",
      "Out-of-sample R-squared: 0.24681120867245976\n",
      "500\n",
      "in-sample R-squared: 0.2546073403473279\n",
      "Out-of-sample R-squared: 0.17824118318444848\n",
      "1000\n",
      "in-sample R-squared: 0.14968383869313473\n",
      "Out-of-sample R-squared: 0.0808903589680322\n",
      "5000\n",
      "in-sample R-squared: 0.0\n",
      "Out-of-sample R-squared: -0.02765171701126068\n"
     ]
    }
   ],
   "source": [
    "# Alphas to go through\n",
    "alphas = [1,5,10,50,100,500,1000,5000]\n",
    "\n",
    "# Loop through alphas\n",
    "for a in alphas:\n",
    "    scaler = StandardScaler()\n",
    "    lasso = Lasso(alpha=a)\n",
    "    model = Pipeline(steps=[(\"scaler\", scaler),\n",
    "                           (\"lasso\", lasso)])\n",
    "    model.fit(x_in,y_in)\n",
    "    \n",
    "    in_score = model.score(x_in,y_in)\n",
    "    out_score = model.score(x_out,y_out)\n",
    "    \n",
    "    print(a)\n",
    "    print(\"in-sample R-squared:\",in_score)\n",
    "    print(\"Out-of-sample R-squared:\",out_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be fitting models repeatedly, let us place the code above in a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for fitting models and printing results\n",
    "def fit_models(data,alphas=[1]):\n",
    "    \n",
    "    y_in,y_out,x_in,x_out = data\n",
    "    \n",
    "    for a in alphas:\n",
    "        scaler = StandardScaler()\n",
    "        lasso = Lasso(alpha=a)\n",
    "        model = Pipeline(steps=[(\"scaler\", scaler),\n",
    "                               (\"lasso\", lasso)])\n",
    "        model.fit(x_in,y_in)\n",
    "        \n",
    "        in_score = model.score(x_in,y_in)\n",
    "        out_score = model.score(x_out,y_out)\n",
    "        \n",
    "        print(str(a).ljust(10), #left-justified, width=10\n",
    "              str(round(in_score,2)).ljust(8), #left justified, width=5\n",
    "              str(round(out_score,2)).rjust(5)) #right justified, width=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.39      -0.3\n",
      "5          0.39     -0.29\n",
      "10         0.39     -0.29\n",
      "50         0.39     -0.25\n",
      "100        0.39      -0.2\n",
      "500        0.32      0.02\n",
      "1000       0.23      0.11\n",
      "5000       0.0       -0.0\n"
     ]
    }
   ],
   "source": [
    "# Alphas to go through\n",
    "alphas = [1,5,10,50,100,500,1000,5000]\n",
    "\n",
    "fit_models([y_in,y_out,x_in,x_out],alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While regularization helps us get more consistent performance, our model simply isn't really good. What could be the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Shuffling data\n",
    "\n",
    "If the data is sorted, splitting the data sequentially would give us unrepresentative sets of data. To deal with that, we can shuffle our data before splitting it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.3       0.46\n",
      "5          0.3       0.46\n",
      "10         0.3       0.46\n",
      "50         0.3       0.46\n",
      "100        0.29      0.46\n",
      "500        0.24      0.36\n",
      "1000       0.16      0.22\n",
      "5000       0.0      -0.07\n"
     ]
    }
   ],
   "source": [
    "# Import function for shuffling\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Shuffle observations\n",
    "y,x = shuffle(auto[\"price\"],auto[[\"mpg\",\"weight\",\"headroom\",\"displacement\"]],random_state=1234)\n",
    "\n",
    "# Use about 3/4 of data for training: 60 samples\n",
    "train_num = 60\n",
    "\n",
    "# In-sample data for training model\n",
    "y_in = y[:train_num]\n",
    "x_in = x[:train_num]\n",
    "\n",
    "# Out-of-sample data for testing model\n",
    "y_out = y[train_num:]\n",
    "x_out = x[train_num:]\n",
    "\n",
    "# Train Ridge model with different alphas and show R-Squared values\n",
    "fit_models([y_in,y_out,x_in,x_out],alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you shuffle your data before splitting it? It depends. For cross-section data it is probably a good idea, but for time-series data it would be a bad idea, since you are introducing *hindsight bias* if you can train with data that is generated after some of your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. train_test_split\n",
    "\n",
    "In practice, you will probably use sckit-learn's ```train_test_split``` method to split the data. ```train_test_split``` shuffles the data by default, so there is no need to call ```shuffle``` separately. The default is a 75/25 split, which you can change by providing a different ```train_size``` or ```test_size```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.37      0.06\n",
      "5          0.37      0.06\n",
      "10         0.37      0.06\n",
      "50         0.37      0.08\n",
      "100        0.37       0.1\n",
      "500        0.28      0.13\n",
      "1000       0.18      0.07\n",
      "5000       0.0      -0.04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "y_in,y_out,x_in,x_out = train_test_split(auto[\"price\"],\n",
    "                                         auto[[\"mpg\",\"weight\",\"headroom\",\"displacement\"]],\n",
    "                                         train_size=0.8)\n",
    "\n",
    "# Train Ridge model with different alphas and show R-Squared values\n",
    "fit_models([y_in,y_out,x_in,x_out],alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Validation\n",
    "\n",
    "So we try out different values of ```alpha``` and pick the one that give us the highest out-of-sample score. Do so is actually problematic: since ```alpha``` is a parameter of our model, we are effectively training our model with the supposingly out-of-sample data, which means the test set no longer gives us truly out-of-sample results. In particular, there is a real chance of overfitting our model to the test set via ```alpha```.\n",
    "\n",
    "\n",
    "The correct approach is to split the data into three parts: besides the train set and test set, we have an additional **validation set** for picking the model's hyperparameters. It is common to use around 60% of the data for training and 20% each for validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.31      0.02\n",
      "5          0.31      0.03\n",
      "10         0.31      0.04\n",
      "50         0.3       0.14\n",
      "100        0.28      0.22\n",
      "500        0.18      0.21\n",
      "1000       0.08      0.09\n",
      "5000       0.0      -0.03\n"
     ]
    }
   ],
   "source": [
    "# 64% for training, 16% for validation and 20% for out-of-sample test\n",
    "y_in,y_out,x_in,x_out = train_test_split(auto[\"price\"],\n",
    "                                         auto[[\"mpg\",\"weight\",\"headroom\",\"displacement\"]],\n",
    "                                         test_size=0.2)\n",
    "data_in = train_test_split(y_in,\n",
    "                           x_in,\n",
    "                           train_size=0.8)\n",
    "\n",
    "# Train Ridge model with different alphas and show R-Squared values\n",
    "fit_models(data_in,alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After picking the best alpha based on validation data, the final step is to test the model's out-of-sample performance with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100        0.27      0.35\n"
     ]
    }
   ],
   "source": [
    "a = 100\n",
    "scaler = StandardScaler()\n",
    "lasso = Lasso(alpha=a)\n",
    "model = Pipeline(steps=[(\"scaler\", scaler),\n",
    "                       (\"lasso\", lasso)])\n",
    "model.fit(x_in,y_in)\n",
    "print(str(a).ljust(10), \n",
    "      str(round(model.score(x_in,y_in),2)).ljust(8), \n",
    "      str(round(model.score(x_out,y_out),2)).rjust(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automate the process of picking alpha. \n",
    "To do so, we need to modify the loop to keep track of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_models(data,alphas=[1]):\n",
    "    \n",
    "    y_train,y_valid,x_train,x_valid = data\n",
    "\n",
    "    # Loop through alphas and update the best model if needed\n",
    "    best_model = None\n",
    "    best_score = -99\n",
    "\n",
    "    for a in alphas:\n",
    "        scaler = StandardScaler()\n",
    "        lasso = Lasso(alpha=a)\n",
    "        model = Pipeline(steps=[(\"scaler\", scaler),\n",
    "                               (\"lasso\", lasso)])\n",
    "        model.fit(x_train,y_train)\n",
    "\n",
    "        training_score = model.score(x_train,y_train)\n",
    "        valid_score = model.score(x_valid,y_valid)\n",
    "        print(str(a).ljust(10), \n",
    "              str(round(training_score,2)).ljust(8), \n",
    "              str(round(valid_score,2)).rjust(5)) \n",
    "\n",
    "        if valid_score > best_score:\n",
    "            best_score = valid_score\n",
    "            best_model = model\n",
    "\n",
    "    # Check model performance with test data\n",
    "    print(\"Best alpha value:\",best_model[\"lasso\"].alpha)\n",
    "    print(\"Test R-Squared:\",round(best_model.score(x_out,y_out),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.31      0.02\n",
      "5          0.31      0.03\n",
      "10         0.31      0.04\n",
      "50         0.3       0.14\n",
      "100        0.28      0.22\n",
      "500        0.18      0.21\n",
      "1000       0.08      0.09\n",
      "5000       0.0      -0.03\n",
      "Best alpha value: 100\n",
      "Test R-Squared: 0.36\n"
     ]
    }
   ],
   "source": [
    "# Try out the model\n",
    "find_best_models(data_in,alphas=alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. K-Fold Cross Validation\n",
    "\n",
    "A problem with dividing the data into three parts is that we are using a lot less data for training. **K-Fold Cross Validation** is a method to overcome that problem: instead of having a separate validation set, we divide our training set into $K$ equal parts. We use $K-1$ parts for training and validate with the remaining part. This process can be repeated for $K$ times, each time using a different part for validation. We then take the average score from these $K$ runs to pick our hyperparameters.\n",
    "\n",
    "<img src=\"../Images/cross_validation.png\" width=\"80%\">\n",
    "Source: <a href=\"https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\">\n",
    "Towards Data Science</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01982245,  0.03936871,  0.17304933,  0.07723184, -0.25782771])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model,x_in,y_in,cv=5)\n",
    "\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can loop through different alphas and pick the one that works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.0703\n",
      "5          0.073\n",
      "10         0.0764\n",
      "50         0.0939\n",
      "100        0.1027\n",
      "500        0.103\n",
      "1000       0.025\n",
      "5000       -0.1568\n",
      "Best alpha value: 500\n",
      "Test R-Squared: 0.23\n"
     ]
    }
   ],
   "source": [
    "train_num = 60\n",
    "alphas = [1,5,10,50,100,500,1000,5000]\n",
    "\n",
    "score_list = [] #List for saving scores\n",
    "\n",
    "# Splitting the data\n",
    "y_in,y_out,x_in,x_out = train_test_split(auto[\"price\"],\n",
    "                                         auto[[\"mpg\",\"weight\",\"headroom\",\"displacement\"]],\n",
    "                                         train_size=0.8)\n",
    "\n",
    "# Loop through different alphas\n",
    "best_alpha = None\n",
    "best_score = -99\n",
    "\n",
    "for a in alphas:\n",
    "    scaler = StandardScaler()\n",
    "    lasso = Lasso(alpha=a)\n",
    "    model = Pipeline(steps=[(\"scaler\", scaler),\n",
    "                           (\"lasso\", lasso)])\n",
    "    scores = cross_val_score(model,x,y,cv=5)\n",
    "    avg_score = np.mean(scores)\n",
    "    print(str(a).ljust(10),\n",
    "          str(round(avg_score,4)).rjust(5))\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_alpha = a\n",
    "\n",
    "# Check model performance with test data\n",
    "scaler = StandardScaler()\n",
    "lasso = Lasso(alpha=best_alpha)\n",
    "best_model = Pipeline(steps=[(\"scaler\", scaler),\n",
    "                       (\"lasso\", lasso)])\n",
    "best_model.fit(x_in,y_in)\n",
    "print(\"Best alpha value:\",best_alpha)\n",
    "print(\"Test R-Squared:\",round(best_model.score(x_out,y_out),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold cross-validation trades data with training time. Having a high number of folds might be worthwhile when data is limited and the model is relatively simple. For models such neural networks that are time-consuming to train, the number of folds will have to be low, perhaps to the point that only the simple train-validation-test split is feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. GridSearchCV\n",
    "\n",
    "In practice, you should use either scikit-learn's `GridSearchCV` or `RandomizedSearchCV` instead of writing your own loop. This is particularly true if the model has multiple hyperparameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;lasso&#x27;, Lasso())]),\n",
       "             param_grid={&#x27;lasso__alpha&#x27;: [1, 5, 10, 50, 100, 500, 1000, 5000]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;lasso&#x27;, Lasso())]),\n",
       "             param_grid={&#x27;lasso__alpha&#x27;: [1, 5, 10, 50, 100, 500, 1000, 5000]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;lasso&#x27;, Lasso())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Lasso</label><div class=\"sk-toggleable__content\"><pre>Lasso()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('lasso', Lasso())]),\n",
       "             param_grid={'lasso__alpha': [1, 5, 10, 50, 100, 500, 1000, 5000]})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use a dictionary to specify the parameters we need to go through\n",
    "parameters = {'lasso__alpha':[1,5,10,50,100,500,1000,5000]}\n",
    "scaler = StandardScaler()\n",
    "lasso = Lasso()\n",
    "model = Pipeline(steps=[(\"scaler\", scaler),\n",
    "                        (\"lasso\", lasso)])\n",
    "gscv = GridSearchCV(model,parameters,cv=5)\n",
    "gscv.fit(x_in, y_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing hyperparameter(s) and the best score are recorded in ```best_params_``` and ```best_score_``` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lasso__alpha': 1000}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameter(s)\n",
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.41717993122383346"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best score\n",
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` object can be used just like any other `scikit-learn` models.\n",
    "It will use the best model it has found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1546671956360689"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.score(x_out,y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the best model directly with ```best_estimator_```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1546671956360689"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = gscv.best_estimator_\n",
    "best_model.score(x_out,y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On powerful computers with many CPU cores, you can speed up the\n",
    "search by setting `n_jobs` to a number bigger than one. \n",
    "This will parallelize the search by the number you specify.\n",
    "Because parameter search is perfectly parallel, you will see\n",
    "speedup proportional to `n_jobs`, \n",
    "as long as you are not maxing out all available CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel search\n",
    "gscv = GridSearchCV(model,parameters,cv=5,n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt\n",
    "Automatically tries many different models and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.98trial/s, best loss: 0.5159810118802733]\n",
      "100%|██████████| 2/2 [00:00<00:00, 15.07trial/s, best loss: 0.4510744726606646]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10.74trial/s, best loss: 0.4510744726606646]\n",
      "100%|██████████| 4/4 [00:00<00:00, 19.17trial/s, best loss: 0.4510744726606646]\n",
      "100%|██████████| 5/5 [00:00<00:00,  2.74trial/s, best loss: 0.4510744726606646]\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.78trial/s, best loss: 0.4510744726606646]\n",
      "100%|██████████| 7/7 [00:00<00:00, 18.52trial/s, best loss: 0.4510744726606646]\n",
      "100%|██████████| 8/8 [00:00<00:00,  1.95trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 9/9 [00:01<00:00,  1.79s/trial, best loss: 0.32721780663228883]\n",
      "100%|██████████| 10/10 [00:00<00:00,  5.97trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 11/11 [00:00<00:00, 17.89trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 12/12 [00:00<00:00,  2.00trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 13/13 [00:00<00:00, 17.63trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 14/14 [00:00<00:00,  5.12trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 15/15 [00:00<00:00, 11.14trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 16/16 [00:00<00:00,  5.59trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 17/17 [00:00<00:00, 10.80trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 18/18 [00:00<00:00,  5.72trial/s, best loss: 0.32721780663228883]\n",
      "100%|██████████| 19/19 [00:00<00:00, 18.72trial/s, best loss: 0.32721780663228883]\n",
      " 95%|█████████▌| 19/20 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/network/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:1527: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 15.06trial/s, best loss: 0.32721780663228883]\n",
      "0.28454360626094366\n",
      "{'learner': AdaBoostRegressor(learning_rate=0.02926615872179483, loss='exponential',\n",
      "                  n_estimators=497, random_state=4), 'preprocs': (Normalizer(),), 'ex_preprocs': ()}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/network/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but Normalizer was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from hpsklearn import HyperoptEstimator,any_preprocessing,any_regressor\n",
    "from hyperopt import tpe\n",
    "\n",
    "model = HyperoptEstimator(regressor=any_regressor(\"my_rego\"),\n",
    "                          preprocessing=any_preprocessing(\"my_pre\"),\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=20)\n",
    "model.fit(x_in, y_in)\n",
    "print(model.score(x_out, y_out))\n",
    "print(model.best_model())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
