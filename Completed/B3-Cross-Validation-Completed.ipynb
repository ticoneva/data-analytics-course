{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "In the previous notebook, we see that regularization is crucial to training a good model. The strength of regularization is controlled by a **hyperparameter** ```alpha```. How should we pick such hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # organize data\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso # regressions\n",
    "import numpy as np # calculate mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>price</th>\n",
       "      <th>mpg</th>\n",
       "      <th>rep78</th>\n",
       "      <th>headroom</th>\n",
       "      <th>trunk</th>\n",
       "      <th>weight</th>\n",
       "      <th>length</th>\n",
       "      <th>turn</th>\n",
       "      <th>displacement</th>\n",
       "      <th>gear_ratio</th>\n",
       "      <th>foreign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMC Concord</td>\n",
       "      <td>4099</td>\n",
       "      <td>22</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>11</td>\n",
       "      <td>2930</td>\n",
       "      <td>186</td>\n",
       "      <td>40</td>\n",
       "      <td>121</td>\n",
       "      <td>3.58</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMC Pacer</td>\n",
       "      <td>4749</td>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3350</td>\n",
       "      <td>173</td>\n",
       "      <td>40</td>\n",
       "      <td>258</td>\n",
       "      <td>2.53</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMC Spirit</td>\n",
       "      <td>3799</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2640</td>\n",
       "      <td>168</td>\n",
       "      <td>35</td>\n",
       "      <td>121</td>\n",
       "      <td>3.08</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buick Century</td>\n",
       "      <td>4816</td>\n",
       "      <td>20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>16</td>\n",
       "      <td>3250</td>\n",
       "      <td>196</td>\n",
       "      <td>40</td>\n",
       "      <td>196</td>\n",
       "      <td>2.93</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buick Electra</td>\n",
       "      <td>7827</td>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20</td>\n",
       "      <td>4080</td>\n",
       "      <td>222</td>\n",
       "      <td>43</td>\n",
       "      <td>350</td>\n",
       "      <td>2.41</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            make  price  mpg  rep78  headroom  trunk  weight  length  turn  \\\n",
       "0    AMC Concord   4099   22    3.0       2.5     11    2930     186    40   \n",
       "1      AMC Pacer   4749   17    3.0       3.0     11    3350     173    40   \n",
       "2     AMC Spirit   3799   22    NaN       3.0     12    2640     168    35   \n",
       "3  Buick Century   4816   20    3.0       4.5     16    3250     196    40   \n",
       "4  Buick Electra   7827   15    4.0       4.0     20    4080     222    43   \n",
       "\n",
       "   displacement  gear_ratio   foreign  \n",
       "0           121        3.58  Domestic  \n",
       "1           258        2.53  Domestic  \n",
       "2           121        3.08  Domestic  \n",
       "3           196        2.93  Domestic  \n",
       "4           350        2.41  Domestic  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data. \n",
    "auto = pd.read_csv(\"../Data/auto.csv\")\n",
    "\n",
    "# Check data\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. A simple out-of-sample test\n",
    "Let's start with a simple out-of-sample test: we will divide our data into two parts, one for training the model and the other for testing the model's out-of-sample performance. The former is commonly called **training set** while the latter **test set** or **holdout set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample R-squared: 0.3239678139252299\n",
      "Out-of-sample R-squared: 0.04190498919752106\n"
     ]
    }
   ],
   "source": [
    "# Pick variables\n",
    "y = auto[\"price\"]\n",
    "x = auto[[\"mpg\",\"weight\"]]\n",
    "\n",
    "# Use about 3/4 of data for training: 60 samples\n",
    "train_num = 60\n",
    "\n",
    "# In-sample data for training model\n",
    "y_in = y[:train_num]\n",
    "x_in = x[:train_num]\n",
    "\n",
    "# Out-of-sample data for testing model\n",
    "y_out = y[train_num:]\n",
    "x_out = x[train_num:]\n",
    "\n",
    "# Train Ridge model and show R-Squared values\n",
    "ridge = Ridge(alpha=5000000)\n",
    "ridge.fit(x_in,y_in)\n",
    "print(\"in-sample R-squared:\",ridge.score(x_in,y_in))\n",
    "print(\"Out-of-sample R-squared:\",ridge.score(x_out,y_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's loop through different values of alpha and see how it affects the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.33     -0.13\n",
      "5          0.33     -0.13\n",
      "50         0.33     -0.12\n",
      "500        0.33     -0.09\n",
      "5000       0.33     -0.06\n",
      "50000      0.33     -0.05\n",
      "500000     0.33     -0.04\n",
      "5000000    0.32      0.04\n",
      "50000000   0.21      0.16\n",
      "500000000  0.04      0.04\n"
     ]
    }
   ],
   "source": [
    "# Alphas to go through\n",
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000,500000000]\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)\n",
    "    print(str(a).ljust(10), #left-justified, width=10\n",
    "          str(round(ridge.score(x_in,y_in),2)).ljust(8), #left justified, width=5\n",
    "          str(round(ridge.score(x_out,y_out),2)).rjust(5)) #right justified, width=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While regularization helps us get more consistent performance, our model simply isn't really good. What could be the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Shuffling data\n",
    "\n",
    "If the data is sorted, splitting the data sequentially would give us unrepresentative sets of data. To deal with that, we can shuffle our data before splitting it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.27       0.4\n",
      "5          0.27       0.4\n",
      "50         0.27       0.4\n",
      "500        0.27       0.4\n",
      "5000       0.27       0.4\n",
      "50000      0.27       0.4\n",
      "500000     0.27       0.4\n",
      "5000000    0.27       0.4\n",
      "50000000   0.18       0.2\n",
      "500000000  0.03      -0.0\n"
     ]
    }
   ],
   "source": [
    "# Import function for shuffling\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Shuffle observations\n",
    "y,x = shuffle(auto[\"price\"],auto[[\"mpg\",\"weight\"]],random_state=1234)\n",
    "\n",
    "# Use about 3/4 of data for training: 60 samples\n",
    "train_num = 60\n",
    "\n",
    "# In-sample data for training model\n",
    "y_in = y[:train_num]\n",
    "x_in = x[:train_num]\n",
    "\n",
    "# Out-of-sample data for testing model\n",
    "y_out = y[train_num:]\n",
    "x_out = x[train_num:]\n",
    "\n",
    "# Train Ridge model with different alphas and show R-Squared values\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)\n",
    "    print(str(a).ljust(10), # left-justified, width=10\n",
    "          str(round(ridge.score(x_in,y_in),2)).ljust(8), # left justified, width=5\n",
    "          str(round(ridge.score(x_out,y_out),1)).rjust(5)) # right justified, width=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you shuffle your data before splitting it? It depends. For cross-section data it is probably a good idea, but for time-series data it would be a bad idea, since you are introducing *hindsight bias* if you can train with data that is generated after some of your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. train_test_split\n",
    "\n",
    "In practice, you will probably use sckit-learn's ```train_test_split``` method to split the data. ```train_test_split``` shuffles the data by default, so there is no need to call ```shuffle``` separately. The default is a 75/25 split, which you can change by providing a different ```train_size``` or ```test_size```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.28       0.3\n",
      "5          0.28       0.3\n",
      "50         0.28       0.3\n",
      "500        0.28       0.3\n",
      "5000       0.28       0.3\n",
      "50000      0.28       0.3\n",
      "500000     0.28       0.3\n",
      "5000000    0.27       0.3\n",
      "50000000   0.18       0.2\n",
      "500000000  0.04       0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "y_in,y_out,x_in,x_out = train_test_split(auto[\"price\"],\n",
    "                                         auto[[\"mpg\",\"weight\"]],\n",
    "                                         train_size=0.8)\n",
    "\n",
    "# Train Ridge model with different alphas and show R-Squared values\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)\n",
    "    print(str(a).ljust(10), # left-justified, width=10\n",
    "          str(round(ridge.score(x_in,y_in),2)).ljust(8), # left justified, width=5\n",
    "          str(round(ridge.score(x_out,y_out),1)).rjust(5)) # right justified, width=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Validation\n",
    "\n",
    "So we try out different values of ```alpha``` and pick the one that give us the highest out-of-sample score. Do so is actually problematic: since ```alpha``` is a parameter of our model, we are effectively training our model with the supposingly out-of-sample data, which means the test set no longer gives us truly out-of-sample results. In particular, there is a real chance of overfitting our model to the test set via ```alpha```.\n",
    "\n",
    "\n",
    "The correct approach is to split the data into three parts: besides the train set and test set, we have an additional **validation set** for picking the model's hyperparameters. It is common to use around 60% of the data for training and 20% each for validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.3        0.2\n",
      "5          0.3        0.2\n",
      "50         0.3        0.2\n",
      "500        0.3        0.2\n",
      "5000       0.29       0.3\n",
      "50000      0.29       0.3\n",
      "500000     0.29       0.3\n",
      "5000000    0.29       0.3\n",
      "50000000   0.19       0.2\n",
      "500000000  0.03       0.0\n"
     ]
    }
   ],
   "source": [
    "# 64% for training, 16% for validation and 20% for out-of-sample test\n",
    "y_in,y_out,x_in,x_out = train_test_split(auto[\"price\"],\n",
    "                                         auto[[\"mpg\",\"weight\"]],\n",
    "                                         test_size=0.2)\n",
    "y_train,y_valid,x_train,x_valid = train_test_split(y_in,\n",
    "                                                   x_in,\n",
    "                                         train_size=0.8)\n",
    "\n",
    "# Train Ridge model with different alphas and show R-Squared values\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)\n",
    "    print(str(a).ljust(10),\n",
    "          str(round(ridge.score(x_in,y_in),2)).ljust(8), \n",
    "          str(round(ridge.score(x_out,y_out),1)).rjust(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After picking the best alpha based on validation data, the final step is to test the model's out-of-sample performance with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000     0.28      0.18\n"
     ]
    }
   ],
   "source": [
    "a = 500000\n",
    "ridge = Ridge(alpha=a)\n",
    "ridge.fit(x_train,y_train)\n",
    "print(str(a).ljust(10), \n",
    "      str(round(ridge.score(x_in,y_in),2)).ljust(8), \n",
    "      str(round(ridge.score(x_out,y_out),2)).rjust(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automate the process of picking alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.29     -0.35\n",
      "5          0.29     -0.35\n",
      "50         0.29     -0.34\n",
      "500        0.29     -0.31\n",
      "5000       0.29     -0.28\n",
      "50000      0.28     -0.27\n",
      "500000     0.28     -0.26\n",
      "5000000    0.28     -0.18\n",
      "50000000   0.18     -0.05\n",
      "Best alpha value: 50000000\n",
      "Test R-Squared: 0.19\n"
     ]
    }
   ],
   "source": [
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000]\n",
    "\n",
    "# Loop through alphas and update the best model if needed\n",
    "best_model = None\n",
    "best_score = -99\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_train,y_train)\n",
    "    \n",
    "    training_score = ridge.score(x_in,y_in)\n",
    "    valid_score = ridge.score(x_valid,y_valid)\n",
    "    print(str(a).ljust(10), \n",
    "          str(round(training_score,2)).ljust(8), \n",
    "          str(round(valid_score,2)).rjust(5)) \n",
    "    \n",
    "    if valid_score > best_score:\n",
    "        best_score = valid_score\n",
    "        best_model = ridge\n",
    "\n",
    "# Check model performance with test data\n",
    "print(\"Best alpha value:\",best_model.alpha)\n",
    "print(\"Test R-Squared:\",round(best_model.score(x_out,y_out),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. K-Fold Cross Validation\n",
    "\n",
    "A problem with dividing the data into three parts is that we are using a lot less data for training. **K-Fold Cross Validation** is a method to overcome that problem: instead of having a separate validation set, we divide our training set into $K$ equal parts. We use $K-1$ parts for training and validate with the remaining part. This process can be repeated for $K$ times, each time using a different part for validation. We then take the average score from these $K$ runs to pick our hyperparameters.\n",
    "\n",
    "<img src=\"../Images/cross_validation.png\" width=\"80%\">\n",
    "Source: <a href=\"https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\">\n",
    "Towards Data Science</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27499178, -5.69854808,  0.38183644, -0.88662471,  0.16251918])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ridge = Ridge(alpha=5000)\n",
    "scores = cross_val_score(ridge,x_in,y_in,cv=5)\n",
    "\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can loop through different alphas and pick the one that works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.1482\n",
      "5          0.1483\n",
      "50         0.1501\n",
      "500        0.1579\n",
      "5000       0.1639\n",
      "50000      0.1649\n",
      "500000     0.1656\n",
      "5000000    0.1664\n",
      "50000000   0.0683\n",
      "500000000  -0.1123\n",
      "Best alpha value: 5000000\n",
      "Test R-Squared: -0.07\n"
     ]
    }
   ],
   "source": [
    "train_num = 60\n",
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000,500000000]\n",
    "\n",
    "score_list = [] #List for saving scores\n",
    "\n",
    "# Splitting the data\n",
    "y_in,y_out,x_in,x_out = train_test_split(auto[\"price\"],\n",
    "                                         auto[[\"mpg\",\"weight\"]],\n",
    "                                         train_size=0.8)\n",
    "\n",
    "# Loop through different alphas\n",
    "best_alpha = None\n",
    "best_score = -99\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    scores = cross_val_score(ridge,x,y,cv=5)\n",
    "    avg_score = np.mean(scores)\n",
    "    print(str(a).ljust(10),\n",
    "          str(round(avg_score,4)).rjust(5))\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_alpha = a\n",
    "\n",
    "# Check model performance with test data\n",
    "best_model = Ridge(alpha=best_alpha)\n",
    "best_model.fit(x_in,y_in)\n",
    "print(\"Best alpha value:\",best_alpha)\n",
    "print(\"Test R-Squared:\",round(best_model.score(x_out,y_out),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold cross-validation trades data with training time. Having a high number of folds might be worthwhile when data is limited and the model is relatively simple. For models such neural networks that are time-consuming to train, the number of folds will have to be low, perhaps to the point that only the simple train-validation-test split is feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. GridSearchCV\n",
    "\n",
    "In practice, you should use scikit-learn's ```GridSearchCV``` instead of writing your own loop. This is particularly true if the model has multiple hyperparameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Ridge(),\n",
       "             param_grid={'alpha': [1, 5, 50, 500, 5000, 50000, 500000, 5000000,\n",
       "                                   50000000]})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use a dictionary to specify the parameters we need to go through\n",
    "parameters = {'alpha':[1,5,50,500,5000,50000,500000,5000000,50000000]}\n",
    "ridge = Ridge()\n",
    "gscv = GridSearchCV(ridge,parameters,cv=5)\n",
    "gscv.fit(x_in, y_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing hyperparameter(s) and the best score are recorded in ```best_params_``` and ```best_score_``` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 500000}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameter(s)\n",
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17953714455658307"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best score\n",
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best_estimator is saved in ```best_estimator_```. We can use that for out-of-sample test or making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11636418877511123"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = gscv.best_estimator_\n",
    "best_model.score(x_out,y_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
