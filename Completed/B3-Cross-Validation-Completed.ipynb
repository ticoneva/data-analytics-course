{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "In the previous notebook, we see that regularization is crucial to training a good model. The strength of regularization is controlled by a **hyperparameter** ```alpha```. How should we pick such hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #organize data\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso #regressions\n",
    "import numpy as np #calculate mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>price</th>\n",
       "      <th>mpg</th>\n",
       "      <th>rep78</th>\n",
       "      <th>headroom</th>\n",
       "      <th>trunk</th>\n",
       "      <th>weight</th>\n",
       "      <th>length</th>\n",
       "      <th>turn</th>\n",
       "      <th>displacement</th>\n",
       "      <th>gear_ratio</th>\n",
       "      <th>foreign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMC Concord</td>\n",
       "      <td>4099</td>\n",
       "      <td>22</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>11</td>\n",
       "      <td>2930</td>\n",
       "      <td>186</td>\n",
       "      <td>40</td>\n",
       "      <td>121</td>\n",
       "      <td>3.58</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMC Pacer</td>\n",
       "      <td>4749</td>\n",
       "      <td>17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3350</td>\n",
       "      <td>173</td>\n",
       "      <td>40</td>\n",
       "      <td>258</td>\n",
       "      <td>2.53</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMC Spirit</td>\n",
       "      <td>3799</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2640</td>\n",
       "      <td>168</td>\n",
       "      <td>35</td>\n",
       "      <td>121</td>\n",
       "      <td>3.08</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buick Century</td>\n",
       "      <td>4816</td>\n",
       "      <td>20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>16</td>\n",
       "      <td>3250</td>\n",
       "      <td>196</td>\n",
       "      <td>40</td>\n",
       "      <td>196</td>\n",
       "      <td>2.93</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buick Electra</td>\n",
       "      <td>7827</td>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20</td>\n",
       "      <td>4080</td>\n",
       "      <td>222</td>\n",
       "      <td>43</td>\n",
       "      <td>350</td>\n",
       "      <td>2.41</td>\n",
       "      <td>Domestic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            make  price  mpg  rep78  headroom  trunk  weight  length  turn  \\\n",
       "0    AMC Concord   4099   22    3.0       2.5     11    2930     186    40   \n",
       "1      AMC Pacer   4749   17    3.0       3.0     11    3350     173    40   \n",
       "2     AMC Spirit   3799   22    NaN       3.0     12    2640     168    35   \n",
       "3  Buick Century   4816   20    3.0       4.5     16    3250     196    40   \n",
       "4  Buick Electra   7827   15    4.0       4.0     20    4080     222    43   \n",
       "\n",
       "   displacement  gear_ratio   foreign  \n",
       "0           121        3.58  Domestic  \n",
       "1           258        2.53  Domestic  \n",
       "2           121        3.08  Domestic  \n",
       "3           196        2.93  Domestic  \n",
       "4           350        2.41  Domestic  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data. \n",
    "auto = pd.read_csv(\"auto.csv\")\n",
    "\n",
    "#Check data\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. A simple out-of-sample test\n",
    "Let's start with a simple out-of-sample test: we will divide our data into two parts, one for training the model and the other for testing the model's out-of-sample performance. The former is commonly called **training set** while the latter **test set** or **holdout set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample R-squared: 0.332711250555\n",
      "Out-of-sample R-squared: -0.13200781595\n"
     ]
    }
   ],
   "source": [
    "#Pick variables\n",
    "y = auto[\"price\"]\n",
    "x = auto[[\"mpg\",\"weight\"]]\n",
    "\n",
    "#Use about 3/4 of data for training: 60 samples\n",
    "train_num = 60\n",
    "\n",
    "#In-sample data for training model\n",
    "y_in = y[:train_num]\n",
    "x_in = x[:train_num]\n",
    "\n",
    "#Out-of-sample data for testing model\n",
    "y_out = y[train_num:]\n",
    "x_out = x[train_num:]\n",
    "\n",
    "#Train OLS model and show R-Squared values\n",
    "ols = LinearRegression()\n",
    "ols.fit(x_in,y_in)\n",
    "print(\"in-sample R-squared:\",ols.score(x_in,y_in))\n",
    "print(\"Out-of-sample R-squared:\",ols.score(x_out,y_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try training a Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample R-squared: 0.323967813925\n",
      "Out-of-sample R-squared: 0.0419049891975\n"
     ]
    }
   ],
   "source": [
    "#Train Ridge model and show R-Squared values\n",
    "ridge = Ridge(alpha=5000000)\n",
    "ridge.fit(x_in,y_in)\n",
    "print(\"in-sample R-squared:\",ridge.score(x_in,y_in))\n",
    "print(\"Out-of-sample R-squared:\",ridge.score(x_out,y_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's loop through different values of alpha and see how it affects the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.33     -0.13\n",
      "5          0.33     -0.13\n",
      "50         0.33     -0.12\n",
      "500        0.33     -0.09\n",
      "5000       0.33     -0.06\n",
      "50000      0.33     -0.05\n",
      "500000     0.33     -0.04\n",
      "5000000    0.32      0.04\n",
      "50000000   0.21      0.16\n",
      "500000000  0.04      0.04\n"
     ]
    }
   ],
   "source": [
    "#Alphas to go through\n",
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000,500000000]\n",
    "\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)\n",
    "    print(str(a).ljust(10), #left-justified, width=10\n",
    "          str(round(ridge.score(x_in,y_in),2)).ljust(8), #left justified, width=5\n",
    "          str(round(ridge.score(x_out,y_out),2)).rjust(5)) #right justified, width=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While regularization helps us get more consistent performance, our model simply isn't really good. What could be the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Shuffling data\n",
    "\n",
    "If the data is sorted, splitting the data sequentially would give us unrepresentative sets of data. To deal with that, we can shuffle our data before splitting it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-sample R-squared: 0.273990201463\n",
      "Out-of-sample R-squared: 0.392093024031\n"
     ]
    }
   ],
   "source": [
    "#Import function for shuffling\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "#Shuffle observations\n",
    "y,x = shuffle(auto[\"price\"],auto[[\"mpg\",\"weight\"]],random_state=1234)\n",
    "\n",
    "#Use about 3/4 of data for training: 60 samples\n",
    "train_num = 60\n",
    "\n",
    "#In-sample data for training model\n",
    "y_in = y[:train_num]\n",
    "x_in = x[:train_num]\n",
    "\n",
    "#Out-of-sample data for testing model\n",
    "y_out = y[train_num:]\n",
    "x_out = x[train_num:]\n",
    "\n",
    "#Train OLS model and show R-Squared values\n",
    "ols = LinearRegression()\n",
    "ols.fit(x_in,y_in)\n",
    "print(\"in-sample R-squared:\",ols.score(x_in,y_in))\n",
    "print(\"Out-of-sample R-squared:\",ols.score(x_out,y_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would the Ridge regression fare in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.27      0.39\n",
      "5          0.27      0.39\n",
      "50         0.27      0.39\n",
      "500        0.27      0.39\n",
      "5000       0.27      0.39\n",
      "50000      0.27      0.39\n",
      "500000     0.27      0.39\n",
      "5000000    0.27      0.39\n",
      "50000000   0.18      0.25\n",
      "500000000  0.03      -0.0\n"
     ]
    }
   ],
   "source": [
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)\n",
    "    print(str(a).ljust(10), \n",
    "          str(round(ridge.score(x_in,y_in),2)).ljust(8), \n",
    "          str(round(ridge.score(x_out,y_out),2)).rjust(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you shuffle your data before splitting it? It depends. For cross-section data it is probably a good idea, but for time-series data it would be a bad idea, since you are introducing *hindsight bias* if you can train with data that is generated after some of your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Validation\n",
    "\n",
    "So we try out different values of ```alpha``` and pick the one that give us the highest out-of-sample score. Do so is actually problematic: since ```alpha``` is a parameter of our model, we are effectively training our model with the supposingly out-of-sample data, which means the test set no longer gives us truly out-of-sample results. In particular, there is a real chance of overfitting our model to the test set via ```alpha```.\n",
    "\n",
    "\n",
    "The correct approach is to split the data into three parts: besides the train set and test set, we have an additional **validation set** for picking the model's hyperparameters. It is common to use around 60% of the data for training and 20% each for validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.35      0.01\n",
      "5          0.35      0.01\n",
      "50         0.35      0.01\n",
      "500        0.34      0.05\n",
      "5000       0.33      0.08\n",
      "50000      0.33      0.08\n",
      "500000     0.33      0.09\n",
      "5000000    0.32      0.08\n",
      "50000000   0.19      0.02\n",
      "500000000  0.03     -0.06\n"
     ]
    }
   ],
   "source": [
    "#Cutoffs\n",
    "train_num = 45 #Number of samples used in training\n",
    "valid_num = 15 #Number of samples used in picking alpha\n",
    "\n",
    "#Data for training model\n",
    "y_in = y[:train_num]\n",
    "x_in = x[:train_num]\n",
    "\n",
    "#Data for picking alpha\n",
    "y_valid = y[train_num:train_num+valid_num]\n",
    "x_valid = x[train_num:train_num+valid_num]\n",
    "\n",
    "#Data for testing model\n",
    "y_test = y[train_num+valid_num:]\n",
    "x_test = x[train_num+valid_num:]\n",
    "\n",
    "#Try different alphas\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)\n",
    "    print(str(a).ljust(10), \n",
    "          str(round(ridge.score(x_in,y_in),2)).ljust(8), \n",
    "          str(round(ridge.score(x_valid,y_valid),2)).rjust(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After picking the best alpha based on validation data, the final step is to test the model's out-of-sample performance with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000     0.33      0.39\n"
     ]
    }
   ],
   "source": [
    "a = 500000\n",
    "ridge = Ridge(alpha=a)\n",
    "ridge.fit(x_in,y_in)\n",
    "print(str(a).ljust(10), \n",
    "      str(round(ridge.score(x_in,y_in),2)).ljust(8), \n",
    "      str(round(ridge.score(x_test,y_test),2)).rjust(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automate the process of picking alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.35      0.01\n",
      "5          0.35      0.01\n",
      "50         0.35      0.01\n",
      "500        0.34      0.05\n",
      "5000       0.33      0.08\n",
      "50000      0.33      0.08\n",
      "500000     0.33      0.09\n",
      "5000000    0.32      0.08\n",
      "50000000   0.19      0.02\n",
      "Best alpha value: 500000\n",
      "Test R-Squared: 0.39\n"
     ]
    }
   ],
   "source": [
    "train_num = 45\n",
    "valid_num = 15\n",
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000]\n",
    "\n",
    "models = [] #List for saving models\n",
    "\n",
    "#Data for training model\n",
    "y_in = y[:train_num]\n",
    "x_in = x[:train_num]\n",
    "\n",
    "#Data for picking alpha\n",
    "y_valid = y[train_num:train_num+valid_num]\n",
    "x_valid = x[train_num:train_num+valid_num]\n",
    "\n",
    "#Data for testing model\n",
    "y_test = y[train_num+valid_num:]\n",
    "x_test = x[train_num+valid_num:]\n",
    "\n",
    "#Try different alphas\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(x_in,y_in)    \n",
    "    models.append(ridge) #Save model to list\n",
    "\n",
    "#Go through saved models and pick the best one based on validation score\n",
    "best_model = None\n",
    "best_score = -99\n",
    "\n",
    "for m in models:\n",
    "    a = m.alpha\n",
    "    training_score = m.score(x_in,y_in)\n",
    "    valid_score = m.score(x_valid,y_valid)\n",
    "    print(str(a).ljust(10), \n",
    "          str(round(training_score,2)).ljust(8), \n",
    "          str(round(valid_score,2)).rjust(5)) \n",
    "    \n",
    "    if valid_score > best_score:\n",
    "        best_score = valid_score\n",
    "        best_model = m\n",
    "\n",
    "#Check model performance with test data\n",
    "print(\"Best alpha value:\",best_model.alpha)\n",
    "print(\"Test R-Squared:\",round(best_model.score(x_test,y_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. K-Fold Cross Validation\n",
    "\n",
    "A problem with dividing the data into three parts is that we are using a lot less data for training. **K-Fold Cross Validation** is a method to overcome that problem: instead of having a separate validation set, we divide our training set into $K$ equal parts. We use $K-1$ parts for training and validate with the remaining part. This process can be repeated for $K$ times, each time using a different part for validation. We then take the average score from these $K$ runs to pick our hyperparameters.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*J2B_bcbd1-s1kpWOu_FZrg.png\" width=\"80%\">\n",
    "Source: <a href=\"https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\">\n",
    "Towards Data Science</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2378138 , -0.25374265,  0.36447818,  0.08180748,  0.3891237 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ridge = Ridge(alpha=5000)\n",
    "scores = cross_val_score(ridge,x,y,cv=5)\n",
    "\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can loop through different alphas and pick the one that works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          0.1482\n",
      "5          0.1483\n",
      "50         0.1501\n",
      "500        0.1579\n",
      "5000       0.1639\n",
      "50000      0.1649\n",
      "500000     0.1656\n",
      "5000000    0.1664\n",
      "50000000   0.0683\n",
      "500000000  -0.1123\n",
      "Best alpha value: 5000000\n"
     ]
    }
   ],
   "source": [
    "train_num = 60\n",
    "alphas = [1,5,50,500,5000,50000,500000,5000000,50000000,500000000]\n",
    "\n",
    "score_list = [] #List for saving scores\n",
    "\n",
    "#Data for training model\n",
    "y_in = y[:train_num]\n",
    "x_in = x[:train_num]\n",
    "\n",
    "#Data for testing model\n",
    "y_test = y[train_num:]\n",
    "x_test = x[train_num:]\n",
    "\n",
    "#Try different alphas\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    scores = cross_val_score(ridge,x,y,cv=5)\n",
    "    \n",
    "    score_list.append([a,np.mean(scores)]) #Save scores to list\n",
    "\n",
    "#Go through saved models and pick the best one based on validation score\n",
    "best_alpha = None\n",
    "best_score = -99\n",
    "\n",
    "for s in score_list:\n",
    "    a = s[0]\n",
    "    avg_score = s[1]\n",
    "    print(str(a).ljust(10),\n",
    "          str(round(avg_score,4)).rjust(5))\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_alpha = a\n",
    "\n",
    "#Check model performance with test data\n",
    "print(\"Best alpha value:\",best_alpha)\n",
    "print(\"Test R-Squared:\",round(best_model.score(x_test,y_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Fold cross-validation** trades a data with training time. This might be a worthwhile tradeoff when data is limited and the model is relatively simple. For models such neural networks that are time-consuming to train, the simple train-validation-test split is often the only feasible way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
