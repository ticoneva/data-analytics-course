{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "How do companies such as Amazon and Netflix choose what to recommend to users out of thousands of available products? A key technique they employ is *collaborative filtering*, which is the use of information from similar users and items to predict preference for a given item.\n",
    "\n",
    "Suppose we have some movie-streaming data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>movie1</th>\n",
       "      <th>movie2</th>\n",
       "      <th>movie3</th>\n",
       "      <th>movie4</th>\n",
       "      <th>movie5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer  movie1  movie2  movie3  movie4  movie5\n",
       "0         1       1       1       0       0       0\n",
       "1         2       1       1       0       0       1\n",
       "2         3       1       0       0       0       0\n",
       "3         4       1       0       0       0       1\n",
       "4         5       0       0       1       0       1\n",
       "5         6       0       1       0       1       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Data\n",
    "raw_data = [\n",
    "            [1,1,1,0,0,0],    \n",
    "            [2,1,1,0,0,1],\n",
    "            [3,1,0,0,0,0],\n",
    "            [4,1,0,0,0,1],\n",
    "            [5,0,0,1,0,1],\n",
    "            [6,0,1,0,1,0],\n",
    "            ]\n",
    "labels = ['customer','movie1','movie2','movie3','movie4','movie5']\n",
    "data = pd.DataFrame.from_records(raw_data,columns=labels)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice data such as the one above is called *implicit data*, because it only reflects the users' preference implicitly through their choices. Because it is possible that some users dislike some choices they have made, a chosen item is not the same as a prefered item. But since it is highly unlikely that most users hate most of the choices they have made, the data is usually quite informative as a whole. \n",
    "\n",
    "Data that includes actual preference is called *explicit data*. For example, the data might contain user ratings from 1 to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer</th>\n",
       "      <th>movie1</th>\n",
       "      <th>movie2</th>\n",
       "      <th>movie3</th>\n",
       "      <th>movie4</th>\n",
       "      <th>movie5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer  movie1  movie2  movie3  movie4  movie5\n",
       "0         1       4       2       0       0       0\n",
       "1         2       3       2       0       0       3\n",
       "2         3       3       0       0       0       0\n",
       "3         4       5       0       0       0       4\n",
       "4         5       0       0       3       0       1\n",
       "5         6       0       3       0       4       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Data\n",
    "raw_data_explicit = [\n",
    "            [1,4,2,0,0,0],    \n",
    "            [2,3,2,0,0,3],\n",
    "            [3,3,0,0,0,0],\n",
    "            [4,5,0,0,0,4],\n",
    "            [5,0,0,3,0,1],\n",
    "            [6,0,3,0,4,0],\n",
    "            ]\n",
    "labels = ['customer','movie1','movie2','movie3','movie4','movie5']\n",
    "data_explicit = pd.DataFrame.from_records(raw_data_explicit,columns=labels)\n",
    "data_explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible that both explicit and implicit data are available. This is often the case because only a minority of users who have chosen an item will voluntarily provide ratings.\n",
    "\n",
    "We will mostly work with implicit data, but the techniques we cover do work with both types of data in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Nearest Neighbor\n",
    "\n",
    "One intuitively appealing approach is to look for other users that have similar records and see what they have chosen before. \n",
    "\n",
    "In the following example, we will look for the three closest neighbors to a new user and average their choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.33333333,  0.        ,  0.        ,  0.33333333]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "#Extract data from dataframe\n",
    "X = np.asarray(data[[\"movie1\",\"movie2\",\"movie3\",\"movie4\",\"movie5\"]])\n",
    "\n",
    "#Only consider the three nearest neighbor\n",
    "nbrs = NearestNeighbors(n_neighbors=3)\n",
    "nbrs.fit(X)\n",
    "\n",
    "#New user who have only watched movie 1\n",
    "x = np.array([1,0,0,0,0])\n",
    "\n",
    "#Get neighbor and calculate average choices\n",
    "neigh = nbrs.kneighbors([x],return_distance=False)\n",
    "np.mean(X[neigh],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model suggests that we should recommend the user to try out movie 2 and movie 5. \n",
    "\n",
    "We can also use ```np.argsort()``` to get a list of index to recommend starting from the most recommended item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 0, 2, 3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-(np.mean(X[neigh],axis=1) - x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above recommendation is based on the fact that the new user having already chosen movie 1. What if the new user has not yet tried anything? In that case, we might want to simply recommend the average of all users. In other words, we recommend the most popular items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Matrix Factorization\n",
    "\n",
    "Matrix factorization assumes that choice data can be represented by a matrix multiplication of item features (usually called *factors*) and user preference on those features:\n",
    "\n",
    "![Collaborative-Filtering-Matrix-Factorization](../images/collaborative-filtering-matrix-factorization.png)\n",
    "\n",
    "Let $P$ be a matrix of user preference of shape $\\text{no. of users} \\times \\text{no. of factors}$ and $Q$ a matrix of item factors of shape $\\text{no. of items} \\times \\text{no. of factors}$. Then the choice data $X$ can be represented by\n",
    "$$\n",
    "X = PQ^T\n",
    "$$\n",
    "\n",
    "There are several ways to get from $X$ to $P$ and $Q$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Single Value Decomposition (SVD)\n",
    "\n",
    "Single value decomposition performs the following factorization:\n",
    "\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $\\Sigma$ is a diagonal matrix of *singular values*. To get $X=PQ^T$, we can let $P=U$ and $Q^T=\\Sigma V^T$.\n",
    "\n",
    "SVD can be performed by calling ```scipy.sparse.linalg.svds(X,k)```, where $1\\leq k<\\min{\\{\\text{X.shape}\\}}$ is the number of singular values. Several things to note:\n",
    "- ```svds()``` expects float-point numbers, so if the data contains integers we will need to convert them to float via ```.astype(float)```.\n",
    "- ```svds()``` does not return a diagonal matrix $\\Sigma$, instead it returns its diagonal values in a 1-D array. We can apply ```np.diag()``` on this array to get $\\Sigma$. \n",
    "\n",
    "Let us try running ```svds``` on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2) (2,) (2, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "P,s,V = svds(X.astype(float),k=2)\n",
    "print(P.shape,s.shape,V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $P$ matrix represents user preference. Each row is one user and each column is the user's preference for a particular factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.71748034e-01  -4.54503234e-01]\n",
      " [ -6.86155564e-17  -6.46068676e-01]\n",
      " [  1.02923335e-16  -2.62937792e-01]\n",
      " [ -3.71748034e-01  -4.54503234e-01]\n",
      " [ -6.01500955e-01  -2.23956027e-01]\n",
      " [  6.01500955e-01  -2.23956027e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the factors are automatically generated, it is hard to know what they represent without further investigation. For movies, you can imagine that one factor might represent how much action element there is, while another might represent how much romance element there is. In any case, from the perspective of generating recommendation we do not necessarily care about what these underlying factors are, since what we want is $PQ^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take a look at $Q^T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.69456788e-16   9.73248989e-01  -6.01500955e-01   6.01500955e-01\n",
      "   -9.73248989e-01]\n",
      " [ -1.81801294e+00  -1.32452794e+00  -2.23956027e-01  -2.23956027e-01\n",
      "   -1.32452794e+00]]\n"
     ]
    }
   ],
   "source": [
    "Qt = np.dot(np.diag(s),V)\n",
    "print(Qt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column is one item and each row is the item's exposure to a particular factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the effect of having different values of $k$? Let's try that out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 4\n",
      "[[ 0.98719  1.01175 -0.01753 -0.01753  0.01175]\n",
      " [ 1.12993  0.88077  0.17779  0.17779  0.88077]\n",
      " [ 0.84446  0.14274 -0.21284 -0.21284  0.14274]\n",
      " [ 0.98719  0.01175 -0.01753 -0.01753  1.01175]\n",
      " [-0.07011  0.06433  0.90407 -0.09593  1.06433]\n",
      " [-0.07011  1.06433 -0.09593  0.90407  0.06433]]\n",
      "k = 2\n",
      "[[ 0.82629  0.96381 -0.12182  0.3254   0.2402 ]\n",
      " [ 1.17456  0.85574  0.14469  0.14469  0.85574]\n",
      " [ 0.47802  0.34827  0.05889  0.05889  0.34827]\n",
      " [ 0.82629  0.2402   0.3254  -0.12182  0.96381]\n",
      " [ 0.40715 -0.28877  0.41196 -0.31165  0.88205]\n",
      " [ 0.40715  0.88205 -0.31165  0.41196 -0.28877]]\n"
     ]
    }
   ],
   "source": [
    "#k=4\n",
    "k=4\n",
    "P,s,V = svds(X.astype(float),k)\n",
    "Qt = np.dot(np.diag(s),V)\n",
    "print(\"k =\",k)\n",
    "print(np.round(np.dot(P,Qt),5))\n",
    "\n",
    "#k=2\n",
    "k=2\n",
    "P,s,V = svds(X.astype(float),k)\n",
    "Qt = np.dot(np.diag(s),V)\n",
    "print(\"k =\",k)\n",
    "print(np.round(np.dot(P,Qt),5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how much more $PQ^T$ resembles $X$ when $k$ is large. Does that mean we want a large $k$ then? Far from it. A large $k$ means that our model will predict the existing chocies of the users perfectly. If the user has not chosen an item before, the model will predict that she does not like the item, resulting in no recommendation. In other words, our model is overfitting the data. \n",
    "\n",
    "As you should now realize, $k$ is the main hyperparameter that you would want to tune in SVD models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we generate recommendations for an arbitrary user, particular one that is not already in the data? Since\n",
    "\n",
    "$$\n",
    "X = PQ^T \\\\\n",
    "XQ = PQ^TQ \\\\\n",
    "XQ(Q^TQ)^{-1} = P\n",
    "$$\n",
    "\n",
    "This gives us an equation to find the preference of a particular user $u$:\n",
    "\n",
    "$$\n",
    "p_u = x_uQ(Q^TQ)^{-1}\n",
    "$$\n",
    "\n",
    "Which in turn allows us to predict what the user might choose:\n",
    "\n",
    "$$\n",
    "\\hat{x}_u = p_uQ^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S=Q(Q^TQ)^{-1}$. Then $p_u = x_uS$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47802431,  0.34826845,  0.0588865 ,  0.0588865 ,  0.34826845])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#New user who have only watched movie 1\n",
    "x = np.array([1,0,0,0,0])\n",
    "\n",
    "#Generate prediction\n",
    "S = np.dot(Qt.T,np.linalg.inv(np.dot(Qt,Qt.T)))\n",
    "p = np.dot(x,S)\n",
    "x_hat = np.dot(p,Qt)\n",
    "x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of nearest neighbor, our model suggests that we should recommend the user to try out movie 2 and movie 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple class that implements an interface similar to ```scikit-learn```. I call it ```SVDarnoldi``` because ```svds``` implements the <a href=\"https://en.wikipedia.org/wiki/Arnoldi_iteration#Implicitly_restarted_Arnoldi_method_.28IRAM.29\">Arnoldi iteration</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDarnoldi():\n",
    "    def __init__(self,k=2):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self,X):\n",
    "        P,s,V = svds(X.astype(float),k=self.k)\n",
    "        Qt = np.dot(np.diag(s),V)\n",
    "        self.S = np.dot(Qt.T,np.linalg.inv(np.dot(Qt,Qt.T)))\n",
    "        self.Qt = Qt\n",
    "        \n",
    "    def predict(self,x):\n",
    "        p = np.dot(x,self.S)\n",
    "        x_hat = np.dot(p,self.Qt)\n",
    "        return x_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this class, it is easy to try out different values of $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "[ 0.47802431  0.34826845  0.0588865   0.0588865   0.34826845]\n",
      "k = 2\n",
      "[ 0.47802431  0.34826845  0.0588865   0.0588865   0.34826845]\n",
      "k = 3\n",
      "[ 0.84445557  0.14273618 -0.21284164 -0.21284164  0.14273618]\n",
      "k = 4\n",
      "[ 0.84445557  0.14273618 -0.21284164 -0.21284164  0.14273618]\n"
     ]
    }
   ],
   "source": [
    "#New user who have only watched movie 1\n",
    "x = np.array([1,0,0,0,0])\n",
    "\n",
    "#Loop through\n",
    "for k in range(1,5):\n",
    "    print(\"k =\",k)\n",
    "    model = SVDarnoldi(k=k)\n",
    "    model.fit(X)\n",
    "    print(model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Alternating Least Squared (ALS)\n",
    "\n",
    "For efficiency reasons, practical implementations of matrix-factorization-based filtering mostly utilize approximation of SVD. Here we will cover a method called *alternating least squared*. The idea is as follows: we first randomly initiatizes $P$ and $Q^T$, and then iteratively update these two matrices until $PQ^T \\approx X$. But how should we update the matrices? \n",
    "\n",
    "For clarity, I will be using $'$ instead of $^T$ to denote inverse. Starting with the objective $PQ' = X$:\n",
    "\n",
    "$$\n",
    "PQ' = X \\\\\n",
    "PQ'Q = XQ \\\\\n",
    "P = XQ(Q'Q)^{-1}\n",
    "$$\n",
    "\n",
    "Similarly, \n",
    "\n",
    "$$\n",
    "PQ' = X  \\\\\n",
    "P'PQ' = P'X \\\\\n",
    "Q' = (P'P)^{-1}P'X \n",
    "$$\n",
    "\n",
    "So what we are going to do is to run $P = XQ(Q'Q)^{-1}$ and $Q = (P'P)^{-1}P'X$ iteratively until $PQ' \\approx X$. \n",
    "\n",
    "Note that what we are doing here are essentially running OLS repeatedly until we converge on a stable combination of $P$ and $Q'.$ Now it is true that when we run OLS we usually have a single column vector of dependent variable, whereas here $X$ is a matrix, but the technique is the same.\n",
    "\n",
    "Reference:\n",
    "- <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=34AEEE06F0C2428083376C26C71D7CFF?doi=10.1.1.167.5120&rep=rep1&type=pdf\">Collaborative Filtering for Implicit Datasets</a>\n",
    "- <a href=\"https://pdfs.semanticscholar.org/dbe9/d04bffb5c1df8eb721dab4f744ea81d9a4c1.pdf\">Alternating Least Squared for Personalized Ranking</a>\n",
    "\n",
    "Below is a straight-forward implementation of ALS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   loss: 0.1114\n",
      "2   loss: 0.1026\n",
      "3   loss: 0.0966\n",
      "4   loss: 0.0911\n",
      "5   loss: 0.087\n",
      "6   loss: 0.0846\n",
      "7   loss: 0.0834\n",
      "8   loss: 0.0828\n",
      "9   loss: 0.0825\n",
      "10  loss: 0.0824\n",
      "11  loss: 0.0823\n",
      "12  loss: 0.0823\n",
      "13  loss: 0.0823\n",
      "14  loss: 0.0823\n",
      "[ 0.47806141  0.35191127  0.05659474  0.06112324  0.34458401]\n"
     ]
    }
   ],
   "source": [
    "k = 2                    #Number of latent factors\n",
    "min_loss_delta = 0.00001 #Minimum change in mean squared error to continue training\n",
    "max_epochs = 20          #Maximum number of training rounds\n",
    "    \n",
    "#Initializes P and Qt with random values\n",
    "user_count = X.shape[0]\n",
    "item_count = X.shape[1]\n",
    "P = np.random.rand(user_count,k)\n",
    "Qt = np.random.rand(k,item_count)\n",
    "X_hat = np.dot(P,Qt)\n",
    "\n",
    "#Initial loss\n",
    "loss_delta = 1 + min_loss_delta\n",
    "loss_prev = np.mean(np.square(X - X_hat))\n",
    "\n",
    "#Main loop\n",
    "epoch = 0\n",
    "while loss_delta > min_loss_delta and epoch < max_epochs:\n",
    "    #Iteratively update P and Qt\n",
    "    P = np.dot(X,np.dot(Qt.T,np.linalg.inv(np.dot(Qt,Qt.T))))\n",
    "    Qt = np.dot(np.dot(np.linalg.inv(np.dot(P.T,P)),P.T),X)\n",
    "    X_hat = np.dot(P,Qt)\n",
    "\n",
    "    #Update loss\n",
    "    loss = np.mean(np.square(X - X_hat))\n",
    "    epoch = epoch + 1\n",
    "    print(str(epoch).ljust(3),\"loss:\",round(loss,4))\n",
    "    loss_delta = abs(loss - loss_prev)\n",
    "    loss_prev = loss\n",
    "\n",
    "#Prediction\n",
    "S = np.dot(Qt.T,np.linalg.inv(np.dot(Qt,Qt.T)))\n",
    "p = np.dot(x,S)\n",
    "x_hat = np.dot(p,Qt)\n",
    "print(x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted preference is essentially the same as what we got from a real SVD.\n",
    "\n",
    "If we are going use the algorithm repeatedly, however, it would be best to write a self-contained class that we can use repeatedly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDals():\n",
    "    \"\"\"\n",
    "    Alternating Least Square SVD\n",
    "    \"\"\"    \n",
    "        \n",
    "    def __init__(self,k=2,min_loss_delta=0.00001,max_epochs=20):\n",
    "        \"\"\"\n",
    "        k:               Number of latent factors\n",
    "        min_loss_delta:  Minimum change in mean squared error to continue training\n",
    "        max_epochs:      Maximum number of training rounds\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.min_loss_delta = min_loss_delta\n",
    "        self.max_epochs = max_epochs\n",
    "       \n",
    "    def fit(self,X):\n",
    "        \"\"\"\n",
    "        Fit the model\n",
    "        X: training data\n",
    "        \"\"\"\n",
    "        \n",
    "        #Initialize model parameters\n",
    "        self.initialize(X)\n",
    "        loss, loss_delta = self.update_loss(X,0) \n",
    "        \n",
    "        print(\"Training...\")\n",
    "        epoch = 0\n",
    "        while loss_delta > self.min_loss_delta and epoch < self.max_epochs:\n",
    "            #Update parameters\n",
    "            self.update_params()\n",
    "            \n",
    "            #Update error and loss\n",
    "            loss, loss_delta = self.update_loss(X,loss)\n",
    "            \n",
    "            #Increment counter\n",
    "            epoch = epoch + 1\n",
    "\n",
    "            #Show each round's epoch and self.error\n",
    "            self._printloss(epoch,loss)\n",
    "       \n",
    "    def initialize(self,X):\n",
    "        \"\"\"\n",
    "        Initializes P and Qt\n",
    "        \"\"\"\n",
    "        self.user_count = X.shape[0]\n",
    "        self.item_count = X.shape[1]\n",
    "        #P and Qt uniformly distributed from -0.5 to 0.5\n",
    "        self.P = np.random.rand(self.user_count,self.k) - 0.5 \n",
    "        self.Qt = np.random.rand(self.k,self.item_count ) - 0.5\n",
    "        \n",
    "    def update_params(self):\n",
    "        \"\"\"\n",
    "        Update parameters\n",
    "        \"\"\"        \n",
    "        #Update P and Qt\n",
    "        self.P = np.dot(X,np.dot(self.Qt.T,np.linalg.inv(np.dot(self.Qt,self.Qt.T))))\n",
    "        self.Qt = np.dot(np.dot(np.linalg.inv(np.dot(self.P.T,self.P)),self.P.T),X)\n",
    "        self.S = np.dot(self.Qt.T,np.linalg.inv(np.dot(self.Qt,self.Qt.T)))\n",
    "                \n",
    "    def update_loss(self,X,loss_prev):\n",
    "        \"\"\"\n",
    "        Update self.error and mean squared self.error\n",
    "        \"\"\"        \n",
    "        #Generate Prediction   \n",
    "        X_hat = np.dot(self.P,self.Qt)\n",
    "        \n",
    "        #Error matrix\n",
    "        self.error = X - X_hat\n",
    "        \n",
    "        #loss\n",
    "        loss = self._loss(X,X_hat)\n",
    "        loss_delta = abs(loss_prev - loss)  \n",
    "        \n",
    "        return loss, loss_delta  \n",
    "            \n",
    "    def _loss(self,X,X_hat):\n",
    "        \"\"\"\n",
    "        Calculate mean squared error\n",
    "        \"\"\"\n",
    "        return np.mean(np.square(X - X_hat))\n",
    "    \n",
    "    def _printloss(self,epoch,loss):\n",
    "        \"\"\"\n",
    "        Print formated loss\n",
    "        \"\"\"\n",
    "        print(str(epoch).ljust(3),\"loss:\",round(loss,4))\n",
    "        \n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Inference\n",
    "        x: input data\n",
    "        \"\"\"        \n",
    "        p = np.dot(x,self.S)\n",
    "        x_hat = np.dot(p,self.Qt)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "1   loss: 0.1172\n",
      "2   loss: 0.1051\n",
      "3   loss: 0.0998\n",
      "4   loss: 0.0941\n",
      "5   loss: 0.0891\n",
      "6   loss: 0.0858\n",
      "7   loss: 0.084\n",
      "8   loss: 0.0831\n",
      "9   loss: 0.0826\n",
      "10  loss: 0.0824\n",
      "11  loss: 0.0823\n",
      "12  loss: 0.0823\n",
      "13  loss: 0.0823\n",
      "14  loss: 0.0823\n",
      "[ 0.47808298  0.35284263  0.05599565  0.06169033  0.34362845]\n"
     ]
    }
   ],
   "source": [
    "model = SVDals(k=2)\n",
    "model.fit(X)\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of ALS is that it is highly parallelizable and converges very quickly, resulting in very fast training. This is in contrast to the stochastic gradient descent approach that we will cover next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Gradient Descent\n",
    "\n",
    "*Gradient descent* nudges parameters by an amount proportional to their contribution to the loss function. Gradient Descent and its approximation, *Stochastic Gradient Descent* (SGD), are very general optimization methods, usable in all sorts of models from logistic regression to neural network.\n",
    "\n",
    "A simple example is as follows: Suppose our model is\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\alpha + x\n",
    "$$\n",
    "\n",
    "As is common in regression problem, we would like to minimize the squared error. So our loss function is:\n",
    "\n",
    "$$\n",
    "c = \\left( y - \\hat{y} \\right)^2\n",
    "$$\n",
    "\n",
    "<img src=\"../Images/loss-error.png\" width=\"300\">\n",
    "\n",
    "We have an initial guess of what $\\alpha$ is---often just a random number---and an initial prediction $\\hat{y}_0 = \\alpha_0 + x$. This prediction is likely inaccurate, which means the loss will be positive:\n",
    "\n",
    "$$\n",
    "c_0 = \\left( y - \\hat{y}_0 \\right)^2 > 0\n",
    "$$\n",
    "\n",
    "How do we use this information to update $\\alpha$? Let $\\epsilon_0 = y - \\hat{y}_0$. \n",
    "The marginal effect, or *gradient*, of $\\alpha$ on $c$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c}{\\partial \\alpha} \n",
    "= \\frac{\\partial}{\\partial \\alpha}\\left( y-\\hat{y} \\right)^2 = -2 \\epsilon\n",
    "$$\n",
    "\n",
    "Suppose the error is positive, so $\\epsilon_0 = y - \\hat{y}_0  > 0$. The loss function will be decreasing in $\\alpha$, which makes sense---$\\hat{y}$ is increasing in $\\alpha$, and right now $\\hat{y} < y$. We can make our model more accurate by increasing $\\alpha$. Conversely, we should decrease $\\alpha$ if the error is negative.\n",
    "\n",
    "The gradient thus tells us the direction we need to adjustment our parameter. Furthermore, the amount we need to adjust is, to a first-order approximation, proportional to\n",
    "\n",
    "$$\n",
    "- \\frac{\\partial c}{\\partial \\alpha}\n",
    "$$\n",
    "\n",
    "We therefore have the following update rule:\n",
    "\n",
    "$$\n",
    "\\alpha_{t} = \\alpha_{t-1} - \\gamma \\frac{\\partial c}{\\partial \\alpha} \\bigg\\rvert_{\\alpha_{t-1}}\n",
    "$$\n",
    "\n",
    "Or more typical in computer science:\n",
    "\n",
    "$$\n",
    "\\alpha \\gets \\alpha - \\gamma \\frac{\\partial c}{\\partial \\alpha} \n",
    "$$\n",
    "\n",
    "$\\gamma$ is called the *learning rate*. Learning rate is usually much smaller than 1 to prevent overshooting. It can be manually specified in simple settings such as ours but is often automatically adjusted in more advance alogrithms.\n",
    "\n",
    "There are a couple of options when it comes to the computation of gradient:\n",
    "- Averaging the gradient from all samples. The advantage of this method is that the \"true\" gradient is used, in the sense that it reflects the overall gradient of the training data. The disadvantage is that the speed of convergence is slow, since we are only updating the model parameters after we compute the gradient of all observations.\n",
    "- **Stochastic Gradient Descent (SGD):** Update parameters with the average gradient of a subset of samples. Some updates will push the parameters in one direction while some others will push them in the other direction---this is the *stochastic* part of the algorithm---but on average the parameters will move towards the right direction. Besides allowing for faster convergence, the fact that the gradient is noisy also helps the model avoid local minimas. \n",
    "\n",
    "Large-scale machine learning models are typically trained on variations of SGD. Data is broken into mutually-exclusive groups called *mini-batches* and model parameters are updated with the average gradient of each mini-batch.  \n",
    "\n",
    "Now specifically for SVD, we have for each user $u$ and each item $i$,\n",
    "\n",
    "$$\n",
    "\\hat{x}_{ui} = p_u q^T_i\n",
    "$$\n",
    "\n",
    "The loss function is:\n",
    "\n",
    "$$\n",
    "\\sum_{u,i}{\\left( x_{ui} - \\hat{x}_{ui} \\right)^2}\n",
    "$$\n",
    "\n",
    "so the gradient consists of:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c_{ui}}{\\partial p_u} = -2 \\epsilon_{ui} \\cdot q_i \\\\\n",
    "\\frac{\\partial c_{ui}}{\\partial q_i} = -2 \\epsilon_{ui} \\cdot p_u\n",
    "$$\n",
    "\n",
    "The update rules for $P$ and $Q^T$ are thus:\n",
    "\n",
    "$$\n",
    "P \\gets P + \\gamma \\mathcal{E} Q \\\\\n",
    "Q^T \\gets Q^T + \\gamma P^T \\mathcal{E}\n",
    "$$\n",
    "\n",
    "As before, here is a straight-forward implementation of gradient descent SVD. Most of the codes are shared with the ALS implementation---the only difference is the few lines updating $P$ and $Q^T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   loss: 0.2349\n",
      "2   loss: 0.2103\n",
      "3   loss: 0.1969\n",
      "4   loss: 0.1874\n",
      "5   loss: 0.1797\n",
      "6   loss: 0.1728\n",
      "7   loss: 0.1663\n",
      "8   loss: 0.1598\n",
      "9   loss: 0.1532\n",
      "10  loss: 0.1465\n",
      "11  loss: 0.1396\n",
      "12  loss: 0.1326\n",
      "13  loss: 0.1257\n",
      "14  loss: 0.119\n",
      "15  loss: 0.1129\n",
      "16  loss: 0.1075\n",
      "17  loss: 0.1028\n",
      "18  loss: 0.0988\n",
      "19  loss: 0.0957\n",
      "20  loss: 0.0931\n",
      "21  loss: 0.0911\n",
      "22  loss: 0.0896\n",
      "23  loss: 0.0883\n",
      "24  loss: 0.0873\n",
      "25  loss: 0.0865\n",
      "26  loss: 0.0859\n",
      "27  loss: 0.0854\n",
      "28  loss: 0.085\n",
      "29  loss: 0.0846\n",
      "30  loss: 0.0843\n",
      "31  loss: 0.0841\n",
      "32  loss: 0.0839\n",
      "33  loss: 0.0837\n",
      "34  loss: 0.0836\n",
      "35  loss: 0.0834\n",
      "36  loss: 0.0833\n",
      "37  loss: 0.0832\n",
      "38  loss: 0.0832\n",
      "39  loss: 0.0831\n",
      "40  loss: 0.083\n",
      "41  loss: 0.083\n",
      "42  loss: 0.0829\n",
      "43  loss: 0.0829\n",
      "44  loss: 0.0828\n",
      "45  loss: 0.0828\n",
      "46  loss: 0.0827\n",
      "47  loss: 0.0827\n",
      "48  loss: 0.0827\n",
      "49  loss: 0.0827\n",
      "50  loss: 0.0826\n",
      "51  loss: 0.0826\n",
      "52  loss: 0.0826\n",
      "53  loss: 0.0826\n",
      "54  loss: 0.0825\n",
      "55  loss: 0.0825\n",
      "56  loss: 0.0825\n",
      "57  loss: 0.0825\n",
      "58  loss: 0.0825\n",
      "59  loss: 0.0825\n",
      "60  loss: 0.0825\n",
      "61  loss: 0.0824\n",
      "62  loss: 0.0824\n",
      "63  loss: 0.0824\n",
      "64  loss: 0.0824\n",
      "[ 0.47924508  0.37198937  0.04257354  0.07299684  0.32257108]\n"
     ]
    }
   ],
   "source": [
    "k = 2                    #Number of latent factors\n",
    "min_loss_delta = 0.00001 #Minimum change in mean squared error to continue training\n",
    "max_epochs = 200         #Maximum number of training rounds\n",
    "learning_rate=0.1        #Learning rate\n",
    "    \n",
    "#Initializes P and Qt with random values\n",
    "user_count = X.shape[0]\n",
    "item_count = X.shape[1]\n",
    "P = np.random.rand(user_count,k)\n",
    "Qt = np.random.rand(k,item_count)\n",
    "X_hat = np.dot(P,Qt)\n",
    "\n",
    "#Initial loss\n",
    "loss_delta = 1 + min_loss_delta\n",
    "loss_prev = np.mean(np.square(X - X_hat))\n",
    "\n",
    "#Main loop\n",
    "epoch = 0\n",
    "while loss_delta > min_loss_delta and epoch < max_epochs:\n",
    "    #Iteratively update P and Qt\n",
    "    error = X - X_hat\n",
    "    P = P + learning_rate * (np.dot(error, Qt.T))\n",
    "    Qt = Qt + learning_rate * (np.dot(P.T,error))\n",
    "    X_hat = np.dot(P,Qt)\n",
    "    \n",
    "    #Update loss\n",
    "    loss = np.mean(np.square(X - X_hat))\n",
    "    epoch = epoch + 1\n",
    "    print(str(epoch).ljust(3),\"loss:\",round(loss,4))\n",
    "    loss_delta = abs(loss - loss_prev)\n",
    "    loss_prev = loss\n",
    "\n",
    "#Prediction\n",
    "S = np.dot(Qt.T,np.linalg.inv(np.dot(Qt,Qt.T)))\n",
    "p = np.dot(x,S)\n",
    "x_hat = np.dot(p,Qt)\n",
    "print(x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how many more epochs it takes for gradient descent to converge in contrast to ALS. Can we speed things up by setting a higher learning rate? If we try different learning rates, we will see that having too high a learning rate would result in constant overshooting, and as a result no convergence.\n",
    "\n",
    "Below is a class implementing gradient descent SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDgd():\n",
    "    \"\"\"\n",
    "    Gradient Descent SVD\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self,k=2,\n",
    "                 min_loss_delta=0.00001,\n",
    "                 max_epochs=200,\n",
    "                 learning_rate=0.1,\n",
    "                 show_progress=True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        k:               Number of latent factors\n",
    "        min_loss_delta:   Minimum change in mean squared error to continue training\n",
    "        max_epochs:      Maximum number of training rounds\n",
    "        learning_rate:   Learning rate\n",
    "        show_progress:   Print error of each epoch\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.min_loss_delta = min_loss_delta\n",
    "        self.max_epochs = max_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.show_progress = show_progress\n",
    "        \n",
    "    def fit(self,X):\n",
    "        \"\"\"\n",
    "        Fit the  model\n",
    "        X: training data\n",
    "        \"\"\"\n",
    "        \n",
    "        #Initialize model parameters\n",
    "        self.initialize(X)\n",
    "        loss, loss_delta = self.update_loss(X,0) \n",
    "        \n",
    "        print(\"Training...\")\n",
    "        epoch = 0\n",
    "        while loss_delta > self.min_loss_delta and epoch < self.max_epochs:\n",
    "            #Update parameters\n",
    "            self.update_params()\n",
    "            \n",
    "            #Update error and loss\n",
    "            loss, loss_delta = self.update_loss(X,loss)\n",
    "            \n",
    "            #Increment counter\n",
    "            epoch = epoch + 1\n",
    "            \n",
    "            if self.show_progress:\n",
    "                #Show each round's epoch and self.error\n",
    "                self._printloss(epoch,loss)\n",
    "                \n",
    "        if not self.show_progress:\n",
    "            #Show the final epoch and self.error\n",
    "            self._printloss(epoch,loss)\n",
    "       \n",
    "    def initialize(self,X):\n",
    "        \"\"\"\n",
    "        Initializes P and Qt\n",
    "        \"\"\"\n",
    "        self.user_count = X.shape[0]\n",
    "        self.item_count = X.shape[1]\n",
    "        #P and Qt uniformly distributed from -0.5 to 0.5\n",
    "        self.P = np.random.rand(self.user_count,self.k) - 0.5 \n",
    "        self.Qt = np.random.rand(self.k,self.item_count ) - 0.5\n",
    "        \n",
    "    def update_params(self):\n",
    "        \"\"\"\n",
    "        Update parameters\n",
    "        \"\"\"        \n",
    "        #Update P and Qt with previous epoch's loss\n",
    "        self.P = self.P + self.learning_rate * (np.dot(self.error, self.Qt.T))\n",
    "        self.Qt = self.Qt + self.learning_rate * (np.dot(self.P.T,self.error))\n",
    "        self.S = np.dot(self.Qt.T,np.linalg.inv(np.dot(self.Qt,self.Qt.T)))\n",
    "                \n",
    "    def update_loss(self,X,loss_prev):\n",
    "        \"\"\"\n",
    "        Update self.error and mean squared self.error\n",
    "        \"\"\"        \n",
    "        #Generate Prediction   \n",
    "        X_hat = np.dot(self.P,self.Qt)\n",
    "        \n",
    "        #Error matrix\n",
    "        self.error = X - X_hat\n",
    "        \n",
    "        #loss\n",
    "        loss = self._loss(X,X_hat)\n",
    "        loss_delta = abs(loss_prev - loss)  \n",
    "        \n",
    "        return loss, loss_delta  \n",
    "            \n",
    "    def _loss(self,X,X_hat):\n",
    "        \"\"\"\n",
    "        Calculate mean squared error\n",
    "        \"\"\"\n",
    "        return np.mean(np.square(X - X_hat))\n",
    "    \n",
    "    def _printloss(self,epoch,loss):\n",
    "        \"\"\"\n",
    "        Print formated loss\n",
    "        \"\"\"\n",
    "        print(str(epoch).ljust(3),\"loss:\",round(loss,4))\n",
    "        \n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Inference\n",
    "        x: input data\n",
    "        \"\"\"        \n",
    "        p = np.dot(x,self.S)\n",
    "        x_hat = np.dot(p,self.Qt)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let us try out the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "1   loss: 0.3877\n",
      "2   loss: 0.3761\n",
      "3   loss: 0.3641\n",
      "4   loss: 0.3505\n",
      "5   loss: 0.3343\n",
      "6   loss: 0.3152\n",
      "7   loss: 0.2937\n",
      "8   loss: 0.2712\n",
      "9   loss: 0.2494\n",
      "10  loss: 0.2301\n",
      "11  loss: 0.2141\n",
      "12  loss: 0.2015\n",
      "13  loss: 0.1917\n",
      "14  loss: 0.1839\n",
      "15  loss: 0.1775\n",
      "16  loss: 0.1719\n",
      "17  loss: 0.1667\n",
      "18  loss: 0.1617\n",
      "19  loss: 0.1566\n",
      "20  loss: 0.1511\n",
      "21  loss: 0.1452\n",
      "22  loss: 0.1389\n",
      "23  loss: 0.1322\n",
      "24  loss: 0.1254\n",
      "25  loss: 0.1186\n",
      "26  loss: 0.1123\n",
      "27  loss: 0.1066\n",
      "28  loss: 0.1018\n",
      "29  loss: 0.0978\n",
      "30  loss: 0.0947\n",
      "31  loss: 0.0922\n",
      "32  loss: 0.0904\n",
      "33  loss: 0.089\n",
      "34  loss: 0.0879\n",
      "35  loss: 0.0871\n",
      "36  loss: 0.0864\n",
      "37  loss: 0.0859\n",
      "38  loss: 0.0855\n",
      "39  loss: 0.0851\n",
      "40  loss: 0.0848\n",
      "41  loss: 0.0846\n",
      "42  loss: 0.0843\n",
      "43  loss: 0.0842\n",
      "44  loss: 0.084\n",
      "45  loss: 0.0839\n",
      "46  loss: 0.0837\n",
      "47  loss: 0.0836\n",
      "48  loss: 0.0835\n",
      "49  loss: 0.0834\n",
      "50  loss: 0.0833\n",
      "51  loss: 0.0833\n",
      "52  loss: 0.0832\n",
      "53  loss: 0.0831\n",
      "54  loss: 0.0831\n",
      "55  loss: 0.083\n",
      "56  loss: 0.083\n",
      "57  loss: 0.0829\n",
      "58  loss: 0.0829\n",
      "59  loss: 0.0829\n",
      "60  loss: 0.0828\n",
      "61  loss: 0.0828\n",
      "62  loss: 0.0827\n",
      "63  loss: 0.0827\n",
      "64  loss: 0.0827\n",
      "65  loss: 0.0827\n",
      "66  loss: 0.0826\n",
      "67  loss: 0.0826\n",
      "68  loss: 0.0826\n",
      "69  loss: 0.0826\n",
      "70  loss: 0.0826\n",
      "71  loss: 0.0825\n",
      "72  loss: 0.0825\n",
      "73  loss: 0.0825\n",
      "74  loss: 0.0825\n",
      "75  loss: 0.0825\n",
      "76  loss: 0.0825\n",
      "77  loss: 0.0824\n",
      "78  loss: 0.0824\n",
      "79  loss: 0.0824\n",
      "80  loss: 0.0824\n",
      "81  loss: 0.0824\n",
      "[ 0.47969757  0.371939    0.04243308  0.0728907   0.32270044]\n"
     ]
    }
   ],
   "source": [
    "model = SVDgd(k=2)\n",
    "model.fit(X)\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Simon Funk's SVD\n",
    "\n",
    "This is a method popularized during the Netflix Prize, and it is usually what people refers to when they mention \"SVD\" in the context of collaborative filtering.  \n",
    "\n",
    "The prediction of the model is given by:\n",
    "$$\n",
    "\\hat{x}_{ui} = \\mu + b_u + b_i + p_u q^T_i\n",
    "$$\n",
    "\n",
    "$\\mu$, $b_u$ and $b_i$ are called *bias* in machine learning, but a more familiar name for economists would be dummy variables. So Funk's SVD is essentially SVD with fixed effects.\n",
    "\n",
    "The model is also regularized, so the loss function is:\n",
    "$$\n",
    "\\sum_{u,i}{\\left( x_{ui} - \\hat{x}_{ui} \\right)^2 \n",
    "+ \\alpha \\left( b_u^2 + b_i^2 + \\lVert p_u \\rVert^2 + \\lVert q_i \\rVert^2  \\right) }\n",
    "$$\n",
    "where $\\alpha$ is the strength of regularization. \n",
    "\n",
    "Reference:\n",
    "- <a href=\"http://sifter.org/~simon/journal/20061211.html\">Netflix Update: Try This at Home</a>\n",
    " \n",
    "Here are the list of changes we have to make in comparison with the simple SGD implementation:\n",
    "- We need to add four variables, three representing the biases (```mu```, ```bu```, ```bi```) and one the strength of regularization (```alpha```). The biases need to be updated in our main loop.\n",
    "- The model's prediction needs to be updated to include the biases.\n",
    "- The loss function needs to be updated to includ regularization.\n",
    "- Since we have more parameters to estimate we will increase the maximum epochs to give the model more time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   loss: 0.7098\n",
      "2   loss: 0.419\n",
      "3   loss: 0.311\n",
      "4   loss: 0.258\n",
      "5   loss: 0.2262\n",
      "6   loss: 0.2048\n",
      "7   loss: 0.1894\n",
      "8   loss: 0.1782\n",
      "9   loss: 0.1696\n",
      "10  loss: 0.1627\n",
      "11  loss: 0.1569\n",
      "12  loss: 0.1517\n",
      "13  loss: 0.1469\n",
      "14  loss: 0.1425\n",
      "15  loss: 0.1384\n",
      "16  loss: 0.1346\n",
      "17  loss: 0.1311\n",
      "18  loss: 0.1279\n",
      "19  loss: 0.125\n",
      "20  loss: 0.1224\n",
      "21  loss: 0.1199\n",
      "22  loss: 0.1177\n",
      "23  loss: 0.1155\n",
      "24  loss: 0.1135\n",
      "25  loss: 0.1115\n",
      "26  loss: 0.1096\n",
      "27  loss: 0.1077\n",
      "28  loss: 0.1059\n",
      "29  loss: 0.104\n",
      "30  loss: 0.1022\n",
      "31  loss: 0.1004\n",
      "32  loss: 0.0986\n",
      "33  loss: 0.0969\n",
      "34  loss: 0.0952\n",
      "35  loss: 0.0935\n",
      "36  loss: 0.0919\n",
      "37  loss: 0.0904\n",
      "38  loss: 0.0889\n",
      "39  loss: 0.0875\n",
      "40  loss: 0.0862\n",
      "41  loss: 0.085\n",
      "42  loss: 0.0838\n",
      "43  loss: 0.0827\n",
      "44  loss: 0.0817\n",
      "45  loss: 0.0808\n",
      "46  loss: 0.0799\n",
      "47  loss: 0.0792\n",
      "48  loss: 0.0784\n",
      "49  loss: 0.0778\n",
      "50  loss: 0.0772\n",
      "51  loss: 0.0766\n",
      "52  loss: 0.0761\n",
      "53  loss: 0.0756\n",
      "54  loss: 0.0752\n",
      "55  loss: 0.0748\n",
      "56  loss: 0.0745\n",
      "57  loss: 0.0741\n",
      "58  loss: 0.0738\n",
      "59  loss: 0.0736\n",
      "60  loss: 0.0733\n",
      "61  loss: 0.0731\n",
      "62  loss: 0.0729\n",
      "63  loss: 0.0727\n",
      "64  loss: 0.0725\n",
      "65  loss: 0.0724\n",
      "66  loss: 0.0722\n",
      "67  loss: 0.0721\n",
      "68  loss: 0.072\n",
      "69  loss: 0.0719\n",
      "70  loss: 0.0718\n",
      "71  loss: 0.0717\n",
      "72  loss: 0.0716\n",
      "73  loss: 0.0715\n",
      "74  loss: 0.0714\n",
      "75  loss: 0.0714\n",
      "76  loss: 0.0713\n",
      "77  loss: 0.0713\n",
      "78  loss: 0.0712\n",
      "79  loss: 0.0711\n",
      "80  loss: 0.0711\n",
      "81  loss: 0.071\n",
      "82  loss: 0.071\n",
      "83  loss: 0.071\n",
      "84  loss: 0.0709\n",
      "85  loss: 0.0709\n",
      "86  loss: 0.0708\n",
      "87  loss: 0.0708\n",
      "88  loss: 0.0708\n",
      "89  loss: 0.0707\n",
      "90  loss: 0.0707\n",
      "91  loss: 0.0707\n",
      "92  loss: 0.0707\n",
      "93  loss: 0.0706\n",
      "94  loss: 0.0706\n",
      "95  loss: 0.0706\n",
      "96  loss: 0.0706\n",
      "97  loss: 0.0705\n",
      "98  loss: 0.0705\n",
      "99  loss: 0.0705\n",
      "100 loss: 0.0705\n",
      "101 loss: 0.0704\n",
      "102 loss: 0.0704\n",
      "103 loss: 0.0704\n",
      "104 loss: 0.0704\n",
      "105 loss: 0.0704\n",
      "106 loss: 0.0703\n",
      "107 loss: 0.0703\n",
      "108 loss: 0.0703\n",
      "109 loss: 0.0703\n",
      "110 loss: 0.0703\n",
      "111 loss: 0.0702\n",
      "112 loss: 0.0702\n",
      "113 loss: 0.0702\n",
      "114 loss: 0.0702\n",
      "115 loss: 0.0702\n",
      "116 loss: 0.0701\n",
      "117 loss: 0.0701\n",
      "118 loss: 0.0701\n",
      "119 loss: 0.0701\n",
      "120 loss: 0.0701\n",
      "121 loss: 0.0701\n",
      "122 loss: 0.07\n",
      "123 loss: 0.07\n",
      "124 loss: 0.07\n",
      "125 loss: 0.07\n",
      "126 loss: 0.07\n",
      "127 loss: 0.07\n",
      "128 loss: 0.0699\n",
      "129 loss: 0.0699\n",
      "130 loss: 0.0699\n",
      "131 loss: 0.0699\n",
      "132 loss: 0.0699\n",
      "133 loss: 0.0699\n",
      "134 loss: 0.0699\n",
      "135 loss: 0.0698\n",
      "136 loss: 0.0698\n",
      "137 loss: 0.0698\n",
      "138 loss: 0.0698\n",
      "139 loss: 0.0698\n",
      "140 loss: 0.0698\n",
      "141 loss: 0.0697\n",
      "142 loss: 0.0697\n",
      "143 loss: 0.0697\n",
      "144 loss: 0.0697\n",
      "145 loss: 0.0697\n",
      "146 loss: 0.0697\n",
      "147 loss: 0.0697\n",
      "148 loss: 0.0696\n",
      "149 loss: 0.0696\n",
      "150 loss: 0.0696\n",
      "151 loss: 0.0696\n",
      "152 loss: 0.0696\n",
      "153 loss: 0.0696\n",
      "154 loss: 0.0696\n",
      "155 loss: 0.0696\n",
      "156 loss: 0.0695\n",
      "157 loss: 0.0695\n",
      "158 loss: 0.0695\n",
      "159 loss: 0.0695\n",
      "160 loss: 0.0695\n",
      "161 loss: 0.0695\n",
      "162 loss: 0.0695\n",
      "163 loss: 0.0695\n",
      "164 loss: 0.0694\n",
      "165 loss: 0.0694\n",
      "166 loss: 0.0694\n",
      "167 loss: 0.0694\n",
      "168 loss: 0.0694\n",
      "169 loss: 0.0694\n",
      "170 loss: 0.0694\n",
      "171 loss: 0.0694\n",
      "172 loss: 0.0693\n",
      "173 loss: 0.0693\n",
      "174 loss: 0.0693\n",
      "175 loss: 0.0693\n",
      "176 loss: 0.0693\n",
      "177 loss: 0.0693\n",
      "178 loss: 0.0693\n",
      "179 loss: 0.0693\n",
      "180 loss: 0.0693\n",
      "181 loss: 0.0692\n",
      "182 loss: 0.0692\n",
      "183 loss: 0.0692\n",
      "184 loss: 0.0692\n",
      "185 loss: 0.0692\n",
      "186 loss: 0.0692\n",
      "187 loss: 0.0692\n",
      "188 loss: 0.0692\n",
      "189 loss: 0.0692\n",
      "190 loss: 0.0691\n",
      "191 loss: 0.0691\n",
      "192 loss: 0.0691\n",
      "193 loss: 0.0691\n",
      "194 loss: 0.0691\n",
      "195 loss: 0.0691\n",
      "196 loss: 0.0691\n",
      "[[ 0.90374984  0.32944241 -0.07233775 -0.14853701  0.45271552]]\n"
     ]
    }
   ],
   "source": [
    "#Simon Funk's SVD\n",
    "k = 2                    #Number of latent factors\n",
    "min_loss_delta = 0.00001 #Minimum change in mean squared error to continue training\n",
    "max_epochs = 800         #Maximum number of training rounds\n",
    "learning_rate=0.1        #Learning rate\n",
    "alpha=0.05               #Regularization strength\n",
    "    \n",
    "#Initializes P and Qt with random values\n",
    "user_count = X.shape[0]\n",
    "item_count = X.shape[1]\n",
    "\n",
    "#Original SVD matrices\n",
    "P = np.random.rand(user_count,k)\n",
    "Qt = np.random.rand(k,item_count)\n",
    "\n",
    "#Biases\n",
    "mu = np.mean(X)\n",
    "bu = np.random.rand(user_count,1) \n",
    "bi = np.random.rand(item_count,1)\n",
    "\n",
    "#Vectors of 1\n",
    "ones_i = np.ones((1,item_count))\n",
    "ones_u = np.ones((user_count,1))   \n",
    "\n",
    "X_hat = (mu + np.dot(bu,ones_i)\n",
    "         + np.dot(ones_u,bi.T) \n",
    "         + np.dot(P,Qt))  \n",
    "\n",
    "#Initial loss and error\n",
    "error = X - X_hat\n",
    "\n",
    "loss_delta = 1 + min_loss_delta\n",
    "loss_prev = np.mean(np.square(X - X_hat)) + alpha * (\n",
    "                np.mean(bu**2)\n",
    "                + np.mean(bi**2)\n",
    "                + np.mean(P**2)\n",
    "                + np.mean(Qt**2)\n",
    "                )\n",
    "\n",
    "#Main loop\n",
    "epoch = 0\n",
    "while loss_delta > min_loss_delta and epoch < max_epochs:\n",
    "    #Iteratively update P and Qt\n",
    "    P = P + learning_rate * (np.dot(error, Qt.T))\n",
    "    Qt = Qt + learning_rate * (np.dot(P.T,error))\n",
    "\n",
    "    #Update biases\n",
    "    mu = mu + learning_rate * np.mean(error)\n",
    "    bu = bu + learning_rate * (\n",
    "         np.mean(error,axis=1).reshape(user_count,1)\n",
    "         - alpha * bu\n",
    "         )\n",
    "    bi = bi + learning_rate * (\n",
    "         np.mean(error,axis=0).reshape(item_count,1)\n",
    "         - alpha * bi\n",
    "         )\n",
    "    \n",
    "    X_hat = (mu + np.dot(bu,ones_i) \n",
    "             + np.dot(ones_u,bi.T) \n",
    "             + np.dot(P,Qt))  \n",
    "    \n",
    "    #Update loss and error\n",
    "    error = X - X_hat\n",
    "    loss = np.mean(np.square(X - X_hat)) + alpha * (\n",
    "                np.mean(bu**2)\n",
    "                + np.mean(bi**2)\n",
    "                + np.mean(P**2)\n",
    "                + np.mean(Qt**2)\n",
    "                )\n",
    "    epoch = epoch + 1\n",
    "    print(str(epoch).ljust(3),\"loss:\",round(loss,4))\n",
    "    loss_delta = abs(loss - loss_prev)\n",
    "    loss_prev = loss\n",
    "\n",
    "#Prediction\n",
    "S = np.dot(Qt.T,np.linalg.inv(np.dot(Qt,Qt.T)))\n",
    "p = np.dot(x,S)\n",
    "x_hat = mu + bi.T + np.dot(p,Qt) #user bias is zero for new user\n",
    "print(x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we extend the ```SVDsgd``` class to create a new ```SVDfunk``` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDfunk(SVDgd):\n",
    "    \"\"\"\n",
    "    Simon Funk's SVD. Regularized.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,alpha=0,max_epochs=500,*args,**kargs):\n",
    "        \"\"\"\n",
    "        alpha: regularization strength\n",
    "        \"\"\"    \n",
    "        self.alpha = alpha\n",
    "        #pass other arguments to parent class\n",
    "        super().__init__(max_epochs=max_epochs,*args,**kargs)\n",
    "      \n",
    "    def initialize(self,X):\n",
    "        \"\"\"\n",
    "        Initializes biases, P and Qt\n",
    "        \"\"\"\n",
    "        super().initialize(X) #Use parent class to initialize P and Qt\n",
    "        self.mu = np.mean(X)\n",
    "        self.bu = np.random.rand(self.user_count,1)\n",
    "        self.bi = np.random.rand(self.item_count,1)\n",
    "        \n",
    "    def update_params(self):\n",
    "        \"\"\"\n",
    "        Update parameters\n",
    "        \"\"\"        \n",
    "         #Update biases, P and Qt with previous epoch's loss\n",
    "        self.mu = self.mu + self.learning_rate * np.mean(self.error)\n",
    "        self.bu = self.bu + self.learning_rate * (\n",
    "                np.mean(self.error,axis=1).reshape(self.user_count,1)\n",
    "                - self.alpha * self.bu\n",
    "                )\n",
    "        self.bi = self.bi + self.learning_rate * (\n",
    "                np.mean(self.error,axis=0).reshape(self.item_count,1)\n",
    "                - self.alpha * self.bi\n",
    "                )\n",
    "        self.P = self.P + self.learning_rate * (\n",
    "                np.dot(self.error, self.Qt.T)\n",
    "                - self.alpha * self.P\n",
    "                )\n",
    "        self.Qt = self.Qt + self.learning_rate * (\n",
    "                np.dot(self.P.T,self.error)\n",
    "                - self.alpha * self.Qt\n",
    "                )\n",
    "        self.S = np.dot(self.Qt.T,np.linalg.inv(np.dot(self.Qt,self.Qt.T)))\n",
    "        \n",
    "    def update_loss(self,X,loss_prev):\n",
    "        \"\"\"\n",
    "        Update self.error and mean squared self.error\n",
    "        \"\"\"        \n",
    "        #Generate Prediction\n",
    "        ones_i = np.ones((1,self.item_count))\n",
    "        ones_u = np.ones((self.user_count,1))        \n",
    "        X_hat = (self.mu + np.dot(self.bu,ones_i) \n",
    "                 + np.dot(ones_u,self.bi.T) \n",
    "                 + np.dot(self.P,self.Qt))  \n",
    "        \n",
    "        #Error matrix\n",
    "        self.error = X - X_hat\n",
    "        \n",
    "        #Regularized loss\n",
    "        loss = self._loss(X,X_hat) + self.alpha * (\n",
    "                np.mean(self.bu**2)\n",
    "                + np.mean(self.bi**2)\n",
    "                + np.mean(self.P**2)\n",
    "                + np.mean(self.Qt**2)\n",
    "                )\n",
    "        loss_delta = abs(loss_prev - loss)  \n",
    "        \n",
    "        return loss, loss_delta  \n",
    "\n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Inference\n",
    "        x: input data\n",
    "        \"\"\"        \n",
    "        p = np.dot(x,self.S)\n",
    "        x_hat = self.mu + self.bi.T + np.dot(p,self.Qt) #user bias is zero for new user\n",
    "        return x_hat    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try out the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "244 loss: 0.0636\n",
      "[[ 1.02129603  0.50273299 -0.18212007 -0.07692987  0.33253277]]\n"
     ]
    }
   ],
   "source": [
    "model = SVDfunk(k=2,alpha=0.05,show_progress=False)\n",
    "model.fit(X)\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, there seems to be little difference between ```SVDfunk``` and ```SVDsgd```. Taking a closer look at the mean-squared errors of the two models, however, and it is clear that ```SVDfunk``` performs better for this particular metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "157 loss: 0.0824\n",
      "[ 0.4794837   0.32214011  0.073204    0.04215209  0.37238314]\n",
      "Training...\n",
      "95  loss: 0.0329\n",
      "[[ 0.72632976  0.80918189 -0.13537655  0.30472425  0.10386051]]\n"
     ]
    }
   ],
   "source": [
    "#SGD\n",
    "model = SVDgd(k=2,show_progress=False)\n",
    "model.fit(X)\n",
    "print(model.predict(x))\n",
    "\n",
    "#Funk's\n",
    "model = SVDfunk(k=2,show_progress=False)\n",
    "model.fit(X)\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar performance lead exists for explicit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "34  loss: 0.6965\n",
      "[ 2.17086641  0.35714357  0.06248979 -0.17769563  1.27941628]\n",
      "Training...\n",
      "61  loss: 0.3174\n",
      "[[ 3.37003186  1.39519828 -0.29970494  0.61011098  1.38816256]]\n"
     ]
    }
   ],
   "source": [
    "#Extract data from dataframe\n",
    "X2 = np.asarray(data_explicit[[\"movie1\",\"movie2\",\"movie3\",\"movie4\",\"movie5\"]])\n",
    "\n",
    "#New user who rated movie1 with a 3\n",
    "x2 = np.array([3,0,0,0,0])\n",
    "\n",
    "#SGD\n",
    "model = SVDgd(k=2,show_progress=False)\n",
    "model.fit(X2)\n",
    "print(model.predict(x2))\n",
    "\n",
    "#Funk's\n",
    "model = SVDfunk(k=2,show_progress=False)\n",
    "model.fit(X2)\n",
    "print(model.predict(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have gone through a few different methods of generating recommendations based on existing records. \n",
    "\n",
    "If you are interested in collaborative filtering, be sure to check out the Netflix Prize. The data is <a href=\"https://www.kaggle.com/netflix-inc/netflix-prize-data\">available on Kaggle</a>, and you can find the winning team's research papers <a href=\"https://netflixprize.com/community/topic_1537.html\">here</a>. One thing you will find is the the winning teams all employ an ensemble of models, which very often perform better than any single model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
